{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"KEROD - Object Detection for TensorFlow 2.X (FasterRCNN, DeTr) Read Latest Documentation - Browse GitHub Code Repository Kerod is pure tensorflow 2 implementation of object detection algorithms (Faster R-CNN, DeTr) aiming production. It stands for Keras Object Detection. It aims to build a clear, reusable, tested, simple and documented codebase for tensorflow 2.X. Many ideas have been based on google object detection , tensorpack and mmdetection . Features As powerful and concise as Keras Low barrier to entry for educators and practitioners Handle batch in training and inference Rich Documentation Multi-GPU Mixed_precision. You can try it with this notebook No need to struggle to download COCO or PascalVoc. We only used tensorflow_datasets Simple (again) Algorithms [x] Feature Pyramidal Network [x] DeTr from: End to end object detection with transformers [x] Single scale SMCA (WIP not 100% the exact implementation of the paper): Fast Convergence of DETR with Spatially Modulated Co-Attention [ ] Mask-RCNN (Much of the pieces are already here just need to put everything together. It will arrive soon.) Try Kerod Notebooks Training an algorithm on COCO or Pascal VOC has never been so easy. You just need to run the cells and everything will be done for you. You can find examples in the notebooks folder . There are no runners shipped with the library. Algorithm Dataset Performance MultiGPU Mixed Precision Notebook FasterRcnnFPNResnet50Pytorch PascalVoc FasterRcnnFPNResnet50Pytorch PascalVoc :heavy_check_mark: FasterRcnnFPNResnet50Pytorch COCO FasterRcnnFPNResnet50Pytorch COCO 30 mAP on old code base. A bug has been removed since. I need 4 GPUs to rerun the training. :heavy_check_mark: DetrResnet50Pytorch COCO NEED 16 GPUS for 3 days :heavy_check_mark: SMCAR50Pytorch COCO NEED 8 GPUS for 1 days :heavy_check_mark: Pytorch: means resnet implementation Pytorch style. In the residual block we have: conv (1x1) stride 1 -> conv (3x3) stride 2 instead of conv (1x1) stride 2 -> conv (3x3) stride 1 (Caffe, Keras implementations) If you want to perform an overfit you have an example with the detr architecture: Requirements If you don't run the examples on Colab please install tensorflow_datasets : pip install tensorflow_datasets No configuration file The code is (I hope) as simple as possible. You won't find any configuration file. All the parameters have already been chosen for you. If you need to change something simply code it and create a new layer. Why: In deep learning each parameter is important. You must think thoroughly before a change on how it will impact your model. Here, the code base is super simple just rewrite the blocks that you need and create new layers using the power of Keras. Also, it makes the code easier to read. Installation This repo is tested on Python 3.6, 3.7, 3.8 and TensorFlow 2.4.0 You may want to install 'kerod' in a virtual environment or with pyenv . Create a virtual environment with the version of Python you wanna use and activate it. With pip pip install git+https://github.com/EmGarr/kerod.git From source git clone https://github.com/EmGarr/kerod.git cd kerod pip install . When you update the repository, you should upgrade installation and its dependencies as follows: git pull pip install --upgrade . You can install the package in dev mode as follow and everytime you refresh the package it will be automatically updated: pip install -e . Tutorials Simple example To run a training you just need to write the following. import numpy as np from kerod.dataset.preprocessing import expand_dims_for_single_batch , preprocess from kerod.model import factory , KerodModel num_classes = 20 model = factory . build_model ( num_classes , name = KerodModel . faster_rcnn_resnet50_pytorch ) # Same format than COCO and Pascal VOC in tensorflow datasets inputs = { 'image' : np . zeros (( 2 , 100 , 50 , 3 )), 'objects' : { BoxField . BOXES : np . array ([[[ 0 , 0 , 1 , 1 ]], [[ 0 , 0 , 1 , 1 ]]], dtype = np . float32 ), BoxField . LABELS : np . array ([[ 1 ], [ 1 ]]) } } data = tf . data . Dataset . from_tensor_slices ( inputs ) data = data . map ( preprocess ) data = data . map ( expand_dims_for_single_batch ) base_lr = 0.02 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( data , epochs = 2 , callbacks = [ ModelCheckpoint ( 'checkpoints' )]) results = model . predict ( data , batch_size = 1 ) use_faster_rcnn = True if use_faster_rcnn : model . export_for_serving ( 'saved_model' ) else : model . save ( 'saved_model' ) reload_model = tf . keras . models . load_model ( 'saved_model' ) for x , _ in data : if use_faster_rcnn : reload_model . serving_step ( x [ DatasetField . IMAGES ], x [ DatasetField . IMAGES_INFO ]) else : reload_model . predict_step ( x ) Mixed Precision from tensorflow.keras.mixed_precision import experimental as mixed_precision from kerod.dataset.preprocessing import expand_dims_for_single_batch , preprocess from kerod.model import factory policy = mixed_precision . Policy ( 'mixed_float16' ) mixed_precision . set_policy ( policy ) num_classes = 20 model = factory . build_model ( num_classes ) Multi-GPU training import numpy as np from kerod.dataset.preprocessing import expand_dims_for_single_batch , preprocess from kerod.model import factory batch_size_per_gpu = 2 num_gpus = 8 batch_size = batch_size_per_gpu * num_gpus padded_shape = ({ DatasetField . IMAGES : [ None , None , 3 ], DatasetField . IMAGES_INFO : [ 2 ] }, { BoxField . BOXES : [ None , 4 ], BoxField . LABELS : [ None ], BoxField . NUM_BOXES : [ 1 ], BoxField . WEIGHTS : [ None ] }) data = tf . data . Dataset . from_tensor_slices ( inputs ) data = data . padded_batch ( batch_size , padded_shape ) data = data . prefetch ( tf . data . experimental . AUTOTUNE ) mirrored_strategy = tf . distribute . MirroredStrategy () with mirrored_strategy . scope (): model = factory . build_model ( num_classes ) base_lr = 0.02 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( data , epochs = 2 , callbacks = [ ModelCheckpoint ( 'checkpoints' )]) Serving You can then use it in production with tensorflow model server . import requests from kerod.core.standard_fields import DatasetField url = 'https://my_server:XXX/v1/models/serving:predict' image = resize_to_min_dim ( inputs [ 'image' ], 800.0 , 1300.0 ) image_information = tf . cast ( tf . shape ( image )[: 2 ], dtype = tf . float32 ) # Will perform a query for a single batch but you can perform query on batch inputs = [ tf . expand_dims ( images , axis = 0 ) . numpy () . tolist (), tf . expand_dims ( image_information , axis = 0 ) . numpy () . tolist () ] headers = { \"content-type\" : \"application/json\" } response = requests . post ( url , data = json . dumps ( inputs ), headers = headers ) outputs = json . loads ( response . text )[ 'outputs' ] See the outputs of the predict_step of your. For FasterRCNN they will have a similar format: bbox : A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. scores : A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. label : A Tensor of shape [batch_size, max_detections] containing the class for boxes. num_boxes : A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings. Tests In order to run the tests you should install pytest. pip install pytest Here's the easiest way to run tests for the library: make test or pytest tests/","title":"Home"},{"location":"#features","text":"As powerful and concise as Keras Low barrier to entry for educators and practitioners Handle batch in training and inference Rich Documentation Multi-GPU Mixed_precision. You can try it with this notebook No need to struggle to download COCO or PascalVoc. We only used tensorflow_datasets Simple (again)","title":"Features"},{"location":"#algorithms","text":"[x] Feature Pyramidal Network [x] DeTr from: End to end object detection with transformers [x] Single scale SMCA (WIP not 100% the exact implementation of the paper): Fast Convergence of DETR with Spatially Modulated Co-Attention [ ] Mask-RCNN (Much of the pieces are already here just need to put everything together. It will arrive soon.)","title":"Algorithms"},{"location":"#try-kerod","text":"","title":"Try Kerod"},{"location":"#notebooks","text":"Training an algorithm on COCO or Pascal VOC has never been so easy. You just need to run the cells and everything will be done for you. You can find examples in the notebooks folder . There are no runners shipped with the library. Algorithm Dataset Performance MultiGPU Mixed Precision Notebook FasterRcnnFPNResnet50Pytorch PascalVoc FasterRcnnFPNResnet50Pytorch PascalVoc :heavy_check_mark: FasterRcnnFPNResnet50Pytorch COCO FasterRcnnFPNResnet50Pytorch COCO 30 mAP on old code base. A bug has been removed since. I need 4 GPUs to rerun the training. :heavy_check_mark: DetrResnet50Pytorch COCO NEED 16 GPUS for 3 days :heavy_check_mark: SMCAR50Pytorch COCO NEED 8 GPUS for 1 days :heavy_check_mark: Pytorch: means resnet implementation Pytorch style. In the residual block we have: conv (1x1) stride 1 -> conv (3x3) stride 2 instead of conv (1x1) stride 2 -> conv (3x3) stride 1 (Caffe, Keras implementations) If you want to perform an overfit you have an example with the detr architecture:","title":"Notebooks"},{"location":"#requirements","text":"If you don't run the examples on Colab please install tensorflow_datasets : pip install tensorflow_datasets","title":"Requirements"},{"location":"#no-configuration-file","text":"The code is (I hope) as simple as possible. You won't find any configuration file. All the parameters have already been chosen for you. If you need to change something simply code it and create a new layer. Why: In deep learning each parameter is important. You must think thoroughly before a change on how it will impact your model. Here, the code base is super simple just rewrite the blocks that you need and create new layers using the power of Keras. Also, it makes the code easier to read.","title":"No configuration file"},{"location":"#installation","text":"This repo is tested on Python 3.6, 3.7, 3.8 and TensorFlow 2.4.0 You may want to install 'kerod' in a virtual environment or with pyenv . Create a virtual environment with the version of Python you wanna use and activate it.","title":"Installation"},{"location":"#with-pip","text":"pip install git+https://github.com/EmGarr/kerod.git","title":"With pip"},{"location":"#from-source","text":"git clone https://github.com/EmGarr/kerod.git cd kerod pip install . When you update the repository, you should upgrade installation and its dependencies as follows: git pull pip install --upgrade . You can install the package in dev mode as follow and everytime you refresh the package it will be automatically updated: pip install -e .","title":"From source"},{"location":"#tutorials","text":"","title":"Tutorials"},{"location":"#simple-example","text":"To run a training you just need to write the following. import numpy as np from kerod.dataset.preprocessing import expand_dims_for_single_batch , preprocess from kerod.model import factory , KerodModel num_classes = 20 model = factory . build_model ( num_classes , name = KerodModel . faster_rcnn_resnet50_pytorch ) # Same format than COCO and Pascal VOC in tensorflow datasets inputs = { 'image' : np . zeros (( 2 , 100 , 50 , 3 )), 'objects' : { BoxField . BOXES : np . array ([[[ 0 , 0 , 1 , 1 ]], [[ 0 , 0 , 1 , 1 ]]], dtype = np . float32 ), BoxField . LABELS : np . array ([[ 1 ], [ 1 ]]) } } data = tf . data . Dataset . from_tensor_slices ( inputs ) data = data . map ( preprocess ) data = data . map ( expand_dims_for_single_batch ) base_lr = 0.02 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( data , epochs = 2 , callbacks = [ ModelCheckpoint ( 'checkpoints' )]) results = model . predict ( data , batch_size = 1 ) use_faster_rcnn = True if use_faster_rcnn : model . export_for_serving ( 'saved_model' ) else : model . save ( 'saved_model' ) reload_model = tf . keras . models . load_model ( 'saved_model' ) for x , _ in data : if use_faster_rcnn : reload_model . serving_step ( x [ DatasetField . IMAGES ], x [ DatasetField . IMAGES_INFO ]) else : reload_model . predict_step ( x )","title":"Simple example"},{"location":"#mixed-precision","text":"from tensorflow.keras.mixed_precision import experimental as mixed_precision from kerod.dataset.preprocessing import expand_dims_for_single_batch , preprocess from kerod.model import factory policy = mixed_precision . Policy ( 'mixed_float16' ) mixed_precision . set_policy ( policy ) num_classes = 20 model = factory . build_model ( num_classes )","title":"Mixed Precision"},{"location":"#multi-gpu-training","text":"import numpy as np from kerod.dataset.preprocessing import expand_dims_for_single_batch , preprocess from kerod.model import factory batch_size_per_gpu = 2 num_gpus = 8 batch_size = batch_size_per_gpu * num_gpus padded_shape = ({ DatasetField . IMAGES : [ None , None , 3 ], DatasetField . IMAGES_INFO : [ 2 ] }, { BoxField . BOXES : [ None , 4 ], BoxField . LABELS : [ None ], BoxField . NUM_BOXES : [ 1 ], BoxField . WEIGHTS : [ None ] }) data = tf . data . Dataset . from_tensor_slices ( inputs ) data = data . padded_batch ( batch_size , padded_shape ) data = data . prefetch ( tf . data . experimental . AUTOTUNE ) mirrored_strategy = tf . distribute . MirroredStrategy () with mirrored_strategy . scope (): model = factory . build_model ( num_classes ) base_lr = 0.02 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( data , epochs = 2 , callbacks = [ ModelCheckpoint ( 'checkpoints' )])","title":"Multi-GPU training"},{"location":"#serving","text":"You can then use it in production with tensorflow model server . import requests from kerod.core.standard_fields import DatasetField url = 'https://my_server:XXX/v1/models/serving:predict' image = resize_to_min_dim ( inputs [ 'image' ], 800.0 , 1300.0 ) image_information = tf . cast ( tf . shape ( image )[: 2 ], dtype = tf . float32 ) # Will perform a query for a single batch but you can perform query on batch inputs = [ tf . expand_dims ( images , axis = 0 ) . numpy () . tolist (), tf . expand_dims ( image_information , axis = 0 ) . numpy () . tolist () ] headers = { \"content-type\" : \"application/json\" } response = requests . post ( url , data = json . dumps ( inputs ), headers = headers ) outputs = json . loads ( response . text )[ 'outputs' ] See the outputs of the predict_step of your. For FasterRCNN they will have a similar format: bbox : A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. scores : A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. label : A Tensor of shape [batch_size, max_detections] containing the class for boxes. num_boxes : A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings.","title":"Serving"},{"location":"#tests","text":"In order to run the tests you should install pytest. pip install pytest Here's the easiest way to run tests for the library: make test or pytest tests/","title":"Tests"},{"location":"CONTRIBUTING/","text":"Contribution Tests make tests Developpers Code coverage Whenever you implement a new functionality you should reach a proper code coverage. Every pull requests will be rejected if the code coverage doesn't reach 90% . Docstring formatting The doc generation tool used is portray which handle markdown format def func ( a , b ): \"\"\"Describe my function Arguments: - *a*: A param a description - *b*: A param b description Returns: The sum of a + b Raises: (If exception are raised) Exceptions: 1 \"\"\" return a + b The code formatting used is yapf The config are automatically loaded from .style.yapf YAPF tries very hard to get the formatting correct. But for some code, it won't be as good as hand-formatting. In particular, large data literals may become horribly disfigured under YAPF. The reasons for this are manyfold. In short, YAPF is simply a tool to help with development. It will format things to coincide with the style guide, but that may not equate with readability. What can be done to alleviate this situation is to indicate regions YAPF should ignore when reformatting something: # yapf: disable FOO = { # ... some very large, complex data literal. } BAR = [ # ... another large data literal. ] # yapf: enable You can also disable formatting for a single literal like this: BAZ = { ( 1 , 2 , 3 , 4 ), ( 5 , 6 , 7 , 8 ), ( 9 , 10 , 11 , 12 ), } # yapf: disable In addition of this, it's recommended to have an automatic formatter of the imports. Imports of the same module should be imported together: from libs.fooo.resnet_v1_101 import ( create_resnet , support_utils_resnet_foo , upload_foo ) Imports should be structured in 3 parts, each separated by a blank line: import os # Base package import keras # External package from pip from libs import foo # import package inner modules Finally, it is prefered that the imports are sorted alphabetically.","title":"Contributing"},{"location":"CONTRIBUTING/#contribution","text":"","title":"Contribution"},{"location":"CONTRIBUTING/#tests","text":"make tests","title":"Tests"},{"location":"CONTRIBUTING/#developpers","text":"","title":"Developpers"},{"location":"CONTRIBUTING/#code-coverage","text":"Whenever you implement a new functionality you should reach a proper code coverage. Every pull requests will be rejected if the code coverage doesn't reach 90% .","title":"Code coverage"},{"location":"CONTRIBUTING/#docstring-formatting","text":"The doc generation tool used is portray which handle markdown format def func ( a , b ): \"\"\"Describe my function Arguments: - *a*: A param a description - *b*: A param b description Returns: The sum of a + b Raises: (If exception are raised) Exceptions: 1 \"\"\" return a + b","title":"Docstring formatting"},{"location":"CONTRIBUTING/#the-code-formatting-used-is-yapf","text":"The config are automatically loaded from .style.yapf YAPF tries very hard to get the formatting correct. But for some code, it won't be as good as hand-formatting. In particular, large data literals may become horribly disfigured under YAPF. The reasons for this are manyfold. In short, YAPF is simply a tool to help with development. It will format things to coincide with the style guide, but that may not equate with readability. What can be done to alleviate this situation is to indicate regions YAPF should ignore when reformatting something: # yapf: disable FOO = { # ... some very large, complex data literal. } BAR = [ # ... another large data literal. ] # yapf: enable You can also disable formatting for a single literal like this: BAZ = { ( 1 , 2 , 3 , 4 ), ( 5 , 6 , 7 , 8 ), ( 9 , 10 , 11 , 12 ), } # yapf: disable In addition of this, it's recommended to have an automatic formatter of the imports. Imports of the same module should be imported together: from libs.fooo.resnet_v1_101 import ( create_resnet , support_utils_resnet_foo , upload_foo ) Imports should be structured in 3 parts, each separated by a blank line: import os # Base package import keras # External package from pip from libs import foo # import package inner modules Finally, it is prefered that the imports are sorted alphabetically.","title":"The code formatting used is yapf"},{"location":"docs/BENCHMARKS/","text":"Benchmarks WIP Using this notebook on Colab using a P100: the code is running at 256ms/step","title":"Benchmarks"},{"location":"docs/BENCHMARKS/#benchmarks-wip","text":"Using this notebook on Colab using a P100: the code is running at 256ms/step","title":"Benchmarks WIP"},{"location":"reference/kerod/","text":"Module kerod None None Sub-modules kerod.core kerod.dataset kerod.layers kerod.model kerod.utils","title":"Index"},{"location":"reference/kerod/#module-kerod","text":"None None","title":"Module kerod"},{"location":"reference/kerod/#sub-modules","text":"kerod.core kerod.dataset kerod.layers kerod.model kerod.utils","title":"Sub-modules"},{"location":"reference/kerod/core/","text":"Module kerod.core None None Sub-modules kerod.core.box_coder kerod.core.box_ops kerod.core.constants kerod.core.learning_rate_schedule kerod.core.losses kerod.core.matcher kerod.core.sampling_ops kerod.core.similarity kerod.core.standard_fields kerod.core.target_assigner","title":"Index"},{"location":"reference/kerod/core/#module-kerodcore","text":"None None","title":"Module kerod.core"},{"location":"reference/kerod/core/#sub-modules","text":"kerod.core.box_coder kerod.core.box_ops kerod.core.constants kerod.core.learning_rate_schedule kerod.core.losses kerod.core.matcher kerod.core.sampling_ops kerod.core.similarity kerod.core.standard_fields kerod.core.target_assigner","title":"Sub-modules"},{"location":"reference/kerod/core/box_coder/","text":"Module kerod.core.box_coder None None View Source import tensorflow as tf from kerod.core import box_ops EPSILON = 1e-8 def encode_boxes_faster_rcnn ( boxes , anchors , scale_factors = None ): \"\"\"Encode a box collection with respect to anchor collection according to the [Faster RCNN paper](http://arxiv.org/abs/1506.01497). Faster RCNN box coder follows the coding schema described below: t_y = (y - y_a) / h_a t_x & = (x - x_a) / w_a t_h & = log(h / h_a) t_w & = log(w / w_a) where y, x h, w denote the box's center coordinates, width and height respectively. Similarly, y_a, x_a, h_a, w_a denote the anchor's center coordinates, width and height. t_y, t_x, t_h and t_w denote the anchor-encoded center, height and width respectively. Arguments: - *boxes*: BoxList holding N boxes to be encoded. - *anchors*: BoxList of anchors. - *scale_factors*: List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: A tensor representing N anchor-encoded boxes of the format [ty, tx, th, tw]. \"\"\" # Convert anchors to the center coordinate representation. anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) boxes = box_ops . convert_to_center_coordinates ( boxes ) ycenter , xcenter , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) # Avoid NaN in division and log below. ha += EPSILON wa += EPSILON h += EPSILON w += EPSILON ty = ( ycenter - ycenter_a ) / ha tx = ( xcenter - xcenter_a ) / wa th = tf . math . log ( h / ha ) tw = tf . math . log ( w / wa ) # Scales location targets as used in paper for joint training. if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty *= scale_factors [ 0 ] tx *= scale_factors [ 1 ] th *= scale_factors [ 2 ] tw *= scale_factors [ 3 ] return tf . concat ([ ty , tx , th , tw ], axis =- 1 ) def decode_boxes_faster_rcnn ( rel_codes , anchors , scale_factors = None ): \"\"\"Decode relative codes to boxes according to the [Faster RCNN paper](http://arxiv.org/abs/1506.01497). Faster RCNN box decoder follows the coding schema described below: ycent = t_y h_a + ycent_a xcent= t_x w_a + xcent_a h = exp(t_h) h_a w = exp(t_w) w_a where t_y, t_x, t_h, t_w denote the encoded box's center coordinates, width and height respectively. Similarly, ycent_a, xcent_a, h_a and w_a denote the anchor's center coordinates, width and height. ycent, xcent, h and w denote the anchor-encoded center, height and width respectively. Arguments: - *rel_codes*: a tensor representing N anchor-encoded boxes. - *anchors*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. - *scale_factors*: List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: - *boxes*: A Tensor of shape [N, ..., (y_max,x_max,y2,x2)]. \"\"\" anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) ty , tx , th , tw = tf . split ( value = rel_codes , num_or_size_splits = 4 , axis =- 1 ) if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty /= scale_factors [ 0 ] tx /= scale_factors [ 1 ] th /= scale_factors [ 2 ] tw /= scale_factors [ 3 ] ycenter = ty * ha + ycenter_a xcenter = tx * wa + xcenter_a h = tf . exp ( th ) * ha w = tf . exp ( tw ) * wa ymin = ycenter - h / 2. xmin = xcenter - w / 2. ymax = ycenter + h / 2. xmax = xcenter + w / 2. return tf . concat ([ ymin , xmin , ymax , xmax ], axis =- 1 ) Variables EPSILON Functions decode_boxes_faster_rcnn def decode_boxes_faster_rcnn ( rel_codes , anchors , scale_factors = None ) Decode relative codes to boxes according to the Faster RCNN paper . Faster RCNN box decoder follows the coding schema described below: ycent = t_y h_a + ycent_a xcent= t_x w_a + xcent_a h = exp(t_h) h_a w = exp(t_w) w_a where t_y, t_x, t_h, t_w denote the encoded box's center coordinates, width and height respectively. Similarly, ycent_a, xcent_a, h_a and w_a denote the anchor's center coordinates, width and height. ycent, xcent, h and w denote the anchor-encoded center, height and width respectively. Arguments: rel_codes : a tensor representing N anchor-encoded boxes. anchors : Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. scale_factors : List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: boxes : A Tensor of shape [N, ..., (y_max,x_max,y2,x2)]. View Source def decode_boxes_faster_rcnn ( rel_codes , anchors , scale_factors = None ) : \"\"\" Decode relative codes to boxes according to the [ Faster RCNN paper ] ( http : // arxiv . org / abs / 1506 . 01497 ) . Faster RCNN box decoder follows the coding schema described below : ycent = t_y h_a + ycent_a xcent = t_x w_a + xcent_a h = exp ( t_h ) h_a w = exp ( t_w ) w_a where t_y , t_x , t_h , t_w denote the encoded box ' s center coordinates, width and height respectively . Similarly , ycent_a , xcent_a , h_a and w_a denote the anchor ' s center coordinates , width and height . ycent , xcent , h and w denote the anchor - encoded center , height and width respectively . Arguments : - * rel_codes * : a tensor representing N anchor - encoded boxes . - * anchors * : Tensor of shape [ N , ..., ( y_min , x_min , y_max , x_max ) ]. - * scale_factors * : List of 4 positive scalars to scale ty , tx , th and tw . If set to None , does not perform scaling . For Faster RCNN , the open - source implementation recommends using [ 10 . 0 , 10 . 0 , 5 . 0 , 5 . 0 ]. Returns : - * boxes * : A Tensor of shape [ N , ..., ( y_max , x_max , y2 , x2 ) ]. \"\"\" anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) ty , tx , th , tw = tf . split ( value = rel_codes , num_or_size_splits = 4 , axis =- 1 ) if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty /= scale_factors [ 0 ] tx /= scale_factors [ 1 ] th /= scale_factors [ 2 ] tw /= scale_factors [ 3 ] ycenter = ty * ha + ycenter_a xcenter = tx * wa + xcenter_a h = tf . exp ( th ) * ha w = tf . exp ( tw ) * wa ymin = ycenter - h / 2 . xmin = xcenter - w / 2 . ymax = ycenter + h / 2 . xmax = xcenter + w / 2 . return tf . concat ( [ ymin , xmin , ymax , xmax ], axis =- 1 ) encode_boxes_faster_rcnn def encode_boxes_faster_rcnn ( boxes , anchors , scale_factors = None ) Encode a box collection with respect to anchor collection according to the Faster RCNN paper . Faster RCNN box coder follows the coding schema described below: t_y = (y - y_a) / h_a t_x & = (x - x_a) / w_a t_h & = log(h / h_a) t_w & = log(w / w_a) where y, x h, w denote the box's center coordinates, width and height respectively. Similarly, y_a, x_a, h_a, w_a denote the anchor's center coordinates, width and height. t_y, t_x, t_h and t_w denote the anchor-encoded center, height and width respectively. Arguments: boxes : BoxList holding N boxes to be encoded. anchors : BoxList of anchors. scale_factors : List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: A tensor representing N anchor-encoded boxes of the format [ty, tx, th, tw]. View Source def encode_boxes_faster_rcnn ( boxes , anchors , scale_factors = None ) : \"\"\" Encode a box collection with respect to anchor collection according to the [ Faster RCNN paper ] ( http : // arxiv . org / abs / 1506 . 01497 ) . Faster RCNN box coder follows the coding schema described below : t_y = ( y - y_a ) / h_a t_x & = ( x - x_a ) / w_a t_h & = log ( h / h_a ) t_w & = log ( w / w_a ) where y , x h , w denote the box ' s center coordinates, width and height respectively . Similarly , y_a , x_a , h_a , w_a denote the anchor ' s center coordinates , width and height . t_y , t_x , t_h and t_w denote the anchor - encoded center , height and width respectively . Arguments : - * boxes * : BoxList holding N boxes to be encoded . - * anchors * : BoxList of anchors . - * scale_factors * : List of 4 positive scalars to scale ty , tx , th and tw . If set to None , does not perform scaling . For Faster RCNN , the open - source implementation recommends using [ 10 . 0 , 10 . 0 , 5 . 0 , 5 . 0 ]. Returns : A tensor representing N anchor - encoded boxes of the format [ ty , tx , th , tw ]. \"\"\" # Convert anchors to the center coordinate representation . anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) boxes = box_ops . convert_to_center_coordinates ( boxes ) ycenter , xcenter , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) # Avoid NaN in division and log below . ha += EPSILON wa += EPSILON h += EPSILON w += EPSILON ty = ( ycenter - ycenter_a ) / ha tx = ( xcenter - xcenter_a ) / wa th = tf . math . log ( h / ha ) tw = tf . math . log ( w / wa ) # Scales location targets as used in paper for joint training . if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty *= scale_factors [ 0 ] tx *= scale_factors [ 1 ] th *= scale_factors [ 2 ] tw *= scale_factors [ 3 ] return tf . concat ( [ ty , tx , th , tw ], axis =- 1 )","title":"Box Coder"},{"location":"reference/kerod/core/box_coder/#module-kerodcorebox_coder","text":"None None View Source import tensorflow as tf from kerod.core import box_ops EPSILON = 1e-8 def encode_boxes_faster_rcnn ( boxes , anchors , scale_factors = None ): \"\"\"Encode a box collection with respect to anchor collection according to the [Faster RCNN paper](http://arxiv.org/abs/1506.01497). Faster RCNN box coder follows the coding schema described below: t_y = (y - y_a) / h_a t_x & = (x - x_a) / w_a t_h & = log(h / h_a) t_w & = log(w / w_a) where y, x h, w denote the box's center coordinates, width and height respectively. Similarly, y_a, x_a, h_a, w_a denote the anchor's center coordinates, width and height. t_y, t_x, t_h and t_w denote the anchor-encoded center, height and width respectively. Arguments: - *boxes*: BoxList holding N boxes to be encoded. - *anchors*: BoxList of anchors. - *scale_factors*: List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: A tensor representing N anchor-encoded boxes of the format [ty, tx, th, tw]. \"\"\" # Convert anchors to the center coordinate representation. anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) boxes = box_ops . convert_to_center_coordinates ( boxes ) ycenter , xcenter , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) # Avoid NaN in division and log below. ha += EPSILON wa += EPSILON h += EPSILON w += EPSILON ty = ( ycenter - ycenter_a ) / ha tx = ( xcenter - xcenter_a ) / wa th = tf . math . log ( h / ha ) tw = tf . math . log ( w / wa ) # Scales location targets as used in paper for joint training. if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty *= scale_factors [ 0 ] tx *= scale_factors [ 1 ] th *= scale_factors [ 2 ] tw *= scale_factors [ 3 ] return tf . concat ([ ty , tx , th , tw ], axis =- 1 ) def decode_boxes_faster_rcnn ( rel_codes , anchors , scale_factors = None ): \"\"\"Decode relative codes to boxes according to the [Faster RCNN paper](http://arxiv.org/abs/1506.01497). Faster RCNN box decoder follows the coding schema described below: ycent = t_y h_a + ycent_a xcent= t_x w_a + xcent_a h = exp(t_h) h_a w = exp(t_w) w_a where t_y, t_x, t_h, t_w denote the encoded box's center coordinates, width and height respectively. Similarly, ycent_a, xcent_a, h_a and w_a denote the anchor's center coordinates, width and height. ycent, xcent, h and w denote the anchor-encoded center, height and width respectively. Arguments: - *rel_codes*: a tensor representing N anchor-encoded boxes. - *anchors*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. - *scale_factors*: List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: - *boxes*: A Tensor of shape [N, ..., (y_max,x_max,y2,x2)]. \"\"\" anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) ty , tx , th , tw = tf . split ( value = rel_codes , num_or_size_splits = 4 , axis =- 1 ) if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty /= scale_factors [ 0 ] tx /= scale_factors [ 1 ] th /= scale_factors [ 2 ] tw /= scale_factors [ 3 ] ycenter = ty * ha + ycenter_a xcenter = tx * wa + xcenter_a h = tf . exp ( th ) * ha w = tf . exp ( tw ) * wa ymin = ycenter - h / 2. xmin = xcenter - w / 2. ymax = ycenter + h / 2. xmax = xcenter + w / 2. return tf . concat ([ ymin , xmin , ymax , xmax ], axis =- 1 )","title":"Module kerod.core.box_coder"},{"location":"reference/kerod/core/box_coder/#variables","text":"EPSILON","title":"Variables"},{"location":"reference/kerod/core/box_coder/#functions","text":"","title":"Functions"},{"location":"reference/kerod/core/box_coder/#decode_boxes_faster_rcnn","text":"def decode_boxes_faster_rcnn ( rel_codes , anchors , scale_factors = None ) Decode relative codes to boxes according to the Faster RCNN paper . Faster RCNN box decoder follows the coding schema described below: ycent = t_y h_a + ycent_a xcent= t_x w_a + xcent_a h = exp(t_h) h_a w = exp(t_w) w_a where t_y, t_x, t_h, t_w denote the encoded box's center coordinates, width and height respectively. Similarly, ycent_a, xcent_a, h_a and w_a denote the anchor's center coordinates, width and height. ycent, xcent, h and w denote the anchor-encoded center, height and width respectively. Arguments: rel_codes : a tensor representing N anchor-encoded boxes. anchors : Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. scale_factors : List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: boxes : A Tensor of shape [N, ..., (y_max,x_max,y2,x2)]. View Source def decode_boxes_faster_rcnn ( rel_codes , anchors , scale_factors = None ) : \"\"\" Decode relative codes to boxes according to the [ Faster RCNN paper ] ( http : // arxiv . org / abs / 1506 . 01497 ) . Faster RCNN box decoder follows the coding schema described below : ycent = t_y h_a + ycent_a xcent = t_x w_a + xcent_a h = exp ( t_h ) h_a w = exp ( t_w ) w_a where t_y , t_x , t_h , t_w denote the encoded box ' s center coordinates, width and height respectively . Similarly , ycent_a , xcent_a , h_a and w_a denote the anchor ' s center coordinates , width and height . ycent , xcent , h and w denote the anchor - encoded center , height and width respectively . Arguments : - * rel_codes * : a tensor representing N anchor - encoded boxes . - * anchors * : Tensor of shape [ N , ..., ( y_min , x_min , y_max , x_max ) ]. - * scale_factors * : List of 4 positive scalars to scale ty , tx , th and tw . If set to None , does not perform scaling . For Faster RCNN , the open - source implementation recommends using [ 10 . 0 , 10 . 0 , 5 . 0 , 5 . 0 ]. Returns : - * boxes * : A Tensor of shape [ N , ..., ( y_max , x_max , y2 , x2 ) ]. \"\"\" anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) ty , tx , th , tw = tf . split ( value = rel_codes , num_or_size_splits = 4 , axis =- 1 ) if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty /= scale_factors [ 0 ] tx /= scale_factors [ 1 ] th /= scale_factors [ 2 ] tw /= scale_factors [ 3 ] ycenter = ty * ha + ycenter_a xcenter = tx * wa + xcenter_a h = tf . exp ( th ) * ha w = tf . exp ( tw ) * wa ymin = ycenter - h / 2 . xmin = xcenter - w / 2 . ymax = ycenter + h / 2 . xmax = xcenter + w / 2 . return tf . concat ( [ ymin , xmin , ymax , xmax ], axis =- 1 )","title":"decode_boxes_faster_rcnn"},{"location":"reference/kerod/core/box_coder/#encode_boxes_faster_rcnn","text":"def encode_boxes_faster_rcnn ( boxes , anchors , scale_factors = None ) Encode a box collection with respect to anchor collection according to the Faster RCNN paper . Faster RCNN box coder follows the coding schema described below: t_y = (y - y_a) / h_a t_x & = (x - x_a) / w_a t_h & = log(h / h_a) t_w & = log(w / w_a) where y, x h, w denote the box's center coordinates, width and height respectively. Similarly, y_a, x_a, h_a, w_a denote the anchor's center coordinates, width and height. t_y, t_x, t_h and t_w denote the anchor-encoded center, height and width respectively. Arguments: boxes : BoxList holding N boxes to be encoded. anchors : BoxList of anchors. scale_factors : List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: A tensor representing N anchor-encoded boxes of the format [ty, tx, th, tw]. View Source def encode_boxes_faster_rcnn ( boxes , anchors , scale_factors = None ) : \"\"\" Encode a box collection with respect to anchor collection according to the [ Faster RCNN paper ] ( http : // arxiv . org / abs / 1506 . 01497 ) . Faster RCNN box coder follows the coding schema described below : t_y = ( y - y_a ) / h_a t_x & = ( x - x_a ) / w_a t_h & = log ( h / h_a ) t_w & = log ( w / w_a ) where y , x h , w denote the box ' s center coordinates, width and height respectively . Similarly , y_a , x_a , h_a , w_a denote the anchor ' s center coordinates , width and height . t_y , t_x , t_h and t_w denote the anchor - encoded center , height and width respectively . Arguments : - * boxes * : BoxList holding N boxes to be encoded . - * anchors * : BoxList of anchors . - * scale_factors * : List of 4 positive scalars to scale ty , tx , th and tw . If set to None , does not perform scaling . For Faster RCNN , the open - source implementation recommends using [ 10 . 0 , 10 . 0 , 5 . 0 , 5 . 0 ]. Returns : A tensor representing N anchor - encoded boxes of the format [ ty , tx , th , tw ]. \"\"\" # Convert anchors to the center coordinate representation . anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) boxes = box_ops . convert_to_center_coordinates ( boxes ) ycenter , xcenter , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) # Avoid NaN in division and log below . ha += EPSILON wa += EPSILON h += EPSILON w += EPSILON ty = ( ycenter - ycenter_a ) / ha tx = ( xcenter - xcenter_a ) / wa th = tf . math . log ( h / ha ) tw = tf . math . log ( w / wa ) # Scales location targets as used in paper for joint training . if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty *= scale_factors [ 0 ] tx *= scale_factors [ 1 ] th *= scale_factors [ 2 ] tw *= scale_factors [ 3 ] return tf . concat ( [ ty , tx , th , tw ], axis =- 1 )","title":"encode_boxes_faster_rcnn"},{"location":"reference/kerod/core/box_ops/","text":"Module kerod.core.box_ops None None View Source import tensorflow as tf def transform_fpcoor_for_tf ( boxes : tf . Tensor , tensor_shape : tuple , crop_shape : tuple ) -> tf . Tensor : \"\"\"The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize This function has been taken from tensorpack: (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Arguments: - *normalized_boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. These boxes have already been normalized in the feature space. The coordinates are not in the input image space. - *tensor_shape*: Height and width respectively - *crop_shape*: Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( boxes , 4 , axis =- 1 ) spacing_h = ( y_max - y_min ) / tf . cast ( crop_shape [ 0 ], boxes . dtype ) spacing_w = ( x_max - x_min ) / tf . cast ( crop_shape [ 1 ], boxes . dtype ) tensor_shape = ( tf . cast ( tensor_shape [ 0 ] - 1 , boxes . dtype ), tf . cast ( tensor_shape [ 1 ] - 1 , boxes . dtype )) ny0 = ( y_min + spacing_h / 2 - 0.5 ) / tensor_shape [ 0 ] nx0 = ( x_min + spacing_w / 2 - 0.5 ) / tensor_shape [ 1 ] nh = spacing_h * tf . cast ( crop_shape [ 0 ] - 1 , boxes . dtype ) / tensor_shape [ 0 ] nw = spacing_w * tf . cast ( crop_shape [ 1 ] - 1 , boxes . dtype ) / tensor_shape [ 1 ] return tf . concat ([ ny0 , nx0 , ny0 + nh , nx0 + nw ], axis =- 1 ) def convert_to_center_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_min, x_min, y_max, x_max -> y_cent, x_cent, h, w Arguments: - *boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] Returns: A tensor of shape [N, ..., num_boxes, (ycenter, xcenter, height, width)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) width = x_max - x_min height = y_max - y_min ycenter = y_min + height / 2. xcenter = x_min + width / 2. return tf . concat ([ ycenter , xcenter , height , width ], axis =- 1 ) def convert_to_xyxy_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_cent, x_cent, h, w -> y_min, x_min, y_max, x_max Arguments: - *boxes*: A Tensor of shape [N, ..., (y_cent, x_cent, h, w)] Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_cent , x_cent , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_cent - 0.5 * h x_min = x_cent - 0.5 * w y_max = y_cent + 0.5 * h x_max = x_cent + 0.5 * w return tf . concat ([ y_min , x_min , y_max , x_max ], axis =- 1 ) def compute_area ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Compute the area of boxes. Arguments: - *boxes*: Tensor of shape [N, ..., (y_min,x_min,y_max_,x_max)] Returns: A tensor of shape [N, ..., num_boxes] \"\"\" with tf . name_scope ( 'Area' ): y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) return tf . squeeze (( y_max - y_min ) * ( x_max - x_min ), - 1 ) def compute_intersection ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , perm = None ) -> tf . Tensor : \"\"\"Compute pairwise intersection areas between boxes. Arguments: - *boxes1*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] - *boxes2*: Tensor of shape [N, ..., (y_max,x_max,y_max,x_max)] Returns: A tensor with shape [N, M] representing pairwise intersections \"\"\" with tf . name_scope ( 'Intersection' ): y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( value = boxes1 , num_or_size_splits = 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( value = boxes2 , num_or_size_splits = 4 , axis =- 1 ) all_pairs_min_ymax = tf . minimum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) all_pairs_max_ymin = tf . maximum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) intersect_heights = tf . maximum ( zero , all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = tf . minimum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) all_pairs_max_xmin = tf . maximum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) intersect_widths = tf . maximum ( zero , all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths def compute_iou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor ) -> tf . Tensor : \"\"\"Computes pairwise intersection-over-union between boxes. Example: The axis x correspond to boxes2 and y the boxes1: ```python from kerod.core.box_ops import compute_iou import numpy as np boxes1 = np.array([[548.26666, 364.57202, 706.1333 , 524.472 ], [473.6 , 547.924 , 565.3333 , 635.336 ], [477.86664, 688.63605, 580.26666, 786.70795], [497.06668, 750.464 , 576. , 857.064 ]]) boxes2 = np.array([[474.74518, 553.37256, 565.2548 , 598.62744], [448., 736., 576., 864.], [464., 672., 592., 800.], [560., 368., 688., 496.] ]) compute_iou(boxes1, boxes2) ``` output ``` <tf.Tensor: shape=(4, 4), dtype=float64, numpy= array([[0. , 0. , 0. , 0.6490545 ], [0.51081317, 0. , 0. , 0. ], [0. , 0.23198337, 0.61294949, 0. ], [0. , 0.51356762, 0.18718853, 0. ]])> ``` Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] Returns: Return: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" return compute_giou ( boxes1 , boxes2 , mode = 'iou' ) def compute_giou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , mode : str = \"giou\" ) -> tf . Tensor : \"\"\"Computes pairwise general intersection-over-union between boxes following: https://giou.stanford.edu/ Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] - *mode*: You can select iou or giou. Returns: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" with tf . name_scope ( mode . upper ()): if len ( boxes1 . shape ) == 2 : perm = None which_dim_expands = 0 elif len ( boxes1 . shape ) == 3 : perm = ( 0 , 2 , 1 ) which_dim_expands = 1 else : raise ValueError ( 'Compute Iou is only suppoted for 2D and 3D Tensor' ) intersections = compute_intersection ( boxes1 , boxes2 , perm = perm ) areas1 = compute_area ( boxes1 ) areas2 = compute_area ( boxes2 ) unions = areas1 [ ... , None ] + tf . expand_dims ( areas2 , which_dim_expands ) - intersections iou = tf . where ( intersections == 0 , tf . zeros_like ( intersections ), tf . truediv ( intersections , unions )) if mode == \"iou\" : return iou y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( boxes1 , 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( boxes2 , 4 , axis =- 1 ) enclose_ymin = tf . minimum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) enclose_xmin = tf . minimum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) enclose_ymax = tf . maximum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) enclose_xmax = tf . maximum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) enclose_width = tf . maximum ( zero , enclose_xmax - enclose_xmin ) enclose_height = tf . maximum ( zero , enclose_ymax - enclose_ymin ) enclose_area = enclose_width * enclose_height giou = iou - tf . math . divide_no_nan (( enclose_area - unions ), enclose_area ) return giou def normalize_box_coordinates ( boxes , height : int , width : int ): \"\"\" Normalize the boxes coordinates with image shape Arguments: - *boxes*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] - *height*: An integer - *width*: An integer \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_min / height x_min = x_min / width y_max = y_max / height x_max = x_max / width # Won't be backpropagated to rois anyway, but to save time boxes = tf . stop_gradient ( tf . concat ([ y_min , x_min , y_max , x_max ], axis =- 1 )) return boxes def clip_boxes ( boxes : tf . Tensor , window : tf . Tensor ) -> tf . Tensor : \"\"\"Perform a clipping according to a window on the boxes. Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] - *window*: A tensor of shape [batch_size, (h, w)] Returns: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" boxes = tf . maximum ( boxes , tf . cast ( 0 , boxes . dtype )) m = tf . tile ( tf . expand_dims ( window , axis = 1 ), [ 1 , 1 , 2 ]) boxes = tf . minimum ( boxes , tf . cast ( m , boxes . dtype )) return boxes def flip_left_right ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"[Taken from tensorflow models] Left-right flip the boxes. Arguments: - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Return: Flipped boxes. \"\"\" ymin , xmin , ymax , xmax = tf . split ( value = boxes , num_or_size_splits = 4 , axis = 1 ) flipped_xmin = tf . subtract ( 1.0 , xmax ) flipped_xmax = tf . subtract ( 1.0 , xmin ) flipped_boxes = tf . concat ([ ymin , flipped_xmin , ymax , flipped_xmax ], 1 ) return flipped_boxes Functions clip_boxes def clip_boxes ( boxes : tensorflow . python . framework . ops . Tensor , window : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Perform a clipping according to a window on the boxes. Arguments: boxes : A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] window : A tensor of shape [batch_size, (h, w)] Returns: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] View Source def clip_boxes ( boxes : tf . Tensor , window : tf . Tensor ) -> tf . Tensor : \"\"\"Perform a clipping according to a window on the boxes. Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] - *window*: A tensor of shape [batch_size, (h, w)] Returns: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" boxes = tf . maximum ( boxes , tf . cast ( 0 , boxes . dtype )) m = tf . tile ( tf . expand_dims ( window , axis = 1 ), [ 1 , 1 , 2 ]) boxes = tf . minimum ( boxes , tf . cast ( m , boxes . dtype )) return boxes compute_area def compute_area ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Compute the area of boxes. Arguments: boxes : Tensor of shape [N, ..., (y_min,x_min,y_max_,x_max)] Returns: A tensor of shape [N, ..., num_boxes] View Source def compute_area ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Compute the area of boxes. Arguments: - *boxes*: Tensor of shape [N, ..., (y_min,x_min,y_max_,x_max)] Returns: A tensor of shape [N, ..., num_boxes] \"\"\" with tf . name_scope ( 'Area' ) : y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) return tf . squeeze (( y_max - y_min ) * ( x_max - x_min ), - 1 ) compute_giou def compute_giou ( boxes1 : tensorflow . python . framework . ops . Tensor , boxes2 : tensorflow . python . framework . ops . Tensor , mode : str = 'giou' ) -> tensorflow . python . framework . ops . Tensor Computes pairwise general intersection-over-union between boxes following: https://giou.stanford.edu/ Arguments: boxes1 : A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] boxes2 : A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] mode : You can select iou or giou. Returns: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. View Source def compute_giou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , mode : str = \"giou\" ) -> tf . Tensor : \"\"\"Computes pairwise general intersection-over-union between boxes following: https://giou.stanford.edu/ Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] - *mode*: You can select iou or giou. Returns: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" with tf . name_scope ( mode . upper ()) : if len ( boxes1 . shape ) == 2 : perm = None which_dim_expands = 0 elif len ( boxes1 . shape ) == 3 : perm = ( 0 , 2 , 1 ) which_dim_expands = 1 else : raise ValueError ( 'Compute Iou is only suppoted for 2D and 3D Tensor' ) intersections = compute_intersection ( boxes1 , boxes2 , perm = perm ) areas1 = compute_area ( boxes1 ) areas2 = compute_area ( boxes2 ) unions = areas1 [..., None ] + tf . expand_dims ( areas2 , which_dim_expands ) - intersections iou = tf . where ( intersections == 0 , tf . zeros_like ( intersections ), tf . truediv ( intersections , unions )) if mode == \"iou\" : return iou y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( boxes1 , 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( boxes2 , 4 , axis =- 1 ) enclose_ymin = tf . minimum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) enclose_xmin = tf . minimum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) enclose_ymax = tf . maximum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) enclose_xmax = tf . maximum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) enclose_width = tf . maximum ( zero , enclose_xmax - enclose_xmin ) enclose_height = tf . maximum ( zero , enclose_ymax - enclose_ymin ) enclose_area = enclose_width * enclose_height giou = iou - tf . math . divide_no_nan (( enclose_area - unions ), enclose_area ) return giou compute_intersection def compute_intersection ( boxes1 : tensorflow . python . framework . ops . Tensor , boxes2 : tensorflow . python . framework . ops . Tensor , perm = None ) -> tensorflow . python . framework . ops . Tensor Compute pairwise intersection areas between boxes. Arguments: boxes1 : Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] boxes2 : Tensor of shape [N, ..., (y_max,x_max,y_max,x_max)] Returns: A tensor with shape [N, M] representing pairwise intersections View Source def compute_intersection ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , perm = None ) -> tf . Tensor : \"\"\"Compute pairwise intersection areas between boxes. Arguments: - *boxes1*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] - *boxes2*: Tensor of shape [N, ..., (y_max,x_max,y_max,x_max)] Returns: A tensor with shape [N, M] representing pairwise intersections \"\"\" with tf . name_scope ( 'Intersection' ) : y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( value = boxes1 , num_or_size_splits = 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( value = boxes2 , num_or_size_splits = 4 , axis =- 1 ) all_pairs_min_ymax = tf . minimum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) all_pairs_max_ymin = tf . maximum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) intersect_heights = tf . maximum ( zero , all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = tf . minimum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) all_pairs_max_xmin = tf . maximum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) intersect_widths = tf . maximum ( zero , all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths compute_iou def compute_iou ( boxes1 : tensorflow . python . framework . ops . Tensor , boxes2 : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Computes pairwise intersection-over-union between boxes. Example: The axis x correspond to boxes2 and y the boxes1: ```python from kerod.core.box_ops import compute_iou import numpy as np boxes1 = np.array([[548.26666, 364.57202, 706.1333 , 524.472 ], [473.6 , 547.924 , 565.3333 , 635.336 ], [477.86664, 688.63605, 580.26666, 786.70795], [497.06668, 750.464 , 576. , 857.064 ]]) boxes2 = np.array([[474.74518, 553.37256, 565.2548 , 598.62744], [448., 736., 576., 864.], [464., 672., 592., 800.], [560., 368., 688., 496.] ]) compute_iou(boxes1, boxes2) ``` output <tf.Tensor: shape=(4, 4), dtype=float64, numpy= array([[0. , 0. , 0. , 0.6490545 ], [0.51081317, 0. , 0. , 0. ], [0. , 0.23198337, 0.61294949, 0. ], [0. , 0.51356762, 0.18718853, 0. ]])> Arguments: boxes1 : A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] boxes2 : A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] Returns: Return: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. View Source def compute_iou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor ) -> tf . Tensor : \"\"\"Computes pairwise intersection-over-union between boxes. Example: The axis x correspond to boxes2 and y the boxes1: ```python from kerod.core.box_ops import compute_iou import numpy as np boxes1 = np.array([[548.26666, 364.57202, 706.1333 , 524.472 ], [473.6 , 547.924 , 565.3333 , 635.336 ], [477.86664, 688.63605, 580.26666, 786.70795], [497.06668, 750.464 , 576. , 857.064 ]]) boxes2 = np.array([[474.74518, 553.37256, 565.2548 , 598.62744], [448., 736., 576., 864.], [464., 672., 592., 800.], [560., 368., 688., 496.] ]) compute_iou(boxes1, boxes2) ``` output ``` <tf.Tensor: shape=(4, 4), dtype=float64, numpy= array([[0. , 0. , 0. , 0.6490545 ], [0.51081317, 0. , 0. , 0. ], [0. , 0.23198337, 0.61294949, 0. ], [0. , 0.51356762, 0.18718853, 0. ]])> ``` Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] Returns: Return: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" return compute_giou ( boxes1 , boxes2 , mode = 'iou' ) convert_to_center_coordinates def convert_to_center_coordinates ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Convert boxes to their center coordinates y_min, x_min, y_max, x_max -> y_cent, x_cent, h, w Arguments: boxes : A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] Returns: A tensor of shape [N, ..., num_boxes, (ycenter, xcenter, height, width)] View Source def convert_to_center_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_min, x_min, y_max, x_max -> y_cent, x_cent, h, w Arguments: - *boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] Returns: A tensor of shape [N, ..., num_boxes, (ycenter, xcenter, height, width)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) width = x_max - x_min height = y_max - y_min ycenter = y_min + height / 2. xcenter = x_min + width / 2. return tf . concat ([ ycenter , xcenter , height , width ], axis =- 1 ) convert_to_xyxy_coordinates def convert_to_xyxy_coordinates ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Convert boxes to their center coordinates y_cent, x_cent, h, w -> y_min, x_min, y_max, x_max Arguments: boxes : A Tensor of shape [N, ..., (y_cent, x_cent, h, w)] Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] View Source def convert_to_xyxy_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_cent, x_cent, h, w -> y_min, x_min, y_max, x_max Arguments: - *boxes*: A Tensor of shape [N, ..., (y_cent, x_cent, h, w)] Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_cent , x_cent , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_cent - 0.5 * h x_min = x_cent - 0.5 * w y_max = y_cent + 0.5 * h x_max = x_cent + 0.5 * w return tf . concat ([ y_min , x_min , y_max , x_max ], axis =- 1 ) flip_left_right def flip_left_right ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor [Taken from tensorflow models] Left-right flip the boxes. Arguments: boxes : rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Return: Flipped boxes. View Source def flip_left_right ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"[Taken from tensorflow models] Left-right flip the boxes. Arguments: - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Return: Flipped boxes. \"\"\" ymin , xmin , ymax , xmax = tf . split ( value = boxes , num_or_size_splits = 4 , axis = 1 ) flipped_xmin = tf . subtract ( 1.0 , xmax ) flipped_xmax = tf . subtract ( 1.0 , xmin ) flipped_boxes = tf . concat ([ ymin , flipped_xmin , ymax , flipped_xmax ], 1 ) return flipped_boxes normalize_box_coordinates def normalize_box_coordinates ( boxes , height : int , width : int ) Normalize the boxes coordinates with image shape Arguments: boxes : Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] height : An integer width : An integer View Source def normalize_box_coordinates ( boxes , height : int , width : int ) : \"\"\" Normalize the boxes coordinates with image shape Arguments : - * boxes * : Tensor of shape [ N , ..., ( y_min , x_min , y_max , x_max ) ] - * height * : An integer - * width * : An integer \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_min / height x_min = x_min / width y_max = y_max / height x_max = x_max / width # Won ' t be backpropagated to rois anyway, but to save time boxes = tf . stop_gradient ( tf . concat ( [ y_min , x_min , y_max , x_max ], axis =- 1 )) return boxes transform_fpcoor_for_tf def transform_fpcoor_for_tf ( boxes : tensorflow . python . framework . ops . Tensor , tensor_shape : tuple , crop_shape : tuple ) -> tensorflow . python . framework . ops . Tensor The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize This function has been taken from tensorpack: (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Arguments: normalized_boxes : A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. These boxes have already been normalized in the feature space. The coordinates are not in the input image space. tensor_shape : Height and width respectively crop_shape : Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] View Source def transform_fpcoor_for_tf ( boxes : tf . Tensor , tensor_shape : tuple , crop_shape : tuple ) -> tf . Tensor : \"\"\"The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize This function has been taken from tensorpack: (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Arguments: - *normalized_boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. These boxes have already been normalized in the feature space. The coordinates are not in the input image space. - *tensor_shape*: Height and width respectively - *crop_shape*: Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( boxes , 4 , axis =- 1 ) spacing_h = ( y_max - y_min ) / tf . cast ( crop_shape [ 0 ], boxes . dtype ) spacing_w = ( x_max - x_min ) / tf . cast ( crop_shape [ 1 ], boxes . dtype ) tensor_shape = ( tf . cast ( tensor_shape [ 0 ] - 1 , boxes . dtype ), tf . cast ( tensor_shape [ 1 ] - 1 , boxes . dtype )) ny0 = ( y_min + spacing_h / 2 - 0.5 ) / tensor_shape [ 0 ] nx0 = ( x_min + spacing_w / 2 - 0.5 ) / tensor_shape [ 1 ] nh = spacing_h * tf . cast ( crop_shape [ 0 ] - 1 , boxes . dtype ) / tensor_shape [ 0 ] nw = spacing_w * tf . cast ( crop_shape [ 1 ] - 1 , boxes . dtype ) / tensor_shape [ 1 ] return tf . concat ([ ny0 , nx0 , ny0 + nh , nx0 + nw ], axis =- 1 )","title":"Box Ops"},{"location":"reference/kerod/core/box_ops/#module-kerodcorebox_ops","text":"None None View Source import tensorflow as tf def transform_fpcoor_for_tf ( boxes : tf . Tensor , tensor_shape : tuple , crop_shape : tuple ) -> tf . Tensor : \"\"\"The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize This function has been taken from tensorpack: (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Arguments: - *normalized_boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. These boxes have already been normalized in the feature space. The coordinates are not in the input image space. - *tensor_shape*: Height and width respectively - *crop_shape*: Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( boxes , 4 , axis =- 1 ) spacing_h = ( y_max - y_min ) / tf . cast ( crop_shape [ 0 ], boxes . dtype ) spacing_w = ( x_max - x_min ) / tf . cast ( crop_shape [ 1 ], boxes . dtype ) tensor_shape = ( tf . cast ( tensor_shape [ 0 ] - 1 , boxes . dtype ), tf . cast ( tensor_shape [ 1 ] - 1 , boxes . dtype )) ny0 = ( y_min + spacing_h / 2 - 0.5 ) / tensor_shape [ 0 ] nx0 = ( x_min + spacing_w / 2 - 0.5 ) / tensor_shape [ 1 ] nh = spacing_h * tf . cast ( crop_shape [ 0 ] - 1 , boxes . dtype ) / tensor_shape [ 0 ] nw = spacing_w * tf . cast ( crop_shape [ 1 ] - 1 , boxes . dtype ) / tensor_shape [ 1 ] return tf . concat ([ ny0 , nx0 , ny0 + nh , nx0 + nw ], axis =- 1 ) def convert_to_center_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_min, x_min, y_max, x_max -> y_cent, x_cent, h, w Arguments: - *boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] Returns: A tensor of shape [N, ..., num_boxes, (ycenter, xcenter, height, width)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) width = x_max - x_min height = y_max - y_min ycenter = y_min + height / 2. xcenter = x_min + width / 2. return tf . concat ([ ycenter , xcenter , height , width ], axis =- 1 ) def convert_to_xyxy_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_cent, x_cent, h, w -> y_min, x_min, y_max, x_max Arguments: - *boxes*: A Tensor of shape [N, ..., (y_cent, x_cent, h, w)] Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_cent , x_cent , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_cent - 0.5 * h x_min = x_cent - 0.5 * w y_max = y_cent + 0.5 * h x_max = x_cent + 0.5 * w return tf . concat ([ y_min , x_min , y_max , x_max ], axis =- 1 ) def compute_area ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Compute the area of boxes. Arguments: - *boxes*: Tensor of shape [N, ..., (y_min,x_min,y_max_,x_max)] Returns: A tensor of shape [N, ..., num_boxes] \"\"\" with tf . name_scope ( 'Area' ): y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) return tf . squeeze (( y_max - y_min ) * ( x_max - x_min ), - 1 ) def compute_intersection ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , perm = None ) -> tf . Tensor : \"\"\"Compute pairwise intersection areas between boxes. Arguments: - *boxes1*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] - *boxes2*: Tensor of shape [N, ..., (y_max,x_max,y_max,x_max)] Returns: A tensor with shape [N, M] representing pairwise intersections \"\"\" with tf . name_scope ( 'Intersection' ): y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( value = boxes1 , num_or_size_splits = 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( value = boxes2 , num_or_size_splits = 4 , axis =- 1 ) all_pairs_min_ymax = tf . minimum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) all_pairs_max_ymin = tf . maximum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) intersect_heights = tf . maximum ( zero , all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = tf . minimum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) all_pairs_max_xmin = tf . maximum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) intersect_widths = tf . maximum ( zero , all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths def compute_iou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor ) -> tf . Tensor : \"\"\"Computes pairwise intersection-over-union between boxes. Example: The axis x correspond to boxes2 and y the boxes1: ```python from kerod.core.box_ops import compute_iou import numpy as np boxes1 = np.array([[548.26666, 364.57202, 706.1333 , 524.472 ], [473.6 , 547.924 , 565.3333 , 635.336 ], [477.86664, 688.63605, 580.26666, 786.70795], [497.06668, 750.464 , 576. , 857.064 ]]) boxes2 = np.array([[474.74518, 553.37256, 565.2548 , 598.62744], [448., 736., 576., 864.], [464., 672., 592., 800.], [560., 368., 688., 496.] ]) compute_iou(boxes1, boxes2) ``` output ``` <tf.Tensor: shape=(4, 4), dtype=float64, numpy= array([[0. , 0. , 0. , 0.6490545 ], [0.51081317, 0. , 0. , 0. ], [0. , 0.23198337, 0.61294949, 0. ], [0. , 0.51356762, 0.18718853, 0. ]])> ``` Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] Returns: Return: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" return compute_giou ( boxes1 , boxes2 , mode = 'iou' ) def compute_giou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , mode : str = \"giou\" ) -> tf . Tensor : \"\"\"Computes pairwise general intersection-over-union between boxes following: https://giou.stanford.edu/ Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] - *mode*: You can select iou or giou. Returns: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" with tf . name_scope ( mode . upper ()): if len ( boxes1 . shape ) == 2 : perm = None which_dim_expands = 0 elif len ( boxes1 . shape ) == 3 : perm = ( 0 , 2 , 1 ) which_dim_expands = 1 else : raise ValueError ( 'Compute Iou is only suppoted for 2D and 3D Tensor' ) intersections = compute_intersection ( boxes1 , boxes2 , perm = perm ) areas1 = compute_area ( boxes1 ) areas2 = compute_area ( boxes2 ) unions = areas1 [ ... , None ] + tf . expand_dims ( areas2 , which_dim_expands ) - intersections iou = tf . where ( intersections == 0 , tf . zeros_like ( intersections ), tf . truediv ( intersections , unions )) if mode == \"iou\" : return iou y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( boxes1 , 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( boxes2 , 4 , axis =- 1 ) enclose_ymin = tf . minimum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) enclose_xmin = tf . minimum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) enclose_ymax = tf . maximum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) enclose_xmax = tf . maximum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) enclose_width = tf . maximum ( zero , enclose_xmax - enclose_xmin ) enclose_height = tf . maximum ( zero , enclose_ymax - enclose_ymin ) enclose_area = enclose_width * enclose_height giou = iou - tf . math . divide_no_nan (( enclose_area - unions ), enclose_area ) return giou def normalize_box_coordinates ( boxes , height : int , width : int ): \"\"\" Normalize the boxes coordinates with image shape Arguments: - *boxes*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] - *height*: An integer - *width*: An integer \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_min / height x_min = x_min / width y_max = y_max / height x_max = x_max / width # Won't be backpropagated to rois anyway, but to save time boxes = tf . stop_gradient ( tf . concat ([ y_min , x_min , y_max , x_max ], axis =- 1 )) return boxes def clip_boxes ( boxes : tf . Tensor , window : tf . Tensor ) -> tf . Tensor : \"\"\"Perform a clipping according to a window on the boxes. Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] - *window*: A tensor of shape [batch_size, (h, w)] Returns: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" boxes = tf . maximum ( boxes , tf . cast ( 0 , boxes . dtype )) m = tf . tile ( tf . expand_dims ( window , axis = 1 ), [ 1 , 1 , 2 ]) boxes = tf . minimum ( boxes , tf . cast ( m , boxes . dtype )) return boxes def flip_left_right ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"[Taken from tensorflow models] Left-right flip the boxes. Arguments: - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Return: Flipped boxes. \"\"\" ymin , xmin , ymax , xmax = tf . split ( value = boxes , num_or_size_splits = 4 , axis = 1 ) flipped_xmin = tf . subtract ( 1.0 , xmax ) flipped_xmax = tf . subtract ( 1.0 , xmin ) flipped_boxes = tf . concat ([ ymin , flipped_xmin , ymax , flipped_xmax ], 1 ) return flipped_boxes","title":"Module kerod.core.box_ops"},{"location":"reference/kerod/core/box_ops/#functions","text":"","title":"Functions"},{"location":"reference/kerod/core/box_ops/#clip_boxes","text":"def clip_boxes ( boxes : tensorflow . python . framework . ops . Tensor , window : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Perform a clipping according to a window on the boxes. Arguments: boxes : A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] window : A tensor of shape [batch_size, (h, w)] Returns: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] View Source def clip_boxes ( boxes : tf . Tensor , window : tf . Tensor ) -> tf . Tensor : \"\"\"Perform a clipping according to a window on the boxes. Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] - *window*: A tensor of shape [batch_size, (h, w)] Returns: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" boxes = tf . maximum ( boxes , tf . cast ( 0 , boxes . dtype )) m = tf . tile ( tf . expand_dims ( window , axis = 1 ), [ 1 , 1 , 2 ]) boxes = tf . minimum ( boxes , tf . cast ( m , boxes . dtype )) return boxes","title":"clip_boxes"},{"location":"reference/kerod/core/box_ops/#compute_area","text":"def compute_area ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Compute the area of boxes. Arguments: boxes : Tensor of shape [N, ..., (y_min,x_min,y_max_,x_max)] Returns: A tensor of shape [N, ..., num_boxes] View Source def compute_area ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Compute the area of boxes. Arguments: - *boxes*: Tensor of shape [N, ..., (y_min,x_min,y_max_,x_max)] Returns: A tensor of shape [N, ..., num_boxes] \"\"\" with tf . name_scope ( 'Area' ) : y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) return tf . squeeze (( y_max - y_min ) * ( x_max - x_min ), - 1 )","title":"compute_area"},{"location":"reference/kerod/core/box_ops/#compute_giou","text":"def compute_giou ( boxes1 : tensorflow . python . framework . ops . Tensor , boxes2 : tensorflow . python . framework . ops . Tensor , mode : str = 'giou' ) -> tensorflow . python . framework . ops . Tensor Computes pairwise general intersection-over-union between boxes following: https://giou.stanford.edu/ Arguments: boxes1 : A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] boxes2 : A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] mode : You can select iou or giou. Returns: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. View Source def compute_giou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , mode : str = \"giou\" ) -> tf . Tensor : \"\"\"Computes pairwise general intersection-over-union between boxes following: https://giou.stanford.edu/ Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] - *mode*: You can select iou or giou. Returns: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" with tf . name_scope ( mode . upper ()) : if len ( boxes1 . shape ) == 2 : perm = None which_dim_expands = 0 elif len ( boxes1 . shape ) == 3 : perm = ( 0 , 2 , 1 ) which_dim_expands = 1 else : raise ValueError ( 'Compute Iou is only suppoted for 2D and 3D Tensor' ) intersections = compute_intersection ( boxes1 , boxes2 , perm = perm ) areas1 = compute_area ( boxes1 ) areas2 = compute_area ( boxes2 ) unions = areas1 [..., None ] + tf . expand_dims ( areas2 , which_dim_expands ) - intersections iou = tf . where ( intersections == 0 , tf . zeros_like ( intersections ), tf . truediv ( intersections , unions )) if mode == \"iou\" : return iou y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( boxes1 , 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( boxes2 , 4 , axis =- 1 ) enclose_ymin = tf . minimum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) enclose_xmin = tf . minimum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) enclose_ymax = tf . maximum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) enclose_xmax = tf . maximum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) enclose_width = tf . maximum ( zero , enclose_xmax - enclose_xmin ) enclose_height = tf . maximum ( zero , enclose_ymax - enclose_ymin ) enclose_area = enclose_width * enclose_height giou = iou - tf . math . divide_no_nan (( enclose_area - unions ), enclose_area ) return giou","title":"compute_giou"},{"location":"reference/kerod/core/box_ops/#compute_intersection","text":"def compute_intersection ( boxes1 : tensorflow . python . framework . ops . Tensor , boxes2 : tensorflow . python . framework . ops . Tensor , perm = None ) -> tensorflow . python . framework . ops . Tensor Compute pairwise intersection areas between boxes. Arguments: boxes1 : Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] boxes2 : Tensor of shape [N, ..., (y_max,x_max,y_max,x_max)] Returns: A tensor with shape [N, M] representing pairwise intersections View Source def compute_intersection ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , perm = None ) -> tf . Tensor : \"\"\"Compute pairwise intersection areas between boxes. Arguments: - *boxes1*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] - *boxes2*: Tensor of shape [N, ..., (y_max,x_max,y_max,x_max)] Returns: A tensor with shape [N, M] representing pairwise intersections \"\"\" with tf . name_scope ( 'Intersection' ) : y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( value = boxes1 , num_or_size_splits = 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( value = boxes2 , num_or_size_splits = 4 , axis =- 1 ) all_pairs_min_ymax = tf . minimum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) all_pairs_max_ymin = tf . maximum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) intersect_heights = tf . maximum ( zero , all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = tf . minimum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) all_pairs_max_xmin = tf . maximum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) intersect_widths = tf . maximum ( zero , all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths","title":"compute_intersection"},{"location":"reference/kerod/core/box_ops/#compute_iou","text":"def compute_iou ( boxes1 : tensorflow . python . framework . ops . Tensor , boxes2 : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Computes pairwise intersection-over-union between boxes. Example: The axis x correspond to boxes2 and y the boxes1: ```python from kerod.core.box_ops import compute_iou import numpy as np boxes1 = np.array([[548.26666, 364.57202, 706.1333 , 524.472 ], [473.6 , 547.924 , 565.3333 , 635.336 ], [477.86664, 688.63605, 580.26666, 786.70795], [497.06668, 750.464 , 576. , 857.064 ]]) boxes2 = np.array([[474.74518, 553.37256, 565.2548 , 598.62744], [448., 736., 576., 864.], [464., 672., 592., 800.], [560., 368., 688., 496.] ]) compute_iou(boxes1, boxes2) ``` output <tf.Tensor: shape=(4, 4), dtype=float64, numpy= array([[0. , 0. , 0. , 0.6490545 ], [0.51081317, 0. , 0. , 0. ], [0. , 0.23198337, 0.61294949, 0. ], [0. , 0.51356762, 0.18718853, 0. ]])> Arguments: boxes1 : A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] boxes2 : A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] Returns: Return: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. View Source def compute_iou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor ) -> tf . Tensor : \"\"\"Computes pairwise intersection-over-union between boxes. Example: The axis x correspond to boxes2 and y the boxes1: ```python from kerod.core.box_ops import compute_iou import numpy as np boxes1 = np.array([[548.26666, 364.57202, 706.1333 , 524.472 ], [473.6 , 547.924 , 565.3333 , 635.336 ], [477.86664, 688.63605, 580.26666, 786.70795], [497.06668, 750.464 , 576. , 857.064 ]]) boxes2 = np.array([[474.74518, 553.37256, 565.2548 , 598.62744], [448., 736., 576., 864.], [464., 672., 592., 800.], [560., 368., 688., 496.] ]) compute_iou(boxes1, boxes2) ``` output ``` <tf.Tensor: shape=(4, 4), dtype=float64, numpy= array([[0. , 0. , 0. , 0.6490545 ], [0.51081317, 0. , 0. , 0. ], [0. , 0.23198337, 0.61294949, 0. ], [0. , 0.51356762, 0.18718853, 0. ]])> ``` Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] Returns: Return: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" return compute_giou ( boxes1 , boxes2 , mode = 'iou' )","title":"compute_iou"},{"location":"reference/kerod/core/box_ops/#convert_to_center_coordinates","text":"def convert_to_center_coordinates ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Convert boxes to their center coordinates y_min, x_min, y_max, x_max -> y_cent, x_cent, h, w Arguments: boxes : A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] Returns: A tensor of shape [N, ..., num_boxes, (ycenter, xcenter, height, width)] View Source def convert_to_center_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_min, x_min, y_max, x_max -> y_cent, x_cent, h, w Arguments: - *boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] Returns: A tensor of shape [N, ..., num_boxes, (ycenter, xcenter, height, width)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) width = x_max - x_min height = y_max - y_min ycenter = y_min + height / 2. xcenter = x_min + width / 2. return tf . concat ([ ycenter , xcenter , height , width ], axis =- 1 )","title":"convert_to_center_coordinates"},{"location":"reference/kerod/core/box_ops/#convert_to_xyxy_coordinates","text":"def convert_to_xyxy_coordinates ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Convert boxes to their center coordinates y_cent, x_cent, h, w -> y_min, x_min, y_max, x_max Arguments: boxes : A Tensor of shape [N, ..., (y_cent, x_cent, h, w)] Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] View Source def convert_to_xyxy_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_cent, x_cent, h, w -> y_min, x_min, y_max, x_max Arguments: - *boxes*: A Tensor of shape [N, ..., (y_cent, x_cent, h, w)] Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_cent , x_cent , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_cent - 0.5 * h x_min = x_cent - 0.5 * w y_max = y_cent + 0.5 * h x_max = x_cent + 0.5 * w return tf . concat ([ y_min , x_min , y_max , x_max ], axis =- 1 )","title":"convert_to_xyxy_coordinates"},{"location":"reference/kerod/core/box_ops/#flip_left_right","text":"def flip_left_right ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor [Taken from tensorflow models] Left-right flip the boxes. Arguments: boxes : rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Return: Flipped boxes. View Source def flip_left_right ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"[Taken from tensorflow models] Left-right flip the boxes. Arguments: - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Return: Flipped boxes. \"\"\" ymin , xmin , ymax , xmax = tf . split ( value = boxes , num_or_size_splits = 4 , axis = 1 ) flipped_xmin = tf . subtract ( 1.0 , xmax ) flipped_xmax = tf . subtract ( 1.0 , xmin ) flipped_boxes = tf . concat ([ ymin , flipped_xmin , ymax , flipped_xmax ], 1 ) return flipped_boxes","title":"flip_left_right"},{"location":"reference/kerod/core/box_ops/#normalize_box_coordinates","text":"def normalize_box_coordinates ( boxes , height : int , width : int ) Normalize the boxes coordinates with image shape Arguments: boxes : Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] height : An integer width : An integer View Source def normalize_box_coordinates ( boxes , height : int , width : int ) : \"\"\" Normalize the boxes coordinates with image shape Arguments : - * boxes * : Tensor of shape [ N , ..., ( y_min , x_min , y_max , x_max ) ] - * height * : An integer - * width * : An integer \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_min / height x_min = x_min / width y_max = y_max / height x_max = x_max / width # Won ' t be backpropagated to rois anyway, but to save time boxes = tf . stop_gradient ( tf . concat ( [ y_min , x_min , y_max , x_max ], axis =- 1 )) return boxes","title":"normalize_box_coordinates"},{"location":"reference/kerod/core/box_ops/#transform_fpcoor_for_tf","text":"def transform_fpcoor_for_tf ( boxes : tensorflow . python . framework . ops . Tensor , tensor_shape : tuple , crop_shape : tuple ) -> tensorflow . python . framework . ops . Tensor The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize This function has been taken from tensorpack: (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Arguments: normalized_boxes : A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. These boxes have already been normalized in the feature space. The coordinates are not in the input image space. tensor_shape : Height and width respectively crop_shape : Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] View Source def transform_fpcoor_for_tf ( boxes : tf . Tensor , tensor_shape : tuple , crop_shape : tuple ) -> tf . Tensor : \"\"\"The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize This function has been taken from tensorpack: (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Arguments: - *normalized_boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. These boxes have already been normalized in the feature space. The coordinates are not in the input image space. - *tensor_shape*: Height and width respectively - *crop_shape*: Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( boxes , 4 , axis =- 1 ) spacing_h = ( y_max - y_min ) / tf . cast ( crop_shape [ 0 ], boxes . dtype ) spacing_w = ( x_max - x_min ) / tf . cast ( crop_shape [ 1 ], boxes . dtype ) tensor_shape = ( tf . cast ( tensor_shape [ 0 ] - 1 , boxes . dtype ), tf . cast ( tensor_shape [ 1 ] - 1 , boxes . dtype )) ny0 = ( y_min + spacing_h / 2 - 0.5 ) / tensor_shape [ 0 ] nx0 = ( x_min + spacing_w / 2 - 0.5 ) / tensor_shape [ 1 ] nh = spacing_h * tf . cast ( crop_shape [ 0 ] - 1 , boxes . dtype ) / tensor_shape [ 0 ] nw = spacing_w * tf . cast ( crop_shape [ 1 ] - 1 , boxes . dtype ) / tensor_shape [ 1 ] return tf . concat ([ ny0 , nx0 , ny0 + nh , nx0 + nw ], axis =- 1 )","title":"transform_fpcoor_for_tf"},{"location":"reference/kerod/core/constants/","text":"Module kerod.core.constants None None View Source # The max image dimension refers to the maximum size of an input image MAX_IMAGE_DIMENSION = 1600 Variables MAX_IMAGE_DIMENSION","title":"Constants"},{"location":"reference/kerod/core/constants/#module-kerodcoreconstants","text":"None None View Source # The max image dimension refers to the maximum size of an input image MAX_IMAGE_DIMENSION = 1600","title":"Module kerod.core.constants"},{"location":"reference/kerod/core/constants/#variables","text":"MAX_IMAGE_DIMENSION","title":"Variables"},{"location":"reference/kerod/core/learning_rate_schedule/","text":"Module kerod.core.learning_rate_schedule Manual Stepping is designed to integrate the computation graph and compute the learning_rate at each step. However, the WarmupLearningRateScheduler is Callback handle by the fit in keras. View Source \"\"\" Manual Stepping is designed to integrate the computation graph and compute the learning_rate at each step. However, the WarmupLearningRateScheduler is Callback handle by the fit in keras. \"\"\" from typing import List import tensorflow as tf from tensorflow.keras import backend as K from tensorflow.keras.optimizers.schedules import LearningRateSchedule from tensorflow.python.keras.callbacks import Callback class LearningRateScheduler ( Callback ): \"\"\"Warmup Learning rate scheduler. It will perform at the beginning of the training a linear warmup from `init_lr` to `base_lr`. The learning rate is decreased by 10 according to the schedule provided by `epochs`. Arguments: - *base_lr*: The target learning rate value after the linear warmup - *num_gpus*: Number of gpus used during the training - *epochs*: A list of epoch on which the learning rate should be reduce. - *use_warmup*: Perform the warmup strategy. - *init_lr*: Learning rate value from which the warmup will start. - *num_warmup_steps*: Number of training step on which the warmup will be performed. \"\"\" def __init__ ( self , base_lr : float , num_gpus : int , epochs : List [ int ], use_warmup : bool = True , init_lr : float = 1e-2 / 3 , num_warmup_steps : int = 1000 ): super () . __init__ () self . _init_lr = init_lr * min ( 8 / num_gpus , 1 ) self . slope = ( base_lr - self . _init_lr ) / num_warmup_steps self . _epochs_to_lr = { epoch : base_lr * 1 / 10 ** ( i + 1 ) for i , epoch in enumerate ( epochs )} self . _epochs = epochs self . _num_gpus = num_gpus self . _use_warmup = use_warmup self . _num_warmup_steps = num_warmup_steps def on_train_batch_begin ( self , batch , logs = None ): global_step = K . get_value (( self . model . optimizer . iterations )) if global_step <= self . _num_warmup_steps and global_step != 0 and self . _use_warmup : lr = self . _init_lr + global_step * self . slope K . set_value ( self . model . optimizer . lr , lr ) def on_epoch_begin ( self , epoch , logs = None ): if not hasattr ( self . model . optimizer , 'lr' ): raise ValueError ( 'Optimizer must have a \"lr\" attribute.' ) if not hasattr ( self . model . optimizer , 'iterations' ): raise ValueError ( 'Optimizer must have an \"iterations\" attribute.' ) global_step = K . get_value ( self . model . optimizer . iterations ) target_epochs = [ e for e in self . _epochs if epoch >= e and global_step > self . _num_warmup_steps ] if target_epochs : lr = self . _epochs_to_lr [ max ( target_epochs )] K . set_value ( self . model . optimizer . lr , lr ) class ManualStepping ( LearningRateSchedule ): \"\"\"Manually stepped learning rate schedule. (Taken and modified from Google object detection) This function provides fine grained control over learning rates. One must specify a sequence of learning rates as well as a set of integer steps at which the current learning rate must transition to the next. For example, if boundaries = [5, 10] and rates = [.1, .01, .001], then the learning rate returned by this function is .1 for step=0,...,4, .01 for step=5...9, and .001 for step=10 and onward. You can pass this schedule directly into a `tf.keras.optimizers.Optimizer` as the learning rate. ```python lr_schedule = tf.keras.optimizers.schedules.ManualStepping( boundaries=[5, 10], rates=[.1, .01, .001], warmup=True) model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule), loss='sparse_categorical_crossentropy', metrics=['accuracy']) model.fit(data, labels, epochs=5) ``` The learning rate schedule is also serializable and deserializable using `tf.keras.optimizers.schedules.serialize` and `tf.keras.optimizers.schedules.deserialize`. Arguments: - *boundaries*: A List of scalar `int32` or `int64` or a `Tensor`. It is a list of global steps at which to switch learning rates. This list is assumed to consist of increasing positive integers. - *rates*: a list of (float) learning rates corresponding to intervals between the boundaries. The length of this list must be exactly len(boundaries) + 1. - *warmup*: Whether to linearly interpolate learning rate for steps in [0, boundaries[0]]. - *name*: String. Optional name of the operation. Defaults to 'ExponentialDecay'. Return: A 1-arg callable learning rate schedule that takes the current optimizer step and outputs the decayed learning rate, a scalar `Tensor` of the same type as `rates`. \"\"\" def __init__ ( self , boundaries , rates , warmup = False , name = None ): super () . __init__ () self . name = name if warmup and boundaries : slope = ( rates [ 1 ] - rates [ 0 ]) * 1.0 / boundaries [ 0 ] warmup_steps = list ( range ( boundaries [ 0 ])) warmup_rates = [ rates [ 0 ] + slope * step for step in warmup_steps ] boundaries = warmup_steps + boundaries rates = warmup_rates + rates [ 1 :] else : boundaries = [ 0 ] + boundaries self . warmup = warmup self . rates = rates self . boundaries = boundaries self . num_boundaries = len ( boundaries ) self . dtype = tf . convert_to_tensor ( rates [ 0 ]) . dtype def __call__ ( self , step ): with tf . name_scope ( self . name or \"ManualStepping\" ): boundaries = tf . convert_to_tensor ( self . boundaries , self . dtype ) rates = tf . convert_to_tensor ( self . rates , self . dtype ) step = tf . convert_to_tensor ( step , self . dtype ) rate_index = tf . reduce_max ( tf . where ( tf . greater_equal ( step , boundaries ), list ( range ( self . num_boundaries )), [ 0 ] * self . num_boundaries )) return tf . reduce_sum ( rates * tf . one_hot ( rate_index , depth = self . num_boundaries )) def get_config ( self ): return { \"boundaries\" : self . boundaries , \"rates\" : self . rates , \"warmup\" : self . warmup , \"name\" : self . name } Classes LearningRateScheduler class LearningRateScheduler ( base_lr : float , num_gpus : int , epochs : List [ int ], use_warmup : bool = True , init_lr : float = 0.0033333333333333335 , num_warmup_steps : int = 1000 ) a linear warmup from init_lr to base_lr . The learning rate is decreased by 10 according to the schedule provided by epochs . Arguments: base_lr : The target learning rate value after the linear warmup num_gpus : Number of gpus used during the training epochs : A list of epoch on which the learning rate should be reduce. use_warmup : Perform the warmup strategy. init_lr : Learning rate value from which the warmup will start. num_warmup_steps : Number of training step on which the warmup will be performed. Ancestors (in MRO) tensorflow.python.keras.callbacks.Callback Methods on_batch_begin def on_batch_begin ( self , batch , logs = None ) A backwards compatibility alias for on_train_batch_begin . View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_batch_begin ( self , batch , logs = None ) : \" \"\" A backwards compatibility alias for `on_train_batch_begin`. \"\" \" on_batch_end def on_batch_end ( self , batch , logs = None ) A backwards compatibility alias for on_train_batch_end . View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_batch_end ( self , batch , logs = None ) : \" \"\" A backwards compatibility alias for `on_train_batch_end`. \"\" \" on_epoch_begin def on_epoch_begin ( self , epoch , logs = None ) View Source def on_epoch_begin ( self , epoch , logs = None ) : if not hasattr ( self . model . optimizer , ' lr ' ) : raise ValueError ( ' Optimizer must have a \"lr\" attribute. ' ) if not hasattr ( self . model . optimizer , ' iterations ' ) : raise ValueError ( ' Optimizer must have an \"iterations\" attribute. ' ) global_step = K . get_value ( self . model . optimizer . iterations ) target_epochs = [ e for e in self . _epochs if epoch >= e and global_step > self . _num_warmup_steps ] if target_epochs : lr = self . _epochs_to_lr [ max ( target_epochs ) ] K . set_value ( self . model . optimizer . lr , lr ) on_epoch_end def on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Description epoch Integer, index of epoch. logs Dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . For training epoch, the values of the Model 's metrics are returned. Example : {'loss': 0.2, 'accuracy':<br> 0.7} . View Source @doc_controls.for_subclass_implementers def on_epoch_end ( self , epoch , logs = None ) : \" \"\" Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Args: epoch: Integer, index of epoch. logs: Dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. For training epoch, the values of the `Model`'s metrics are returned. Example : `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" on_predict_batch_begin def on_predict_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict, contains the return value of model.predict_step , it typically returns a dict with a key 'outputs' containing the model's outputs. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_predict_batch_begin ( self , batch , logs = None ) : \" \"\" Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Args: batch: Integer, index of batch within the current epoch. logs: Dict, contains the return value of `model.predict_step`, it typically returns a dict with a key 'outputs' containing the model's outputs. \"\" \" on_predict_batch_end def on_predict_batch_end ( self , batch , logs = None ) Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict. Aggregated metric results up until this batch. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_predict_batch_end ( self , batch , logs = None ) : \" \"\" Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Args: batch: Integer, index of batch within the current epoch. logs: Dict. Aggregated metric results up until this batch. \"\" \" on_predict_begin def on_predict_begin ( self , logs = None ) Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_predict_begin ( self , logs = None ) : \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Args: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" on_predict_end def on_predict_end ( self , logs = None ) Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_predict_end ( self , logs = None ) : \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Args: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" on_test_batch_begin def on_test_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict, contains the return value of model.test_step . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_test_batch_begin ( self , batch , logs = None ) : \" \"\" Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Args: batch: Integer, index of batch within the current epoch. logs: Dict, contains the return value of `model.test_step`. Typically, the values of the `Model`'s metrics are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" on_test_batch_end def on_test_batch_end ( self , batch , logs = None ) Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict. Aggregated metric results up until this batch. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_test_batch_end ( self , batch , logs = None ) : \" \"\" Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Args: batch: Integer, index of batch within the current epoch. logs: Dict. Aggregated metric results up until this batch. \"\" \" on_test_begin def on_test_begin ( self , logs = None ) Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_test_begin ( self , logs = None ) : \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Args: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" on_test_end def on_test_end ( self , logs = None ) Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently the output of the last call to on_test_batch_end() is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_test_end ( self , logs = None ) : \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Args: logs: Dict. Currently the output of the last call to `on_test_batch_end()` is passed to this argument for this method but that may change in the future. \"\"\" on_train_batch_begin def on_train_batch_begin ( self , batch , logs = None ) View Source def on_train_batch_begin ( self , batch , logs = None ) : global_step = K . get_value (( self . model . optimizer . iterations )) if global_step <= self . _num_warmup_steps and global_step != 0 and self . _use_warmup : lr = self . _init_lr + global_step * self . slope K . set_value ( self . model . optimizer . lr , lr ) on_train_batch_end def on_train_batch_end ( self , batch , logs = None ) Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict. Aggregated metric results up until this batch. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_train_batch_end ( self , batch , logs = None ) : \" \"\" Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Args: batch: Integer, index of batch within the current epoch. logs: Dict. Aggregated metric results up until this batch. \"\" \" # For backwards compatibility. self . on_batch_end ( batch , logs = logs ) on_train_begin def on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_train_begin ( self , logs = None ) : \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Args: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" on_train_end def on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently the output of the last call to on_epoch_end() is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_train_end ( self , logs = None ) : \"\"\"Called at the end of training. Subclasses should override for any actions to run. Args: logs: Dict. Currently the output of the last call to `on_epoch_end()` is passed to this argument for this method but that may change in the future. \"\"\" set_model def set_model ( self , model ) View Source def set_model(self, model): self.model = model set_params def set_params ( self , params ) View Source def set_params(self, params): self.params = params ManualStepping class ManualStepping ( boundaries , rates , warmup = False , name = None ) This function provides fine grained control over learning rates. One must specify a sequence of learning rates as well as a set of integer steps at which the current learning rate must transition to the next. For example, if boundaries = [5, 10] and rates = [.1, .01, .001], then the learning rate returned by this function is .1 for step=0,...,4, .01 for step=5...9, and .001 for step=10 and onward. You can pass this schedule directly into a tf.keras.optimizers.Optimizer as the learning rate. lr_schedule = tf . keras . optimizers . schedules . ManualStepping ( boundaries = [ 5 , 10 ], rates = [ .1 , .01 , .001 ], warmup = True ) model . compile ( optimizer = tf . keras . optimizers . SGD ( learning_rate = lr_schedule ), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( data , labels , epochs = 5 ) The learning rate schedule is also serializable and deserializable using tf.keras.optimizers.schedules.serialize and tf.keras.optimizers.schedules.deserialize . Arguments: boundaries : A List of scalar int32 or int64 or a Tensor . It is a list of global steps at which to switch learning rates. This list is assumed to consist of increasing positive integers. rates : a list of (float) learning rates corresponding to intervals between the boundaries. The length of this list must be exactly len(boundaries) + 1. warmup : Whether to linearly interpolate learning rate for steps in [0, boundaries[0]]. name : String. Optional name of the operation. Defaults to 'ExponentialDecay'. Return: A 1-arg callable learning rate schedule that takes the current optimizer step and outputs the decayed learning rate, a scalar Tensor of the same type as rates . Ancestors (in MRO) keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule Static methods from_config def from_config ( config ) Instantiates a LearningRateSchedule from its config. Parameters: Name Description config Output of get_config() . Returns: Type Description None A LearningRateSchedule instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Instantiates a `LearningRateSchedule` from its config. Args: config: Output of `get_config()`. Returns: A `LearningRateSchedule` instance. \"\" \" return cls ( ** config ) Methods get_config def get_config ( self ) View Source def get_config ( self ) : return { \" boundaries \" : self . boundaries , \" rates \" : self . rates , \" warmup \" : self . warmup , \" name \" : self . name }","title":"Learning Rate Schedule"},{"location":"reference/kerod/core/learning_rate_schedule/#module-kerodcorelearning_rate_schedule","text":"Manual Stepping is designed to integrate the computation graph and compute the learning_rate at each step. However, the WarmupLearningRateScheduler is Callback handle by the fit in keras. View Source \"\"\" Manual Stepping is designed to integrate the computation graph and compute the learning_rate at each step. However, the WarmupLearningRateScheduler is Callback handle by the fit in keras. \"\"\" from typing import List import tensorflow as tf from tensorflow.keras import backend as K from tensorflow.keras.optimizers.schedules import LearningRateSchedule from tensorflow.python.keras.callbacks import Callback class LearningRateScheduler ( Callback ): \"\"\"Warmup Learning rate scheduler. It will perform at the beginning of the training a linear warmup from `init_lr` to `base_lr`. The learning rate is decreased by 10 according to the schedule provided by `epochs`. Arguments: - *base_lr*: The target learning rate value after the linear warmup - *num_gpus*: Number of gpus used during the training - *epochs*: A list of epoch on which the learning rate should be reduce. - *use_warmup*: Perform the warmup strategy. - *init_lr*: Learning rate value from which the warmup will start. - *num_warmup_steps*: Number of training step on which the warmup will be performed. \"\"\" def __init__ ( self , base_lr : float , num_gpus : int , epochs : List [ int ], use_warmup : bool = True , init_lr : float = 1e-2 / 3 , num_warmup_steps : int = 1000 ): super () . __init__ () self . _init_lr = init_lr * min ( 8 / num_gpus , 1 ) self . slope = ( base_lr - self . _init_lr ) / num_warmup_steps self . _epochs_to_lr = { epoch : base_lr * 1 / 10 ** ( i + 1 ) for i , epoch in enumerate ( epochs )} self . _epochs = epochs self . _num_gpus = num_gpus self . _use_warmup = use_warmup self . _num_warmup_steps = num_warmup_steps def on_train_batch_begin ( self , batch , logs = None ): global_step = K . get_value (( self . model . optimizer . iterations )) if global_step <= self . _num_warmup_steps and global_step != 0 and self . _use_warmup : lr = self . _init_lr + global_step * self . slope K . set_value ( self . model . optimizer . lr , lr ) def on_epoch_begin ( self , epoch , logs = None ): if not hasattr ( self . model . optimizer , 'lr' ): raise ValueError ( 'Optimizer must have a \"lr\" attribute.' ) if not hasattr ( self . model . optimizer , 'iterations' ): raise ValueError ( 'Optimizer must have an \"iterations\" attribute.' ) global_step = K . get_value ( self . model . optimizer . iterations ) target_epochs = [ e for e in self . _epochs if epoch >= e and global_step > self . _num_warmup_steps ] if target_epochs : lr = self . _epochs_to_lr [ max ( target_epochs )] K . set_value ( self . model . optimizer . lr , lr ) class ManualStepping ( LearningRateSchedule ): \"\"\"Manually stepped learning rate schedule. (Taken and modified from Google object detection) This function provides fine grained control over learning rates. One must specify a sequence of learning rates as well as a set of integer steps at which the current learning rate must transition to the next. For example, if boundaries = [5, 10] and rates = [.1, .01, .001], then the learning rate returned by this function is .1 for step=0,...,4, .01 for step=5...9, and .001 for step=10 and onward. You can pass this schedule directly into a `tf.keras.optimizers.Optimizer` as the learning rate. ```python lr_schedule = tf.keras.optimizers.schedules.ManualStepping( boundaries=[5, 10], rates=[.1, .01, .001], warmup=True) model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule), loss='sparse_categorical_crossentropy', metrics=['accuracy']) model.fit(data, labels, epochs=5) ``` The learning rate schedule is also serializable and deserializable using `tf.keras.optimizers.schedules.serialize` and `tf.keras.optimizers.schedules.deserialize`. Arguments: - *boundaries*: A List of scalar `int32` or `int64` or a `Tensor`. It is a list of global steps at which to switch learning rates. This list is assumed to consist of increasing positive integers. - *rates*: a list of (float) learning rates corresponding to intervals between the boundaries. The length of this list must be exactly len(boundaries) + 1. - *warmup*: Whether to linearly interpolate learning rate for steps in [0, boundaries[0]]. - *name*: String. Optional name of the operation. Defaults to 'ExponentialDecay'. Return: A 1-arg callable learning rate schedule that takes the current optimizer step and outputs the decayed learning rate, a scalar `Tensor` of the same type as `rates`. \"\"\" def __init__ ( self , boundaries , rates , warmup = False , name = None ): super () . __init__ () self . name = name if warmup and boundaries : slope = ( rates [ 1 ] - rates [ 0 ]) * 1.0 / boundaries [ 0 ] warmup_steps = list ( range ( boundaries [ 0 ])) warmup_rates = [ rates [ 0 ] + slope * step for step in warmup_steps ] boundaries = warmup_steps + boundaries rates = warmup_rates + rates [ 1 :] else : boundaries = [ 0 ] + boundaries self . warmup = warmup self . rates = rates self . boundaries = boundaries self . num_boundaries = len ( boundaries ) self . dtype = tf . convert_to_tensor ( rates [ 0 ]) . dtype def __call__ ( self , step ): with tf . name_scope ( self . name or \"ManualStepping\" ): boundaries = tf . convert_to_tensor ( self . boundaries , self . dtype ) rates = tf . convert_to_tensor ( self . rates , self . dtype ) step = tf . convert_to_tensor ( step , self . dtype ) rate_index = tf . reduce_max ( tf . where ( tf . greater_equal ( step , boundaries ), list ( range ( self . num_boundaries )), [ 0 ] * self . num_boundaries )) return tf . reduce_sum ( rates * tf . one_hot ( rate_index , depth = self . num_boundaries )) def get_config ( self ): return { \"boundaries\" : self . boundaries , \"rates\" : self . rates , \"warmup\" : self . warmup , \"name\" : self . name }","title":"Module kerod.core.learning_rate_schedule"},{"location":"reference/kerod/core/learning_rate_schedule/#classes","text":"","title":"Classes"},{"location":"reference/kerod/core/learning_rate_schedule/#learningratescheduler","text":"class LearningRateScheduler ( base_lr : float , num_gpus : int , epochs : List [ int ], use_warmup : bool = True , init_lr : float = 0.0033333333333333335 , num_warmup_steps : int = 1000 ) a linear warmup from init_lr to base_lr . The learning rate is decreased by 10 according to the schedule provided by epochs . Arguments: base_lr : The target learning rate value after the linear warmup num_gpus : Number of gpus used during the training epochs : A list of epoch on which the learning rate should be reduce. use_warmup : Perform the warmup strategy. init_lr : Learning rate value from which the warmup will start. num_warmup_steps : Number of training step on which the warmup will be performed.","title":"LearningRateScheduler"},{"location":"reference/kerod/core/learning_rate_schedule/#ancestors-in-mro","text":"tensorflow.python.keras.callbacks.Callback","title":"Ancestors (in MRO)"},{"location":"reference/kerod/core/learning_rate_schedule/#methods","text":"","title":"Methods"},{"location":"reference/kerod/core/learning_rate_schedule/#on_batch_begin","text":"def on_batch_begin ( self , batch , logs = None ) A backwards compatibility alias for on_train_batch_begin . View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_batch_begin ( self , batch , logs = None ) : \" \"\" A backwards compatibility alias for `on_train_batch_begin`. \"\" \"","title":"on_batch_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_batch_end","text":"def on_batch_end ( self , batch , logs = None ) A backwards compatibility alias for on_train_batch_end . View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_batch_end ( self , batch , logs = None ) : \" \"\" A backwards compatibility alias for `on_train_batch_end`. \"\" \"","title":"on_batch_end"},{"location":"reference/kerod/core/learning_rate_schedule/#on_epoch_begin","text":"def on_epoch_begin ( self , epoch , logs = None ) View Source def on_epoch_begin ( self , epoch , logs = None ) : if not hasattr ( self . model . optimizer , ' lr ' ) : raise ValueError ( ' Optimizer must have a \"lr\" attribute. ' ) if not hasattr ( self . model . optimizer , ' iterations ' ) : raise ValueError ( ' Optimizer must have an \"iterations\" attribute. ' ) global_step = K . get_value ( self . model . optimizer . iterations ) target_epochs = [ e for e in self . _epochs if epoch >= e and global_step > self . _num_warmup_steps ] if target_epochs : lr = self . _epochs_to_lr [ max ( target_epochs ) ] K . set_value ( self . model . optimizer . lr , lr )","title":"on_epoch_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_epoch_end","text":"def on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Description epoch Integer, index of epoch. logs Dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . For training epoch, the values of the Model 's metrics are returned. Example : {'loss': 0.2, 'accuracy':<br> 0.7} . View Source @doc_controls.for_subclass_implementers def on_epoch_end ( self , epoch , logs = None ) : \" \"\" Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Args: epoch: Integer, index of epoch. logs: Dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. For training epoch, the values of the `Model`'s metrics are returned. Example : `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \"","title":"on_epoch_end"},{"location":"reference/kerod/core/learning_rate_schedule/#on_predict_batch_begin","text":"def on_predict_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict, contains the return value of model.predict_step , it typically returns a dict with a key 'outputs' containing the model's outputs. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_predict_batch_begin ( self , batch , logs = None ) : \" \"\" Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Args: batch: Integer, index of batch within the current epoch. logs: Dict, contains the return value of `model.predict_step`, it typically returns a dict with a key 'outputs' containing the model's outputs. \"\" \"","title":"on_predict_batch_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_predict_batch_end","text":"def on_predict_batch_end ( self , batch , logs = None ) Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict. Aggregated metric results up until this batch. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_predict_batch_end ( self , batch , logs = None ) : \" \"\" Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Args: batch: Integer, index of batch within the current epoch. logs: Dict. Aggregated metric results up until this batch. \"\" \"","title":"on_predict_batch_end"},{"location":"reference/kerod/core/learning_rate_schedule/#on_predict_begin","text":"def on_predict_begin ( self , logs = None ) Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_predict_begin ( self , logs = None ) : \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Args: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\"","title":"on_predict_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_predict_end","text":"def on_predict_end ( self , logs = None ) Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_predict_end ( self , logs = None ) : \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Args: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\"","title":"on_predict_end"},{"location":"reference/kerod/core/learning_rate_schedule/#on_test_batch_begin","text":"def on_test_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict, contains the return value of model.test_step . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_test_batch_begin ( self , batch , logs = None ) : \" \"\" Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Args: batch: Integer, index of batch within the current epoch. logs: Dict, contains the return value of `model.test_step`. Typically, the values of the `Model`'s metrics are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \"","title":"on_test_batch_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_test_batch_end","text":"def on_test_batch_end ( self , batch , logs = None ) Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict. Aggregated metric results up until this batch. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_test_batch_end ( self , batch , logs = None ) : \" \"\" Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Args: batch: Integer, index of batch within the current epoch. logs: Dict. Aggregated metric results up until this batch. \"\" \"","title":"on_test_batch_end"},{"location":"reference/kerod/core/learning_rate_schedule/#on_test_begin","text":"def on_test_begin ( self , logs = None ) Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_test_begin ( self , logs = None ) : \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Args: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\"","title":"on_test_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_test_end","text":"def on_test_end ( self , logs = None ) Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently the output of the last call to on_test_batch_end() is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_test_end ( self , logs = None ) : \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Args: logs: Dict. Currently the output of the last call to `on_test_batch_end()` is passed to this argument for this method but that may change in the future. \"\"\"","title":"on_test_end"},{"location":"reference/kerod/core/learning_rate_schedule/#on_train_batch_begin","text":"def on_train_batch_begin ( self , batch , logs = None ) View Source def on_train_batch_begin ( self , batch , logs = None ) : global_step = K . get_value (( self . model . optimizer . iterations )) if global_step <= self . _num_warmup_steps and global_step != 0 and self . _use_warmup : lr = self . _init_lr + global_step * self . slope K . set_value ( self . model . optimizer . lr , lr )","title":"on_train_batch_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_train_batch_end","text":"def on_train_batch_end ( self , batch , logs = None ) Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict. Aggregated metric results up until this batch. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_train_batch_end ( self , batch , logs = None ) : \" \"\" Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Args: batch: Integer, index of batch within the current epoch. logs: Dict. Aggregated metric results up until this batch. \"\" \" # For backwards compatibility. self . on_batch_end ( batch , logs = logs )","title":"on_train_batch_end"},{"location":"reference/kerod/core/learning_rate_schedule/#on_train_begin","text":"def on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_train_begin ( self , logs = None ) : \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Args: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\"","title":"on_train_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_train_end","text":"def on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently the output of the last call to on_epoch_end() is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_train_end ( self , logs = None ) : \"\"\"Called at the end of training. Subclasses should override for any actions to run. Args: logs: Dict. Currently the output of the last call to `on_epoch_end()` is passed to this argument for this method but that may change in the future. \"\"\"","title":"on_train_end"},{"location":"reference/kerod/core/learning_rate_schedule/#set_model","text":"def set_model ( self , model ) View Source def set_model(self, model): self.model = model","title":"set_model"},{"location":"reference/kerod/core/learning_rate_schedule/#set_params","text":"def set_params ( self , params ) View Source def set_params(self, params): self.params = params","title":"set_params"},{"location":"reference/kerod/core/learning_rate_schedule/#manualstepping","text":"class ManualStepping ( boundaries , rates , warmup = False , name = None ) This function provides fine grained control over learning rates. One must specify a sequence of learning rates as well as a set of integer steps at which the current learning rate must transition to the next. For example, if boundaries = [5, 10] and rates = [.1, .01, .001], then the learning rate returned by this function is .1 for step=0,...,4, .01 for step=5...9, and .001 for step=10 and onward. You can pass this schedule directly into a tf.keras.optimizers.Optimizer as the learning rate. lr_schedule = tf . keras . optimizers . schedules . ManualStepping ( boundaries = [ 5 , 10 ], rates = [ .1 , .01 , .001 ], warmup = True ) model . compile ( optimizer = tf . keras . optimizers . SGD ( learning_rate = lr_schedule ), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( data , labels , epochs = 5 ) The learning rate schedule is also serializable and deserializable using tf.keras.optimizers.schedules.serialize and tf.keras.optimizers.schedules.deserialize . Arguments: boundaries : A List of scalar int32 or int64 or a Tensor . It is a list of global steps at which to switch learning rates. This list is assumed to consist of increasing positive integers. rates : a list of (float) learning rates corresponding to intervals between the boundaries. The length of this list must be exactly len(boundaries) + 1. warmup : Whether to linearly interpolate learning rate for steps in [0, boundaries[0]]. name : String. Optional name of the operation. Defaults to 'ExponentialDecay'. Return: A 1-arg callable learning rate schedule that takes the current optimizer step and outputs the decayed learning rate, a scalar Tensor of the same type as rates .","title":"ManualStepping"},{"location":"reference/kerod/core/learning_rate_schedule/#ancestors-in-mro_1","text":"keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule","title":"Ancestors (in MRO)"},{"location":"reference/kerod/core/learning_rate_schedule/#static-methods","text":"","title":"Static methods"},{"location":"reference/kerod/core/learning_rate_schedule/#from_config","text":"def from_config ( config ) Instantiates a LearningRateSchedule from its config. Parameters: Name Description config Output of get_config() . Returns: Type Description None A LearningRateSchedule instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Instantiates a `LearningRateSchedule` from its config. Args: config: Output of `get_config()`. Returns: A `LearningRateSchedule` instance. \"\" \" return cls ( ** config )","title":"from_config"},{"location":"reference/kerod/core/learning_rate_schedule/#methods_1","text":"","title":"Methods"},{"location":"reference/kerod/core/learning_rate_schedule/#get_config","text":"def get_config ( self ) View Source def get_config ( self ) : return { \" boundaries \" : self . boundaries , \" rates \" : self . rates , \" warmup \" : self . warmup , \" name \" : self . name }","title":"get_config"},{"location":"reference/kerod/core/losses/","text":"Module kerod.core.losses None None View Source import tensorflow as tf from tensorflow.keras.losses import Loss class L1Loss ( Loss ): def call ( self , y_true , y_pred ): return tf . norm ( y_true - y_pred , ord = 1 , axis =- 1 ) Classes L1Loss class L1Loss ( reduction = 'auto' , name = None ) To be implemented by subclasses: * call() : Contains the logic for loss calculation using y_true , y_pred . Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , y_true , y_pred ): y_pred = tf . convert_to_tensor_v2 ( y_pred ) y_true = tf . cast ( y_true , y_pred . dtype ) return tf . reduce_mean ( math_ops . square ( y_pred - y_true ), axis =- 1 ) When used with tf.distribute.Strategy , outside of built-in training loops such as tf.keras compile and fit , please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training tutorial for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: with strategy . scope (): loss_obj = tf . keras . losses . CategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE ) .... loss = ( tf . reduce_sum ( loss_obj ( labels , predictions )) * ( 1. / global_batch_size )) Ancestors (in MRO) keras.losses.Loss Static methods from_config def from_config ( config ) Instantiates a Loss from its config (output of get_config() ). Parameters: Name Description config Output of get_config() . Returns: Type Description None A Loss instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Instantiates a `Loss` from its config (output of `get_config()`). Args: config: Output of `get_config()`. Returns: A `Loss` instance. \"\" \" return cls ( ** config ) Methods call def call ( self , y_true , y_pred ) View Source def call ( self , y_true , y_pred ) : return tf . norm ( y_true - y_pred , ord = 1 , axis =- 1 ) get_config def get_config ( self ) Returns the config dictionary for a Loss instance. View Source def get_config ( self ) : \" \"\" Returns the config dictionary for a `Loss` instance. \"\" \" return { 'reduction' : self . reduction , 'name' : self . name }","title":"Losses"},{"location":"reference/kerod/core/losses/#module-kerodcorelosses","text":"None None View Source import tensorflow as tf from tensorflow.keras.losses import Loss class L1Loss ( Loss ): def call ( self , y_true , y_pred ): return tf . norm ( y_true - y_pred , ord = 1 , axis =- 1 )","title":"Module kerod.core.losses"},{"location":"reference/kerod/core/losses/#classes","text":"","title":"Classes"},{"location":"reference/kerod/core/losses/#l1loss","text":"class L1Loss ( reduction = 'auto' , name = None ) To be implemented by subclasses: * call() : Contains the logic for loss calculation using y_true , y_pred . Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , y_true , y_pred ): y_pred = tf . convert_to_tensor_v2 ( y_pred ) y_true = tf . cast ( y_true , y_pred . dtype ) return tf . reduce_mean ( math_ops . square ( y_pred - y_true ), axis =- 1 ) When used with tf.distribute.Strategy , outside of built-in training loops such as tf.keras compile and fit , please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training tutorial for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: with strategy . scope (): loss_obj = tf . keras . losses . CategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE ) .... loss = ( tf . reduce_sum ( loss_obj ( labels , predictions )) * ( 1. / global_batch_size ))","title":"L1Loss"},{"location":"reference/kerod/core/losses/#ancestors-in-mro","text":"keras.losses.Loss","title":"Ancestors (in MRO)"},{"location":"reference/kerod/core/losses/#static-methods","text":"","title":"Static methods"},{"location":"reference/kerod/core/losses/#from_config","text":"def from_config ( config ) Instantiates a Loss from its config (output of get_config() ). Parameters: Name Description config Output of get_config() . Returns: Type Description None A Loss instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Instantiates a `Loss` from its config (output of `get_config()`). Args: config: Output of `get_config()`. Returns: A `Loss` instance. \"\" \" return cls ( ** config )","title":"from_config"},{"location":"reference/kerod/core/losses/#methods","text":"","title":"Methods"},{"location":"reference/kerod/core/losses/#call","text":"def call ( self , y_true , y_pred ) View Source def call ( self , y_true , y_pred ) : return tf . norm ( y_true - y_pred , ord = 1 , axis =- 1 )","title":"call"},{"location":"reference/kerod/core/losses/#get_config","text":"def get_config ( self ) Returns the config dictionary for a Loss instance. View Source def get_config ( self ) : \" \"\" Returns the config dictionary for a `Loss` instance. \"\" \" return { 'reduction' : self . reduction , 'name' : self . name }","title":"get_config"},{"location":"reference/kerod/core/matcher/","text":"Module kerod.core.matcher None None View Source from typing import List import tensorflow as tf from kerod . utils import item_assignment from scipy . optimize import linear_sum_assignment class Matcher : \"\"\"This class assigns to each predicted \" element \" (e.g., a box) a ground-truth element. Each predicted element will have exactly zero or one matches; each ground-truth element may be matched to zero or more predicted elements. The matching is determined by the MxN match_quality_matrix, that characterizes how well each (ground-truth, prediction)-pair match each other. For example, if the elements are boxes, this matrix may contain box intersection-over-union overlap values. The matcher returns (a) a vector of length N containing the index of the ground-truth element m in [0, M) that matches to prediction n in [0, N). (b) a vector of length N containing the labels for each prediction. Arguments: - *thresholds*: a list of thresholds used to stratify predictions into levels. - *labels*: a list of values to label predictions belonging at each level. A label can be one of {-1, 0, 1} signifying {ignore, negative class, positive class}, respectively. - *allow_low_quality_matches*: if True, produce additional matches for predictions with maximum match quality lower than high_threshold. See set_low_quality_matches_ for more details. Example: thresholds = [0.3, 0.5] labels = [0, -1, 1] All predictions with iou < 0.3 will be marked with 0 and thus will be considered as false positives while training. All predictions with 0.3 <= iou < 0.5 will be marked with -1 and thus will be ignored. All predictions with 0.5 <= iou will be marked with 1 and thus will be considered as true positives. \"\"\" def __ init__ ( self , thresholds : List [ float ], labels : List [ int ], allow_low_quality_matches: bool = False ) : # Add - inf and + inf to first and last position in thresholds thresholds = thresholds [ : ] assert thresholds [ 0 ] > 0 thresholds . insert ( 0 , - float ( \"inf\" )) thresholds . append ( float ( \"inf\" )) assert all ( low <= high for ( low , high ) in zip ( thresholds [:- 1 ], thresholds [ 1 : ])) assert all ( l in [ - 1 , 0 , 1 ] for l in labels ) assert len ( labels ) == len ( thresholds ) - 1 self . thresholds = thresholds self . labels = labels self . allow_low_quality_matches = allow_low_quality_matches def __ call__ ( self , match_quality_matrix: tf . Tensor , num_valid_boxes: tf . Tensor ) : \"\"\" Arguments: - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. Returns: - *matches*: a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) \"\"\" assert len ( match_quality_matrix . shape ) == 3 num_valid_boxes = tf . squeeze ( num_valid_boxes , - 1 ) # match_quality_matrix is B ( batch ) x M ( gt ) x N ( predicted ) # Max over gt elements to find best gt candidate for each prediction matches = tf . argmax ( match_quality_matrix , axis = 1 , output_type = tf . int32 ) matched_vals = tf . math . reduce_max ( match_quality_matrix , axis = 1 ) # matched_vals , matches = match_quality_matrix . max ( dim = 0 ) match_labels = matches for ( l , low , high ) in zip ( self . labels , self . thresholds [:- 1 ], self . thresholds [ 1 : ]) : low_high = ( matched_vals >= low ) & ( matched_vals < high ) match_labels = item_assignment ( match_labels , low_high , l ) if self . allow_low_quality_matches: match_labels = self . _ set_low_quality_matches ( match_labels , match_quality_matrix , num_valid_boxes ) # Remove all the padded groundtruths # e . g : matches = [ 1 , 0 , 4 , 3 ] num_valid_boxes = [ 3 ] # mask_padded_boxes = [ 0 , 0 , 1 , 1 ] mask_padded_boxes = matches >= num_valid_boxes [ : , None ] # Will flag to - 1 all the padded boxes to avoid sampling them match_labels = item_assignment ( match_labels , mask_padded_boxes , - 1 ) return matches , match_labels def _ set_low_quality_matches ( self , match_labels , match_quality_matrix , num_valid_boxes ) : \"\"\" Produce additional matches for predictions that have only low-quality matches. Specifically, for each ground-truth G find the set of predictions that have maximum overlap with it (including ties); for each prediction in that set, if it is unmatched, then match it to the ground-truth G. This function implements the RPN assignment case (i) in Sec. 3.1.2 of the Faster R-CNN paper: https://arxiv.org/pdf/1506.01497v3.pdf. Arguments: - *match_labels*: a tensor of int8 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. \"\"\" # For each gt , find the prediction with which it has highest quality : shape [ batch_size , M ] highest_quality_gt = tf . math . reduce_max ( match_quality_matrix , axis = 2 ) # Find the highest quality match available , even if it is low , including ties . hq_foreach_gt_mask = match_quality_matrix == highest_quality_gt [..., None ] # Create a mask for the valid_boxes , shape = [ batch_size , M ] mask_valid_boxes = tf . sequence_mask ( num_valid_boxes , maxlen = tf . shape ( match_quality_matrix )[ 1 ], dtype = tf . bool ) hq_foreach_gt_mask = hq_foreach_gt_mask & mask_valid_boxes [..., None ] # If an anchor was labeled positive only due to a low - quality match # with gt_A , but it has larger overlap with gt_B , it's matched index will still be gt_B. # This follows the implementation in Detectron, and is found to have no significant impact. # shape = [batch_size, N] masks_for_labels = tf.reduce_max(tf.cast(hq_foreach_gt_mask, tf.int8), 1) match_labels = item_assignment(match_labels, masks_for_labels, 1) return match_labels def hungarian_matching(match_quality_matrix: tf.Tensor, num_valid_boxes: tf.Tensor): \"\"\"Find the maximum-weight matching of the match_quality_matrix. A maximum-weight matching is also a perfect matching. Arguments: - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. Returns: - *matches*: a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) \"\"\" def hungarian_assignment(cost_matrix): return tf.py_function(lambda c: linear_sum_assignment(c), [cost_matrix], Tout=(tf.int32, tf.int32)) # We do not want to match our padded ground_truth during the hungarian # assignment so we create a mask to increase the cost of fake ground_truth # [batch_size, M] mask = ~tf.sequence_mask(tf.squeeze(num_valid_boxes, 1), tf.shape(match_quality_matrix)[1]) # [batch_size, M, N] mask = tf.tile(mask[..., None], [1, 1, tf.shape(match_quality_matrix)[-1]]) # We set to inf all cost which corresponds to padding. # They won't interfere during the linear assignment match_quality_matrix = item_assignment ( match_quality_matrix , mask , 10 e5 ) # [ batch_size , num_gt_boxes ] indices = tf . vectorized_map ( hungarian_assignment , match_quality_matrix ) matches = tf . one_hot ( indices [ 1 ], tf . shape ( match_quality_matrix )[ - 1 ], dtype = tf . int32 ) match_labels = tf . cast ( tf . reduce_max ( matches , axis = 1 ), tf . int32 ) matches = tf . reduce_max ( matches * indices [ 0 ][..., None ], axis = 1 ) # Remove all the padded groundtruths # e . g : matches = [ 1 , 0 , 4 , 3 ] num_valid_boxes = [ 3 ] # mask_padded_boxes = [ 0 , 0 , 1 , 1 ] mask_padded_boxes = matches >= num_valid_boxes # Will flag to - 1 all the padded boxes to avoid sampling them match_labels = item_assignment ( match_labels , mask_padded_boxes , - 1 ) return tf . stop_gradient ( matches ), tf . stop_gradient ( match_labels ) Functions hungarian_matching def hungarian_matching ( match_quality_matrix : tensorflow . python . framework . ops . Tensor , num_valid_boxes : tensorflow . python . framework . ops . Tensor ) Find the maximum-weight matching of the match_quality_matrix. A maximum-weight matching is also a perfect matching. Arguments: match_quality_matrix : A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. num_valid_boxes : A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and num_valid_boxes is equal to [3, 4] the boxes for the batch=0 is padded from pos=3 . It means, that quality_matrix[0, 3:] all the values from this pattern should not be considered because of the padding. Returns: matches : a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) match_labels : a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) View Source def hungarian_matching ( match_quality_matrix : tf . Tensor , num_valid_boxes : tf . Tensor ) : \"\"\"Find the maximum-weight matching of the match_quality_matrix. A maximum-weight matching is also a perfect matching. Arguments: - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. Returns: - *matches*: a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) \"\"\" def hungarian_assignment ( cost_matrix ) : return tf . py_function ( lambda c : linear_sum_assignment ( c ), [ cost_matrix ] , Tout = ( tf . int32 , tf . int32 )) # We do not want to match our padded ground_truth during the hungarian # assignment so we create a mask to increase the cost of fake ground_truth # [ batch_size, M ] mask = ~ tf . sequence_mask ( tf . squeeze ( num_valid_boxes , 1 ), tf . shape ( match_quality_matrix ) [ 1 ] ) # [ batch_size, M, N ] mask = tf . tile ( mask [ ..., None ] , [ 1, 1, tf.shape(match_quality_matrix)[-1 ] ] ) # We set to inf all cost which corresponds to padding . # They won ' t interfere during the linear assignment match_quality_matrix = item_assignment ( match_quality_matrix , mask , 10e5 ) # [ batch_size, num_gt_boxes ] indices = tf . vectorized_map ( hungarian_assignment , match_quality_matrix ) matches = tf . one_hot ( indices [ 1 ] , tf . shape ( match_quality_matrix ) [ -1 ] , dtype = tf . int32 ) match_labels = tf . cast ( tf . reduce_max ( matches , axis = 1 ), tf . int32 ) matches = tf . reduce_max ( matches * indices [ 0 ][ ..., None ] , axis = 1 ) # Remove all the padded groundtruths # e . g : matches = [ 1, 0, 4, 3 ] num_valid_boxes = [ 3 ] # mask_padded_boxes = [ 0, 0, 1, 1 ] mask_padded_boxes = matches >= num_valid_boxes # Will flag to - 1 all the padded boxes to avoid sampling them match_labels = item_assignment ( match_labels , mask_padded_boxes , - 1 ) return tf . stop_gradient ( matches ), tf . stop_gradient ( match_labels ) Classes Matcher class Matcher ( thresholds : List [ float ], labels : List [ int ], allow_low_quality_matches : bool = False ) element. Each predicted element will have exactly zero or one matches; each ground-truth element may be matched to zero or more predicted elements. The matching is determined by the MxN match_quality_matrix, that characterizes how well each (ground-truth, prediction)-pair match each other. For example, if the elements are boxes, this matrix may contain box intersection-over-union overlap values. The matcher returns (a) a vector of length N containing the index of the ground-truth element m in [0, M) that matches to prediction n in [0, N). (b) a vector of length N containing the labels for each prediction. Arguments: thresholds : a list of thresholds used to stratify predictions into levels. labels : a list of values to label predictions belonging at each level. A label can be one of {-1, 0, 1} signifying {ignore, negative class, positive class}, respectively. allow_low_quality_matches : if True, produce additional matches for predictions with maximum match quality lower than high_threshold. See set_low_quality_matches_ for more details. Example: thresholds = [0.3, 0.5] labels = [0, -1, 1] All predictions with iou < 0.3 will be marked with 0 and thus will be considered as false positives while training. All predictions with 0.3 <= iou < 0.5 will be marked with -1 and thus will be ignored. All predictions with 0.5 <= iou will be marked with 1 and thus will be considered as true positives.","title":"Matcher"},{"location":"reference/kerod/core/matcher/#module-kerodcorematcher","text":"None None View Source from typing import List import tensorflow as tf from kerod . utils import item_assignment from scipy . optimize import linear_sum_assignment class Matcher : \"\"\"This class assigns to each predicted \" element \" (e.g., a box) a ground-truth element. Each predicted element will have exactly zero or one matches; each ground-truth element may be matched to zero or more predicted elements. The matching is determined by the MxN match_quality_matrix, that characterizes how well each (ground-truth, prediction)-pair match each other. For example, if the elements are boxes, this matrix may contain box intersection-over-union overlap values. The matcher returns (a) a vector of length N containing the index of the ground-truth element m in [0, M) that matches to prediction n in [0, N). (b) a vector of length N containing the labels for each prediction. Arguments: - *thresholds*: a list of thresholds used to stratify predictions into levels. - *labels*: a list of values to label predictions belonging at each level. A label can be one of {-1, 0, 1} signifying {ignore, negative class, positive class}, respectively. - *allow_low_quality_matches*: if True, produce additional matches for predictions with maximum match quality lower than high_threshold. See set_low_quality_matches_ for more details. Example: thresholds = [0.3, 0.5] labels = [0, -1, 1] All predictions with iou < 0.3 will be marked with 0 and thus will be considered as false positives while training. All predictions with 0.3 <= iou < 0.5 will be marked with -1 and thus will be ignored. All predictions with 0.5 <= iou will be marked with 1 and thus will be considered as true positives. \"\"\" def __ init__ ( self , thresholds : List [ float ], labels : List [ int ], allow_low_quality_matches: bool = False ) : # Add - inf and + inf to first and last position in thresholds thresholds = thresholds [ : ] assert thresholds [ 0 ] > 0 thresholds . insert ( 0 , - float ( \"inf\" )) thresholds . append ( float ( \"inf\" )) assert all ( low <= high for ( low , high ) in zip ( thresholds [:- 1 ], thresholds [ 1 : ])) assert all ( l in [ - 1 , 0 , 1 ] for l in labels ) assert len ( labels ) == len ( thresholds ) - 1 self . thresholds = thresholds self . labels = labels self . allow_low_quality_matches = allow_low_quality_matches def __ call__ ( self , match_quality_matrix: tf . Tensor , num_valid_boxes: tf . Tensor ) : \"\"\" Arguments: - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. Returns: - *matches*: a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) \"\"\" assert len ( match_quality_matrix . shape ) == 3 num_valid_boxes = tf . squeeze ( num_valid_boxes , - 1 ) # match_quality_matrix is B ( batch ) x M ( gt ) x N ( predicted ) # Max over gt elements to find best gt candidate for each prediction matches = tf . argmax ( match_quality_matrix , axis = 1 , output_type = tf . int32 ) matched_vals = tf . math . reduce_max ( match_quality_matrix , axis = 1 ) # matched_vals , matches = match_quality_matrix . max ( dim = 0 ) match_labels = matches for ( l , low , high ) in zip ( self . labels , self . thresholds [:- 1 ], self . thresholds [ 1 : ]) : low_high = ( matched_vals >= low ) & ( matched_vals < high ) match_labels = item_assignment ( match_labels , low_high , l ) if self . allow_low_quality_matches: match_labels = self . _ set_low_quality_matches ( match_labels , match_quality_matrix , num_valid_boxes ) # Remove all the padded groundtruths # e . g : matches = [ 1 , 0 , 4 , 3 ] num_valid_boxes = [ 3 ] # mask_padded_boxes = [ 0 , 0 , 1 , 1 ] mask_padded_boxes = matches >= num_valid_boxes [ : , None ] # Will flag to - 1 all the padded boxes to avoid sampling them match_labels = item_assignment ( match_labels , mask_padded_boxes , - 1 ) return matches , match_labels def _ set_low_quality_matches ( self , match_labels , match_quality_matrix , num_valid_boxes ) : \"\"\" Produce additional matches for predictions that have only low-quality matches. Specifically, for each ground-truth G find the set of predictions that have maximum overlap with it (including ties); for each prediction in that set, if it is unmatched, then match it to the ground-truth G. This function implements the RPN assignment case (i) in Sec. 3.1.2 of the Faster R-CNN paper: https://arxiv.org/pdf/1506.01497v3.pdf. Arguments: - *match_labels*: a tensor of int8 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. \"\"\" # For each gt , find the prediction with which it has highest quality : shape [ batch_size , M ] highest_quality_gt = tf . math . reduce_max ( match_quality_matrix , axis = 2 ) # Find the highest quality match available , even if it is low , including ties . hq_foreach_gt_mask = match_quality_matrix == highest_quality_gt [..., None ] # Create a mask for the valid_boxes , shape = [ batch_size , M ] mask_valid_boxes = tf . sequence_mask ( num_valid_boxes , maxlen = tf . shape ( match_quality_matrix )[ 1 ], dtype = tf . bool ) hq_foreach_gt_mask = hq_foreach_gt_mask & mask_valid_boxes [..., None ] # If an anchor was labeled positive only due to a low - quality match # with gt_A , but it has larger overlap with gt_B , it's matched index will still be gt_B. # This follows the implementation in Detectron, and is found to have no significant impact. # shape = [batch_size, N] masks_for_labels = tf.reduce_max(tf.cast(hq_foreach_gt_mask, tf.int8), 1) match_labels = item_assignment(match_labels, masks_for_labels, 1) return match_labels def hungarian_matching(match_quality_matrix: tf.Tensor, num_valid_boxes: tf.Tensor): \"\"\"Find the maximum-weight matching of the match_quality_matrix. A maximum-weight matching is also a perfect matching. Arguments: - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. Returns: - *matches*: a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) \"\"\" def hungarian_assignment(cost_matrix): return tf.py_function(lambda c: linear_sum_assignment(c), [cost_matrix], Tout=(tf.int32, tf.int32)) # We do not want to match our padded ground_truth during the hungarian # assignment so we create a mask to increase the cost of fake ground_truth # [batch_size, M] mask = ~tf.sequence_mask(tf.squeeze(num_valid_boxes, 1), tf.shape(match_quality_matrix)[1]) # [batch_size, M, N] mask = tf.tile(mask[..., None], [1, 1, tf.shape(match_quality_matrix)[-1]]) # We set to inf all cost which corresponds to padding. # They won't interfere during the linear assignment match_quality_matrix = item_assignment ( match_quality_matrix , mask , 10 e5 ) # [ batch_size , num_gt_boxes ] indices = tf . vectorized_map ( hungarian_assignment , match_quality_matrix ) matches = tf . one_hot ( indices [ 1 ], tf . shape ( match_quality_matrix )[ - 1 ], dtype = tf . int32 ) match_labels = tf . cast ( tf . reduce_max ( matches , axis = 1 ), tf . int32 ) matches = tf . reduce_max ( matches * indices [ 0 ][..., None ], axis = 1 ) # Remove all the padded groundtruths # e . g : matches = [ 1 , 0 , 4 , 3 ] num_valid_boxes = [ 3 ] # mask_padded_boxes = [ 0 , 0 , 1 , 1 ] mask_padded_boxes = matches >= num_valid_boxes # Will flag to - 1 all the padded boxes to avoid sampling them match_labels = item_assignment ( match_labels , mask_padded_boxes , - 1 ) return tf . stop_gradient ( matches ), tf . stop_gradient ( match_labels )","title":"Module kerod.core.matcher"},{"location":"reference/kerod/core/matcher/#functions","text":"","title":"Functions"},{"location":"reference/kerod/core/matcher/#hungarian_matching","text":"def hungarian_matching ( match_quality_matrix : tensorflow . python . framework . ops . Tensor , num_valid_boxes : tensorflow . python . framework . ops . Tensor ) Find the maximum-weight matching of the match_quality_matrix. A maximum-weight matching is also a perfect matching. Arguments: match_quality_matrix : A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. num_valid_boxes : A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and num_valid_boxes is equal to [3, 4] the boxes for the batch=0 is padded from pos=3 . It means, that quality_matrix[0, 3:] all the values from this pattern should not be considered because of the padding. Returns: matches : a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) match_labels : a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) View Source def hungarian_matching ( match_quality_matrix : tf . Tensor , num_valid_boxes : tf . Tensor ) : \"\"\"Find the maximum-weight matching of the match_quality_matrix. A maximum-weight matching is also a perfect matching. Arguments: - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. Returns: - *matches*: a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) \"\"\" def hungarian_assignment ( cost_matrix ) : return tf . py_function ( lambda c : linear_sum_assignment ( c ), [ cost_matrix ] , Tout = ( tf . int32 , tf . int32 )) # We do not want to match our padded ground_truth during the hungarian # assignment so we create a mask to increase the cost of fake ground_truth # [ batch_size, M ] mask = ~ tf . sequence_mask ( tf . squeeze ( num_valid_boxes , 1 ), tf . shape ( match_quality_matrix ) [ 1 ] ) # [ batch_size, M, N ] mask = tf . tile ( mask [ ..., None ] , [ 1, 1, tf.shape(match_quality_matrix)[-1 ] ] ) # We set to inf all cost which corresponds to padding . # They won ' t interfere during the linear assignment match_quality_matrix = item_assignment ( match_quality_matrix , mask , 10e5 ) # [ batch_size, num_gt_boxes ] indices = tf . vectorized_map ( hungarian_assignment , match_quality_matrix ) matches = tf . one_hot ( indices [ 1 ] , tf . shape ( match_quality_matrix ) [ -1 ] , dtype = tf . int32 ) match_labels = tf . cast ( tf . reduce_max ( matches , axis = 1 ), tf . int32 ) matches = tf . reduce_max ( matches * indices [ 0 ][ ..., None ] , axis = 1 ) # Remove all the padded groundtruths # e . g : matches = [ 1, 0, 4, 3 ] num_valid_boxes = [ 3 ] # mask_padded_boxes = [ 0, 0, 1, 1 ] mask_padded_boxes = matches >= num_valid_boxes # Will flag to - 1 all the padded boxes to avoid sampling them match_labels = item_assignment ( match_labels , mask_padded_boxes , - 1 ) return tf . stop_gradient ( matches ), tf . stop_gradient ( match_labels )","title":"hungarian_matching"},{"location":"reference/kerod/core/matcher/#classes","text":"","title":"Classes"},{"location":"reference/kerod/core/matcher/#matcher","text":"class Matcher ( thresholds : List [ float ], labels : List [ int ], allow_low_quality_matches : bool = False ) element. Each predicted element will have exactly zero or one matches; each ground-truth element may be matched to zero or more predicted elements. The matching is determined by the MxN match_quality_matrix, that characterizes how well each (ground-truth, prediction)-pair match each other. For example, if the elements are boxes, this matrix may contain box intersection-over-union overlap values. The matcher returns (a) a vector of length N containing the index of the ground-truth element m in [0, M) that matches to prediction n in [0, N). (b) a vector of length N containing the labels for each prediction. Arguments: thresholds : a list of thresholds used to stratify predictions into levels. labels : a list of values to label predictions belonging at each level. A label can be one of {-1, 0, 1} signifying {ignore, negative class, positive class}, respectively. allow_low_quality_matches : if True, produce additional matches for predictions with maximum match quality lower than high_threshold. See set_low_quality_matches_ for more details. Example: thresholds = [0.3, 0.5] labels = [0, -1, 1] All predictions with iou < 0.3 will be marked with 0 and thus will be considered as false positives while training. All predictions with 0.3 <= iou < 0.5 will be marked with -1 and thus will be ignored. All predictions with 0.5 <= iou will be marked with 1 and thus will be considered as true positives.","title":"Matcher"},{"location":"reference/kerod/core/sampling_ops/","text":"Module kerod.core.sampling_ops Method to subsample minibatches by balancing positives and negatives. Subsamples minibatches based on a pre-specified positive fraction in range [0,1]. The class presumes there are many more negatives than positive examples: if the desired sample_size cannot be achieved with the pre-specified positive fraction, it fills the rest with negative examples. If this is not sufficient for obtaining the desired sample_size, it returns fewer examples. The main function to call is Subsample(self, indicator, labels). For convenience one can also call SubsampleWeights(self, weights, labels) which is defined in the minibatch_sampler base class. When is_static is True, it implements a method that guarantees static shapes. It also ensures the length of output of the subsample is always sample_size, even when number of examples set to True in indicator is less than sample_size. View Source # Copyright 2017 The TensorFlow Authors and modified by Emilien Garreau. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== \" \"\" Method to subsample minibatches by balancing positives and negatives. Subsamples minibatches based on a pre-specified positive fraction in range [0,1]. The class presumes there are many more negatives than positive examples: if the desired sample_size cannot be achieved with the pre-specified positive fraction, it fills the rest with negative examples. If this is not sufficient for obtaining the desired sample_size, it returns fewer examples. The main function to call is Subsample(self, indicator, labels). For convenience one can also call SubsampleWeights(self, weights, labels) which is defined in the minibatch_sampler base class. When is_static is True, it implements a method that guarantees static shapes. It also ensures the length of output of the subsample is always sample_size, even when number of examples set to True in indicator is less than sample_size. \"\" \" import tensorflow as tf from kerod . utils import ops def subsample_indicator ( indicator , num_samples ) : \" \"\" Subsample indicator vector. Given a boolean indicator vector with M elements set to `True`, the function assigns all but `num_samples` of these previously `True` elements to `False`. If `num_samples` is greater than M, the original indicator vector is returned. Arguments: - *indicator*: a 1-dimensional boolean tensor indicating which elements are allowed to be sampled and which are not. - *num_samples*: int32 scalar tensor Returns: A boolean tensor with the same shape as input (indicator) tensor \"\" \" indices = tf . where ( indicator ) indices = tf . random . shuffle ( indices ) indices = tf . reshape ( indices , [ - 1 ] ) num_samples = tf . minimum ( tf . size ( indices ), num_samples ) selected_indices = tf . slice ( indices , [ 0 ] , tf . reshape ( num_samples , [ 1 ] )) selected_indicator = ops . indices_to_dense_vector ( selected_indices , tf . shape ( indicator ) [ 0 ] ) return tf . equal ( selected_indicator , 1 ) def sample_balanced_positive_negative ( indicator , sample_size , labels , positive_fraction = 0.5 ) : \" \"\" Subsamples minibatches to a desired balance of positives and negatives. Arguments: - *indicator*: boolean tensor of shape [N] whose True entries can be sampled. - *sample_size*: desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. - *labels*: boolean tensor of shape [N] denoting positive(=True) and negative (=False) examples. - *positive_fraction*: desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: *sampled_idx_indicator*: boolean tensor of shape [N], True for entries which are sampled. \"\" \" negative_idx = tf . logical_not ( labels ) positive_idx = tf . logical_and ( labels , indicator ) negative_idx = tf . logical_and ( negative_idx , indicator ) # Sample positive and negative samples separately if sample_size is None : max_num_pos = tf . reduce_sum ( tf . cast ( positive_idx , dtype = tf . int32 )) else : max_num_pos = int ( positive_fraction * sample_size ) sampled_pos_idx = subsample_indicator ( positive_idx , max_num_pos ) num_sampled_pos = tf . reduce_sum ( tf . cast ( sampled_pos_idx , tf . int32 )) if sample_size is None : negative_positive_ratio = ( 1 - positive_fraction ) / positive_fraction max_num_neg = tf . cast ( negative_positive_ratio * tf . cast ( num_sampled_pos , dtype = tf . float32 ), dtype = tf . int32 ) else : max_num_neg = sample_size - num_sampled_pos sampled_neg_idx = subsample_indicator ( negative_idx , max_num_neg ) return tf . logical_or ( sampled_pos_idx , sampled_neg_idx ) def batch_sample_balanced_positive_negative ( indicators , sample_size , labels , positive_fraction = 0.5 , dtype = tf . float32 ) : \" \"\" Subsamples minibatches to a desired balance of positives and negatives. Arguments: - *indicator*: boolean tensor of shape [batch_size, N] whose True entries can be sampled. - *sample_size*: desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. - *labels*: boolean tensor of shape [batch_size, N] denoting positive(=True) and negative (=False) examples. - *positive_fraction*: desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: A boolean tensor of shape [M, N], True for entries which are sampled. \"\" \" def _minibatch_subsample_fn ( inputs ) : indicators , targets = inputs return sample_balanced_positive_negative ( tf . cast ( indicators , tf . bool ), sample_size , tf . cast ( targets , tf . bool ), positive_fraction = positive_fraction ) return tf . cast ( tf . map_fn ( _minibatch_subsample_fn , [ indicators , labels ] , dtype = tf . bool , parallel_iterations = 16 , back_prop = True ), dtype = dtype ) Functions batch_sample_balanced_positive_negative def batch_sample_balanced_positive_negative ( indicators , sample_size , labels , positive_fraction = 0.5 , dtype = tf . float32 ) Subsamples minibatches to a desired balance of positives and negatives. Arguments: indicator : boolean tensor of shape [batch_size, N] whose True entries can be sampled. sample_size : desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. labels : boolean tensor of shape [batch_size, N] denoting positive(=True) and negative (=False) examples. positive_fraction : desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: A boolean tensor of shape [M, N], True for entries which are sampled. View Source def batch_sample_balanced_positive_negative ( indicators , sample_size , labels , positive_fraction = 0 . 5 , dtype = tf . float32 ) : \"\"\" Subsamples minibatches to a desired balance of positives and negatives. Arguments : - * indicator * : boolean tensor of shape [ batch_size , N ] whose True entries can be sampled . - * sample_size * : desired batch size . If None , keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction . - * labels * : boolean tensor of shape [ batch_size , N ] denoting positive ( = True ) and negative ( = False ) examples . - * positive_fraction * : desired fraction of positive examples ( scalar in [ 0 , 1 ] ) in the batch . Returns : A boolean tensor of shape [ M , N ], True for entries which are sampled . \"\"\" def _minibatch_subsample_fn ( inputs ) : indicators , targets = inputs return sample_balanced_positive_negative ( tf . cast ( indicators , tf . bool ) , sample_size , tf . cast ( targets , tf . bool ) , positive_fraction = positive_fraction ) return tf . cast ( tf . map_fn ( _minibatch_subsample_fn , [ indicators , labels ], dtype = tf . bool , parallel_iterations = 16 , back_prop = True ) , dtype = dtype ) sample_balanced_positive_negative def sample_balanced_positive_negative ( indicator , sample_size , labels , positive_fraction = 0.5 ) Subsamples minibatches to a desired balance of positives and negatives. Arguments: indicator : boolean tensor of shape [N] whose True entries can be sampled. sample_size : desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. labels : boolean tensor of shape [N] denoting positive(=True) and negative (=False) examples. positive_fraction : desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: sampled_idx_indicator : boolean tensor of shape [N], True for entries which are sampled. View Source def sample_balanced_positive_negative ( indicator , sample_size , labels , positive_fraction = 0.5 ) : \"\"\"Subsamples minibatches to a desired balance of positives and negatives. Arguments: - *indicator*: boolean tensor of shape [N] whose True entries can be sampled. - *sample_size*: desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. - *labels*: boolean tensor of shape [N] denoting positive(=True) and negative (=False) examples. - *positive_fraction*: desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: *sampled_idx_indicator*: boolean tensor of shape [N], True for entries which are sampled. \"\"\" negative_idx = tf . logical_not ( labels ) positive_idx = tf . logical_and ( labels , indicator ) negative_idx = tf . logical_and ( negative_idx , indicator ) # Sample positive and negative samples separately if sample_size is None : max_num_pos = tf . reduce_sum ( tf . cast ( positive_idx , dtype = tf . int32 )) else : max_num_pos = int ( positive_fraction * sample_size ) sampled_pos_idx = subsample_indicator ( positive_idx , max_num_pos ) num_sampled_pos = tf . reduce_sum ( tf . cast ( sampled_pos_idx , tf . int32 )) if sample_size is None : negative_positive_ratio = ( 1 - positive_fraction ) / positive_fraction max_num_neg = tf . cast ( negative_positive_ratio * tf . cast ( num_sampled_pos , dtype = tf . float32 ), dtype = tf . int32 ) else : max_num_neg = sample_size - num_sampled_pos sampled_neg_idx = subsample_indicator ( negative_idx , max_num_neg ) return tf . logical_or ( sampled_pos_idx , sampled_neg_idx ) subsample_indicator def subsample_indicator ( indicator , num_samples ) Subsample indicator vector. Given a boolean indicator vector with M elements set to True , the function assigns all but num_samples of these previously True elements to False . If num_samples is greater than M, the original indicator vector is returned. Arguments: - indicator : a 1-dimensional boolean tensor indicating which elements are allowed to be sampled and which are not. num_samples : int32 scalar tensor Returns: A boolean tensor with the same shape as input (indicator) tensor View Source def subsample_indicator ( indicator , num_samples ) : \" \"\" Subsample indicator vector. Given a boolean indicator vector with M elements set to `True`, the function assigns all but `num_samples` of these previously `True` elements to `False`. If `num_samples` is greater than M, the original indicator vector is returned. Arguments: - *indicator*: a 1-dimensional boolean tensor indicating which elements are allowed to be sampled and which are not. - *num_samples*: int32 scalar tensor Returns: A boolean tensor with the same shape as input (indicator) tensor \"\" \" indices = tf . where ( indicator ) indices = tf . random . shuffle ( indices ) indices = tf . reshape ( indices , [ - 1 ] ) num_samples = tf . minimum ( tf . size ( indices ), num_samples ) selected_indices = tf . slice ( indices , [ 0 ] , tf . reshape ( num_samples , [ 1 ] )) selected_indicator = ops . indices_to_dense_vector ( selected_indices , tf . shape ( indicator ) [ 0 ] ) return tf . equal ( selected_indicator , 1 )","title":"Sampling Ops"},{"location":"reference/kerod/core/sampling_ops/#module-kerodcoresampling_ops","text":"Method to subsample minibatches by balancing positives and negatives. Subsamples minibatches based on a pre-specified positive fraction in range [0,1]. The class presumes there are many more negatives than positive examples: if the desired sample_size cannot be achieved with the pre-specified positive fraction, it fills the rest with negative examples. If this is not sufficient for obtaining the desired sample_size, it returns fewer examples. The main function to call is Subsample(self, indicator, labels). For convenience one can also call SubsampleWeights(self, weights, labels) which is defined in the minibatch_sampler base class. When is_static is True, it implements a method that guarantees static shapes. It also ensures the length of output of the subsample is always sample_size, even when number of examples set to True in indicator is less than sample_size. View Source # Copyright 2017 The TensorFlow Authors and modified by Emilien Garreau. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== \" \"\" Method to subsample minibatches by balancing positives and negatives. Subsamples minibatches based on a pre-specified positive fraction in range [0,1]. The class presumes there are many more negatives than positive examples: if the desired sample_size cannot be achieved with the pre-specified positive fraction, it fills the rest with negative examples. If this is not sufficient for obtaining the desired sample_size, it returns fewer examples. The main function to call is Subsample(self, indicator, labels). For convenience one can also call SubsampleWeights(self, weights, labels) which is defined in the minibatch_sampler base class. When is_static is True, it implements a method that guarantees static shapes. It also ensures the length of output of the subsample is always sample_size, even when number of examples set to True in indicator is less than sample_size. \"\" \" import tensorflow as tf from kerod . utils import ops def subsample_indicator ( indicator , num_samples ) : \" \"\" Subsample indicator vector. Given a boolean indicator vector with M elements set to `True`, the function assigns all but `num_samples` of these previously `True` elements to `False`. If `num_samples` is greater than M, the original indicator vector is returned. Arguments: - *indicator*: a 1-dimensional boolean tensor indicating which elements are allowed to be sampled and which are not. - *num_samples*: int32 scalar tensor Returns: A boolean tensor with the same shape as input (indicator) tensor \"\" \" indices = tf . where ( indicator ) indices = tf . random . shuffle ( indices ) indices = tf . reshape ( indices , [ - 1 ] ) num_samples = tf . minimum ( tf . size ( indices ), num_samples ) selected_indices = tf . slice ( indices , [ 0 ] , tf . reshape ( num_samples , [ 1 ] )) selected_indicator = ops . indices_to_dense_vector ( selected_indices , tf . shape ( indicator ) [ 0 ] ) return tf . equal ( selected_indicator , 1 ) def sample_balanced_positive_negative ( indicator , sample_size , labels , positive_fraction = 0.5 ) : \" \"\" Subsamples minibatches to a desired balance of positives and negatives. Arguments: - *indicator*: boolean tensor of shape [N] whose True entries can be sampled. - *sample_size*: desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. - *labels*: boolean tensor of shape [N] denoting positive(=True) and negative (=False) examples. - *positive_fraction*: desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: *sampled_idx_indicator*: boolean tensor of shape [N], True for entries which are sampled. \"\" \" negative_idx = tf . logical_not ( labels ) positive_idx = tf . logical_and ( labels , indicator ) negative_idx = tf . logical_and ( negative_idx , indicator ) # Sample positive and negative samples separately if sample_size is None : max_num_pos = tf . reduce_sum ( tf . cast ( positive_idx , dtype = tf . int32 )) else : max_num_pos = int ( positive_fraction * sample_size ) sampled_pos_idx = subsample_indicator ( positive_idx , max_num_pos ) num_sampled_pos = tf . reduce_sum ( tf . cast ( sampled_pos_idx , tf . int32 )) if sample_size is None : negative_positive_ratio = ( 1 - positive_fraction ) / positive_fraction max_num_neg = tf . cast ( negative_positive_ratio * tf . cast ( num_sampled_pos , dtype = tf . float32 ), dtype = tf . int32 ) else : max_num_neg = sample_size - num_sampled_pos sampled_neg_idx = subsample_indicator ( negative_idx , max_num_neg ) return tf . logical_or ( sampled_pos_idx , sampled_neg_idx ) def batch_sample_balanced_positive_negative ( indicators , sample_size , labels , positive_fraction = 0.5 , dtype = tf . float32 ) : \" \"\" Subsamples minibatches to a desired balance of positives and negatives. Arguments: - *indicator*: boolean tensor of shape [batch_size, N] whose True entries can be sampled. - *sample_size*: desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. - *labels*: boolean tensor of shape [batch_size, N] denoting positive(=True) and negative (=False) examples. - *positive_fraction*: desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: A boolean tensor of shape [M, N], True for entries which are sampled. \"\" \" def _minibatch_subsample_fn ( inputs ) : indicators , targets = inputs return sample_balanced_positive_negative ( tf . cast ( indicators , tf . bool ), sample_size , tf . cast ( targets , tf . bool ), positive_fraction = positive_fraction ) return tf . cast ( tf . map_fn ( _minibatch_subsample_fn , [ indicators , labels ] , dtype = tf . bool , parallel_iterations = 16 , back_prop = True ), dtype = dtype )","title":"Module kerod.core.sampling_ops"},{"location":"reference/kerod/core/sampling_ops/#functions","text":"","title":"Functions"},{"location":"reference/kerod/core/sampling_ops/#batch_sample_balanced_positive_negative","text":"def batch_sample_balanced_positive_negative ( indicators , sample_size , labels , positive_fraction = 0.5 , dtype = tf . float32 ) Subsamples minibatches to a desired balance of positives and negatives. Arguments: indicator : boolean tensor of shape [batch_size, N] whose True entries can be sampled. sample_size : desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. labels : boolean tensor of shape [batch_size, N] denoting positive(=True) and negative (=False) examples. positive_fraction : desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: A boolean tensor of shape [M, N], True for entries which are sampled. View Source def batch_sample_balanced_positive_negative ( indicators , sample_size , labels , positive_fraction = 0 . 5 , dtype = tf . float32 ) : \"\"\" Subsamples minibatches to a desired balance of positives and negatives. Arguments : - * indicator * : boolean tensor of shape [ batch_size , N ] whose True entries can be sampled . - * sample_size * : desired batch size . If None , keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction . - * labels * : boolean tensor of shape [ batch_size , N ] denoting positive ( = True ) and negative ( = False ) examples . - * positive_fraction * : desired fraction of positive examples ( scalar in [ 0 , 1 ] ) in the batch . Returns : A boolean tensor of shape [ M , N ], True for entries which are sampled . \"\"\" def _minibatch_subsample_fn ( inputs ) : indicators , targets = inputs return sample_balanced_positive_negative ( tf . cast ( indicators , tf . bool ) , sample_size , tf . cast ( targets , tf . bool ) , positive_fraction = positive_fraction ) return tf . cast ( tf . map_fn ( _minibatch_subsample_fn , [ indicators , labels ], dtype = tf . bool , parallel_iterations = 16 , back_prop = True ) , dtype = dtype )","title":"batch_sample_balanced_positive_negative"},{"location":"reference/kerod/core/sampling_ops/#sample_balanced_positive_negative","text":"def sample_balanced_positive_negative ( indicator , sample_size , labels , positive_fraction = 0.5 ) Subsamples minibatches to a desired balance of positives and negatives. Arguments: indicator : boolean tensor of shape [N] whose True entries can be sampled. sample_size : desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. labels : boolean tensor of shape [N] denoting positive(=True) and negative (=False) examples. positive_fraction : desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: sampled_idx_indicator : boolean tensor of shape [N], True for entries which are sampled. View Source def sample_balanced_positive_negative ( indicator , sample_size , labels , positive_fraction = 0.5 ) : \"\"\"Subsamples minibatches to a desired balance of positives and negatives. Arguments: - *indicator*: boolean tensor of shape [N] whose True entries can be sampled. - *sample_size*: desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. - *labels*: boolean tensor of shape [N] denoting positive(=True) and negative (=False) examples. - *positive_fraction*: desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: *sampled_idx_indicator*: boolean tensor of shape [N], True for entries which are sampled. \"\"\" negative_idx = tf . logical_not ( labels ) positive_idx = tf . logical_and ( labels , indicator ) negative_idx = tf . logical_and ( negative_idx , indicator ) # Sample positive and negative samples separately if sample_size is None : max_num_pos = tf . reduce_sum ( tf . cast ( positive_idx , dtype = tf . int32 )) else : max_num_pos = int ( positive_fraction * sample_size ) sampled_pos_idx = subsample_indicator ( positive_idx , max_num_pos ) num_sampled_pos = tf . reduce_sum ( tf . cast ( sampled_pos_idx , tf . int32 )) if sample_size is None : negative_positive_ratio = ( 1 - positive_fraction ) / positive_fraction max_num_neg = tf . cast ( negative_positive_ratio * tf . cast ( num_sampled_pos , dtype = tf . float32 ), dtype = tf . int32 ) else : max_num_neg = sample_size - num_sampled_pos sampled_neg_idx = subsample_indicator ( negative_idx , max_num_neg ) return tf . logical_or ( sampled_pos_idx , sampled_neg_idx )","title":"sample_balanced_positive_negative"},{"location":"reference/kerod/core/sampling_ops/#subsample_indicator","text":"def subsample_indicator ( indicator , num_samples ) Subsample indicator vector. Given a boolean indicator vector with M elements set to True , the function assigns all but num_samples of these previously True elements to False . If num_samples is greater than M, the original indicator vector is returned. Arguments: - indicator : a 1-dimensional boolean tensor indicating which elements are allowed to be sampled and which are not. num_samples : int32 scalar tensor Returns: A boolean tensor with the same shape as input (indicator) tensor View Source def subsample_indicator ( indicator , num_samples ) : \" \"\" Subsample indicator vector. Given a boolean indicator vector with M elements set to `True`, the function assigns all but `num_samples` of these previously `True` elements to `False`. If `num_samples` is greater than M, the original indicator vector is returned. Arguments: - *indicator*: a 1-dimensional boolean tensor indicating which elements are allowed to be sampled and which are not. - *num_samples*: int32 scalar tensor Returns: A boolean tensor with the same shape as input (indicator) tensor \"\" \" indices = tf . where ( indicator ) indices = tf . random . shuffle ( indices ) indices = tf . reshape ( indices , [ - 1 ] ) num_samples = tf . minimum ( tf . size ( indices ), num_samples ) selected_indices = tf . slice ( indices , [ 0 ] , tf . reshape ( num_samples , [ 1 ] )) selected_indicator = ops . indices_to_dense_vector ( selected_indices , tf . shape ( indicator ) [ 0 ] ) return tf . equal ( selected_indicator , 1 )","title":"subsample_indicator"},{"location":"reference/kerod/core/similarity/","text":"Module kerod.core.similarity None None View Source from abc import abstractmethod from typing import Dict import tensorflow as tf from kerod.core.box_ops import compute_giou , compute_iou , convert_to_xyxy_coordinates from kerod.core.standard_fields import BoxField from kerod.utils import get_full_indices class Similarity : def __call__ ( self , inputs1 : Dict [ str , tf . Tensor ], inputs2 : Dict [ str , tf . Tensor ]): return self . call ( inputs1 , inputs2 ) @abstractmethod def call ( self , inputs1 , inputs2 ) -> tf . Tensor : pass class IoUSimilarity ( Similarity ): def call ( self , y_true : Dict [ str , tf . Tensor ], anchors : Dict [ str , tf . Tensor ]): \"\"\"Computes pairwise intersection-over-union between boxes. Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. \"\"\" return compute_iou ( y_true [ BoxField . BOXES ], anchors [ BoxField . BOXES ]) class DetrSimilarity ( Similarity ): def __init__ ( self , weight_class = 1 , weight_l1 = 5 , weight_giou = 2 ): \"\"\"Instantiate a callable object which will compute the similarity according to the default parameters: [End to end object detection with transformers](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers). Arguments: - *weight_class*: Weight which ponderates the cost of the class similarity. - *weight_l1*: Weight which ponderates the cost of the l1 similarity between boxes. - *weight_giou*: Weight which ponderates the cost of the giou similarity between boxes. \"\"\" self . weight_class = weight_class self . weight_l1 = weight_l1 self . weight_giou = weight_giou def call ( self , y_true : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ]) -> tf . Tensor : \"\"\" Compute the cost matrix according to the paper [End to end object detection with transformers](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers). Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. \"\"\" classification_logits = y_pred [ BoxField . SCORES ] localization_pred = y_pred [ BoxField . BOXES ] out_prob = tf . nn . softmax ( classification_logits , - 1 ) # Extract the target classes to approximate the classification cost # [batch_size, nb_class, num_detection] out_prob = tf . transpose ( out_prob , [ 0 , 2 , 1 ]) # [batch_size, nb_target, num_detection] cost = tf . gather_nd ( out_prob , get_full_indices ( y_true [ BoxField . LABELS ])) # Compute the classification cost. Contrary to the loss, we don't use the NLL, # but approximate it in 1 - proba[target class]. # The 1 is a constant that doesn't change the matching, it can be ommitted. # [batch_size, num_detection, nb_target] cost_class = - cost gt_boxes = y_true [ BoxField . BOXES ] # Compute the L1 cost between boxes # [batch_size, nb_target, num_detection] cost_bbox = tf . norm ( localization_pred [:, None ] - gt_boxes [:, :, None ], ord = 1 , axis =- 1 , ) # Compute the giou cost betwen boxes # [batch_size, nb_target, num_detection] # loss_giou= 1 - giou but we approximate it with -giou cost_giou = - compute_giou ( convert_to_xyxy_coordinates ( gt_boxes ), convert_to_xyxy_coordinates ( localization_pred )) # Final cost matrix cost_matrix = self . weight_l1 * cost_bbox + self . weight_class * cost_class + self . weight_giou * cost_giou return cost_matrix Classes DetrSimilarity class DetrSimilarity ( weight_class = 1 , weight_l1 = 5 , weight_giou = 2 ) Ancestors (in MRO) kerod.core.similarity.Similarity Methods call def call ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> tensorflow . python . framework . ops . Tensor Compute the cost matrix according to the paper End to end object detection with transformers . Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. View Source def call ( self , y_true : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ]) -> tf . Tensor : \"\"\" Compute the cost matrix according to the paper [ End to end object detection with transformers ]( https : //ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers). Return : A 3 - D tensor of float32 with shape [ batch_size , N , M ] representing pairwise similarity scores defined in DeTr . \"\"\" classification_logits = y_pred [ BoxField . SCORES ] localization_pred = y_pred [ BoxField . BOXES ] out_prob = tf . nn . softmax ( classification_logits , -1 ) # Extract the target classes to approximate the classification cost # [batch_size, nb_class, num_detection] out_prob = tf . transpose ( out_prob , [ 0 , 2 , 1 ]) # [batch_size, nb_target, num_detection] cost = tf . gather_nd ( out_prob , get_full_indices ( y_true [ BoxField . LABELS ])) # Compute the classification cost. Contrary to the loss, we don't use the NLL, # but approximate it in 1 - proba[target class]. # The 1 is a constant that doesn't change the matching, it can be ommitted. # [batch_size, num_detection, nb_target] cost_class = - cost gt_boxes = y_true [ BoxField . BOXES ] # Compute the L1 cost between boxes # [batch_size, nb_target, num_detection] cost_bbox = tf . norm ( localization_pred [ : , None ] - gt_boxes [ : , : , None ], ord = 1 , axis = -1 , ) # Compute the giou cost betwen boxes # [batch_size, nb_target, num_detection] # loss_giou= 1 - giou but we approximate it with -giou cost_giou = - compute_giou ( convert_to_xyxy_coordinates ( gt_boxes ), convert_to_xyxy_coordinates ( localization_pred )) # Final cost matrix cost_matrix = self . weight_l1 * cost_bbox + self . weight_class * cost_class + self . weight_giou * cost_giou return cost_matrix IoUSimilarity class IoUSimilarity ( / , * args , ** kwargs ) Ancestors (in MRO) kerod.core.similarity.Similarity Methods call def call ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], anchors : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) Computes pairwise intersection-over-union between boxes. Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. View Source def call ( self , y_true : Dict [ str , tf . Tensor ], anchors : Dict [ str , tf . Tensor ] ) : \"\"\" Computes pairwise intersection-over-union between boxes. Return : A 3 - D tensor of float32 with shape [ batch_size , N , M ] representing pairwise similarity scores defined in DeTr . \"\"\" return compute_iou ( y_true [ BoxField . BOXES ], anchors [ BoxField . BOXES ] ) Similarity class Similarity ( / , * args , ** kwargs ) Descendants kerod.core.similarity.IoUSimilarity kerod.core.similarity.DetrSimilarity Methods call def call ( self , inputs1 , inputs2 ) -> tensorflow . python . framework . ops . Tensor View Source @abstractmethod def call ( self , inputs1 , inputs2 ) -> tf . Tensor : pass","title":"Similarity"},{"location":"reference/kerod/core/similarity/#module-kerodcoresimilarity","text":"None None View Source from abc import abstractmethod from typing import Dict import tensorflow as tf from kerod.core.box_ops import compute_giou , compute_iou , convert_to_xyxy_coordinates from kerod.core.standard_fields import BoxField from kerod.utils import get_full_indices class Similarity : def __call__ ( self , inputs1 : Dict [ str , tf . Tensor ], inputs2 : Dict [ str , tf . Tensor ]): return self . call ( inputs1 , inputs2 ) @abstractmethod def call ( self , inputs1 , inputs2 ) -> tf . Tensor : pass class IoUSimilarity ( Similarity ): def call ( self , y_true : Dict [ str , tf . Tensor ], anchors : Dict [ str , tf . Tensor ]): \"\"\"Computes pairwise intersection-over-union between boxes. Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. \"\"\" return compute_iou ( y_true [ BoxField . BOXES ], anchors [ BoxField . BOXES ]) class DetrSimilarity ( Similarity ): def __init__ ( self , weight_class = 1 , weight_l1 = 5 , weight_giou = 2 ): \"\"\"Instantiate a callable object which will compute the similarity according to the default parameters: [End to end object detection with transformers](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers). Arguments: - *weight_class*: Weight which ponderates the cost of the class similarity. - *weight_l1*: Weight which ponderates the cost of the l1 similarity between boxes. - *weight_giou*: Weight which ponderates the cost of the giou similarity between boxes. \"\"\" self . weight_class = weight_class self . weight_l1 = weight_l1 self . weight_giou = weight_giou def call ( self , y_true : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ]) -> tf . Tensor : \"\"\" Compute the cost matrix according to the paper [End to end object detection with transformers](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers). Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. \"\"\" classification_logits = y_pred [ BoxField . SCORES ] localization_pred = y_pred [ BoxField . BOXES ] out_prob = tf . nn . softmax ( classification_logits , - 1 ) # Extract the target classes to approximate the classification cost # [batch_size, nb_class, num_detection] out_prob = tf . transpose ( out_prob , [ 0 , 2 , 1 ]) # [batch_size, nb_target, num_detection] cost = tf . gather_nd ( out_prob , get_full_indices ( y_true [ BoxField . LABELS ])) # Compute the classification cost. Contrary to the loss, we don't use the NLL, # but approximate it in 1 - proba[target class]. # The 1 is a constant that doesn't change the matching, it can be ommitted. # [batch_size, num_detection, nb_target] cost_class = - cost gt_boxes = y_true [ BoxField . BOXES ] # Compute the L1 cost between boxes # [batch_size, nb_target, num_detection] cost_bbox = tf . norm ( localization_pred [:, None ] - gt_boxes [:, :, None ], ord = 1 , axis =- 1 , ) # Compute the giou cost betwen boxes # [batch_size, nb_target, num_detection] # loss_giou= 1 - giou but we approximate it with -giou cost_giou = - compute_giou ( convert_to_xyxy_coordinates ( gt_boxes ), convert_to_xyxy_coordinates ( localization_pred )) # Final cost matrix cost_matrix = self . weight_l1 * cost_bbox + self . weight_class * cost_class + self . weight_giou * cost_giou return cost_matrix","title":"Module kerod.core.similarity"},{"location":"reference/kerod/core/similarity/#classes","text":"","title":"Classes"},{"location":"reference/kerod/core/similarity/#detrsimilarity","text":"class DetrSimilarity ( weight_class = 1 , weight_l1 = 5 , weight_giou = 2 )","title":"DetrSimilarity"},{"location":"reference/kerod/core/similarity/#ancestors-in-mro","text":"kerod.core.similarity.Similarity","title":"Ancestors (in MRO)"},{"location":"reference/kerod/core/similarity/#methods","text":"","title":"Methods"},{"location":"reference/kerod/core/similarity/#call","text":"def call ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> tensorflow . python . framework . ops . Tensor Compute the cost matrix according to the paper End to end object detection with transformers . Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. View Source def call ( self , y_true : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ]) -> tf . Tensor : \"\"\" Compute the cost matrix according to the paper [ End to end object detection with transformers ]( https : //ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers). Return : A 3 - D tensor of float32 with shape [ batch_size , N , M ] representing pairwise similarity scores defined in DeTr . \"\"\" classification_logits = y_pred [ BoxField . SCORES ] localization_pred = y_pred [ BoxField . BOXES ] out_prob = tf . nn . softmax ( classification_logits , -1 ) # Extract the target classes to approximate the classification cost # [batch_size, nb_class, num_detection] out_prob = tf . transpose ( out_prob , [ 0 , 2 , 1 ]) # [batch_size, nb_target, num_detection] cost = tf . gather_nd ( out_prob , get_full_indices ( y_true [ BoxField . LABELS ])) # Compute the classification cost. Contrary to the loss, we don't use the NLL, # but approximate it in 1 - proba[target class]. # The 1 is a constant that doesn't change the matching, it can be ommitted. # [batch_size, num_detection, nb_target] cost_class = - cost gt_boxes = y_true [ BoxField . BOXES ] # Compute the L1 cost between boxes # [batch_size, nb_target, num_detection] cost_bbox = tf . norm ( localization_pred [ : , None ] - gt_boxes [ : , : , None ], ord = 1 , axis = -1 , ) # Compute the giou cost betwen boxes # [batch_size, nb_target, num_detection] # loss_giou= 1 - giou but we approximate it with -giou cost_giou = - compute_giou ( convert_to_xyxy_coordinates ( gt_boxes ), convert_to_xyxy_coordinates ( localization_pred )) # Final cost matrix cost_matrix = self . weight_l1 * cost_bbox + self . weight_class * cost_class + self . weight_giou * cost_giou return cost_matrix","title":"call"},{"location":"reference/kerod/core/similarity/#iousimilarity","text":"class IoUSimilarity ( / , * args , ** kwargs )","title":"IoUSimilarity"},{"location":"reference/kerod/core/similarity/#ancestors-in-mro_1","text":"kerod.core.similarity.Similarity","title":"Ancestors (in MRO)"},{"location":"reference/kerod/core/similarity/#methods_1","text":"","title":"Methods"},{"location":"reference/kerod/core/similarity/#call_1","text":"def call ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], anchors : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) Computes pairwise intersection-over-union between boxes. Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. View Source def call ( self , y_true : Dict [ str , tf . Tensor ], anchors : Dict [ str , tf . Tensor ] ) : \"\"\" Computes pairwise intersection-over-union between boxes. Return : A 3 - D tensor of float32 with shape [ batch_size , N , M ] representing pairwise similarity scores defined in DeTr . \"\"\" return compute_iou ( y_true [ BoxField . BOXES ], anchors [ BoxField . BOXES ] )","title":"call"},{"location":"reference/kerod/core/similarity/#similarity","text":"class Similarity ( / , * args , ** kwargs )","title":"Similarity"},{"location":"reference/kerod/core/similarity/#descendants","text":"kerod.core.similarity.IoUSimilarity kerod.core.similarity.DetrSimilarity","title":"Descendants"},{"location":"reference/kerod/core/similarity/#methods_2","text":"","title":"Methods"},{"location":"reference/kerod/core/similarity/#call_2","text":"def call ( self , inputs1 , inputs2 ) -> tensorflow . python . framework . ops . Tensor View Source @abstractmethod def call ( self , inputs1 , inputs2 ) -> tf . Tensor : pass","title":"call"},{"location":"reference/kerod/core/standard_fields/","text":"Module kerod.core.standard_fields None None View Source class BoxField: BOXES = 'bbox' KEYPOINTS = 'keypoints' LABELS = 'label' MASKS = 'masks' NUM_BOXES = 'num_boxes' SCORES = 'scores' WEIGHTS = 'weights' class DatasetField: IMAGES = 'images' IMAGES_INFO = 'images_information' IMAGES_PMASK = 'images_padding_mask' Classes BoxField class BoxField ( / , * args , ** kwargs ) Class variables BOXES KEYPOINTS LABELS MASKS NUM_BOXES SCORES WEIGHTS DatasetField class DatasetField ( / , * args , ** kwargs ) Class variables IMAGES IMAGES_INFO IMAGES_PMASK","title":"Standard Fields"},{"location":"reference/kerod/core/standard_fields/#module-kerodcorestandard_fields","text":"None None View Source class BoxField: BOXES = 'bbox' KEYPOINTS = 'keypoints' LABELS = 'label' MASKS = 'masks' NUM_BOXES = 'num_boxes' SCORES = 'scores' WEIGHTS = 'weights' class DatasetField: IMAGES = 'images' IMAGES_INFO = 'images_information' IMAGES_PMASK = 'images_padding_mask'","title":"Module kerod.core.standard_fields"},{"location":"reference/kerod/core/standard_fields/#classes","text":"","title":"Classes"},{"location":"reference/kerod/core/standard_fields/#boxfield","text":"class BoxField ( / , * args , ** kwargs )","title":"BoxField"},{"location":"reference/kerod/core/standard_fields/#class-variables","text":"BOXES KEYPOINTS LABELS MASKS NUM_BOXES SCORES WEIGHTS","title":"Class variables"},{"location":"reference/kerod/core/standard_fields/#datasetfield","text":"class DatasetField ( / , * args , ** kwargs )","title":"DatasetField"},{"location":"reference/kerod/core/standard_fields/#class-variables_1","text":"IMAGES IMAGES_INFO IMAGES_PMASK","title":"Class variables"},{"location":"reference/kerod/core/target_assigner/","text":"Module kerod.core.target_assigner [Documentation taken from tensorflow/models/object_detection]. The code has been completely rewritten. Base target assigner module. The job of a TargetAssigner is, for a given set of anchors (bounding boxes) and groundtruth detections (bounding boxes), to assign classification and regression targets to each anchor as well as weights to each anchor (specifying, e.g., which anchors should not contribute to training loss). It assigns classification/regression targets by performing the following steps: Computing pairwise similarity between anchors and groundtruth boxes using a provided RegionSimilarity Calculator Computing a matching based on the similarity matrix using a provided Matcher Assigning regression targets based on the matching and a provided BoxCoder Assigning classification targets based on the matching and groundtruth labels View Source \"\"\"[Documentation taken from tensorflow/models/object_detection]. The code has been completely rewritten. Base target assigner module. The job of a TargetAssigner is, for a given set of anchors (bounding boxes) and groundtruth detections (bounding boxes), to assign classification and regression targets to each anchor as well as weights to each anchor (specifying, e.g., which anchors should not contribute to training loss). It assigns classification/regression targets by performing the following steps: 1. Computing pairwise similarity between anchors and groundtruth boxes using a provided RegionSimilarity Calculator 2. Computing a matching based on the similarity matrix using a provided Matcher 3. Assigning regression targets based on the matching and a provided BoxCoder 4. Assigning classification targets based on the matching and groundtruth labels \"\"\" from typing import Callable import tensorflow as tf from tensorflow.keras import backend as K from kerod.core.matcher import Matcher from kerod.core.standard_fields import BoxField from kerod.utils import item_assignment , get_full_indices class TargetAssigner : \"\"\"Target assigner to compute classification and regression targets.\"\"\" def __init__ ( self , similarity_calc : Callable , matcher : Matcher , box_encoder : Callable , negative_class_weight = 0. , positive_class_weight = 1. , dtype = None ): \"\"\"Construct Object Detection Target Assigner. Arguments: - *similarity_calc*: a method wich allow to compute a similarity between two batch of boxes - *matcher*: an od.core.Matcher used to match groundtruth to anchors. - *box_encoder*: a method which allow to encode matching groundtruth boxes with respect to anchors. - *negative_class_weight*: A negative_class can be an unmatched anchors or a padded boxes. All negative classes will have a associated set to this corresponding value for the classification target. - *positive_class_weight*: A positive_class is a matched foreground object \"\"\" self . _similarity_calc = similarity_calc self . _matcher = matcher self . _box_encoder = box_encoder if dtype is None : dtype = K . floatx () self . dtype = dtype self . negative_class_weight = tf . constant ( negative_class_weight , dtype = dtype ) self . positive_class_weight = tf . constant ( positive_class_weight , dtype = dtype ) @property def box_encoder ( self ): return self . _box_encoder def assign ( self , anchors : dict , groundtruth : dict ): \"\"\"Assign classification and regression targets to each anchor. For a given set of anchors and groundtruth detections, match anchors to groundtruth and assign classification and regression targets to each anchor as well as weights based on the resulting match (specifying, e.g., which anchors should not contribute to training loss). Anchors that are not matched to anything are given a classification target of self._unmatched_cls_target which can be specified via the constructor. Arguments: - *anchors*: a dict representing a batch of M anchors 1. BoxField.BOXES: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. - *groundtruth*: a dict representing a batch of M groundtruth boxes 1. BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape 2. BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] 3. BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] Returns: - *y_true*: A dict with : - *BoxField.LABELS*: a tensor with shape [batch_size, num_anchors] - *BoxField.BOXES*: a tensor with shape [batch_size, num_anchors, box_code_dimension] - *weights*: A dict with: - *BoxField.LABELS*: a tensor with shape [batch_size, num_anchors], - *BoxField.BOXES*: a tensor with shape [batch_size, num_anchors], \"\"\" shape = tf . shape ( groundtruth [ BoxField . BOXES ]) batch_size = shape [ 0 ] num_gt_boxes = shape [ 1 ] groundtruth_labels = groundtruth . get ( BoxField . LABELS ) groundtruth_weights = groundtruth . get ( BoxField . WEIGHTS ) if groundtruth_weights is None : groundtruth_weights = tf . ones ([ batch_size , num_gt_boxes ], self . dtype ) match_quality_matrix = self . _similarity_calc ( groundtruth , anchors ) matches , matched_labels = self . _matcher ( match_quality_matrix , groundtruth [ BoxField . NUM_BOXES ]) reg_targets = self . _create_regression_targets ( anchors , groundtruth , matches , matched_labels ) cls_targets = self . _create_classification_targets ( groundtruth_labels , matches , matched_labels ) groundtruth_weights = self . gather ( groundtruth_weights , matches ) reg_weights = self . _create_regression_weights ( groundtruth_weights , matched_labels ) cls_weights = self . _create_classification_weights ( groundtruth_weights , matched_labels ) y_true = { BoxField . LABELS : tf . cast ( cls_targets , tf . int32 ), BoxField . BOXES : tf . cast ( reg_targets , self . dtype ) } weights = { BoxField . LABELS : tf . cast ( cls_weights , self . dtype ), BoxField . BOXES : tf . cast ( reg_weights , self . dtype ) } return y_true , weights def gather ( self , tensor , indices ): indices = get_full_indices ( indices ) return tf . gather_nd ( tensor , indices ) def _create_regression_targets ( self , anchors : dict , groundtruth : dict , matches : tf . Tensor , matched_labels : tf . Tensor ) -> tf . Tensor : \"\"\"Returns a regression target for each anchor. Arguments: - *anchors*: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. - *groundtruth*: a dict representing a batch of M groundtruth boxes 1. BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape 2. BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] 3. BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] - *matches*: a tensor of float32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int8 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: *reg_targets*: A tensor with shape [N, box_code_dimension] \"\"\" matched_gt_boxes = self . gather ( groundtruth [ BoxField . BOXES ], matches ) matched_reg_targets = self . _box_encoder ( matched_gt_boxes , anchors [ BoxField . BOXES ]) # Zero out the unmatched and ignored regression targets. unmatched_ignored_reg_targets = tf . zeros_like ( matched_reg_targets , dtype = matched_reg_targets . dtype ) matched_anchors_mask = matched_labels >= 1 reg_targets = tf . where ( matched_anchors_mask [ ... , None ], x = matched_reg_targets , y = unmatched_ignored_reg_targets ) return reg_targets def _create_classification_targets ( self , groundtruth_labels : tf . Tensor , matches : tf . Tensor , matched_labels : tf . Tensor ): \"\"\"Create classification targets for each anchor. Assign a classification target of for each anchor to the matching groundtruth label that is provided by match. Anchors that are not matched to anything are given the target self._unmatched_cls_target Arguments: - *groundtruth_labels*: a tensor of shape [num_gt_boxes, d_1, ... d_k] with labels for each of the ground_truth boxes. The subshape [d_1, ... d_k] can be empty (corresponding to scalar labels). - *matches*: a tensor of float32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: A tensor of shape [num_anchors, d_1, d_2 ... d_k], where the subshape [d_1, ..., d_k] is compatible with groundtruth_labels which has shape [num_gt_boxes, d_1, d_2, ... d_k]. \"\"\" gathered_tensor = self . gather ( groundtruth_labels , matches ) # Set all the match values inferior or equal to 0 to background_classes indicator = matched_labels <= 0 gathered_tensor = item_assignment ( gathered_tensor , indicator , 0 ) return gathered_tensor def _create_regression_weights ( self , groundtruth_weights : tf . Tensor , matched_labels : tf . Tensor ): \"\"\"Set regression weight for each anchor. Only positive anchors are set to contribute to the regression loss, so this method returns a weight of 1 for every positive anchor and 0 for every negative anchor. Arguments: - *groundtruth_weights*: a float tensor of shape [M] indicating the weight to assign to all anchors match to a particular groundtruth box. - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: A tensor of shape [batch_size, num_anchors] representing the box regression weights. \"\"\" indicator = matched_labels > 0 weights = tf . where ( indicator , groundtruth_weights , 0 ) return weights def _create_classification_weights ( self , groundtruth_weights : tf . Tensor , matched_labels : tf . Tensor ): \"\"\"Create classification weights for each anchor. Positive (matched) anchors are associated with a weight of positive_class_weight and negative (unmatched) anchors are associated with a weight of negative_class_weight. When anchors are ignored, weights are set to zero. By default, both positive/negative weights are set to 1.0, but they can be adjusted to handle class imbalance (which is almost always the case in object detection). Arguments: - *groundtruth_weights*: a float tensor of shape [M] indicating the weight to assign to all anchors match to a particular groundtruth box. - *match_labels*: a tensor of int8 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: A tensor of shape [batch_size, num_anchors] representing classification weights. \"\"\" indicator = matched_labels < 0 weights = tf . where ( indicator , self . negative_class_weight , groundtruth_weights ) indicator = matched_labels == 0 weights = tf . where ( indicator , self . positive_class_weight , weights ) return weights Classes TargetAssigner class TargetAssigner ( similarity_calc : Callable , matcher : kerod . core . matcher . Matcher , box_encoder : Callable , negative_class_weight = 0.0 , positive_class_weight = 1.0 , dtype = None ) Methods assign def assign ( self , anchors : dict , groundtruth : dict ) Assign classification and regression targets to each anchor. For a given set of anchors and groundtruth detections, match anchors to groundtruth and assign classification and regression targets to each anchor as well as weights based on the resulting match (specifying, e.g., which anchors should not contribute to training loss). Anchors that are not matched to anything are given a classification target of self._unmatched_cls_target which can be specified via the constructor. Arguments: anchors : a dict representing a batch of M anchors BoxField.BOXES: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. groundtruth : a dict representing a batch of M groundtruth boxes BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] Returns: y_true : A dict with : BoxField.LABELS : a tensor with shape [batch_size, num_anchors] BoxField.BOXES : a tensor with shape [batch_size, num_anchors, box_code_dimension] weights : A dict with: BoxField.LABELS : a tensor with shape [batch_size, num_anchors], BoxField.BOXES : a tensor with shape [batch_size, num_anchors], View Source def assign ( self , anchors : dict , groundtruth : dict ) : \"\"\"Assign classification and regression targets to each anchor. For a given set of anchors and groundtruth detections, match anchors to groundtruth and assign classification and regression targets to each anchor as well as weights based on the resulting match (specifying, e.g., which anchors should not contribute to training loss). Anchors that are not matched to anything are given a classification target of self._unmatched_cls_target which can be specified via the constructor. Arguments: - *anchors*: a dict representing a batch of M anchors 1. BoxField.BOXES: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. - *groundtruth*: a dict representing a batch of M groundtruth boxes 1. BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape 2. BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] 3. BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] Returns: - *y_true*: A dict with : - *BoxField.LABELS*: a tensor with shape [batch_size, num_anchors] - *BoxField.BOXES*: a tensor with shape [batch_size, num_anchors, box_code_dimension] - *weights*: A dict with: - *BoxField.LABELS*: a tensor with shape [batch_size, num_anchors], - *BoxField.BOXES*: a tensor with shape [batch_size, num_anchors], \"\"\" shape = tf . shape ( groundtruth [ BoxField.BOXES ] ) batch_size = shape [ 0 ] num_gt_boxes = shape [ 1 ] groundtruth_labels = groundtruth . get ( BoxField . LABELS ) groundtruth_weights = groundtruth . get ( BoxField . WEIGHTS ) if groundtruth_weights is None : groundtruth_weights = tf . ones ( [ batch_size, num_gt_boxes ] , self . dtype ) match_quality_matrix = self . _similarity_calc ( groundtruth , anchors ) matches , matched_labels = self . _matcher ( match_quality_matrix , groundtruth [ BoxField.NUM_BOXES ] ) reg_targets = self . _create_regression_targets ( anchors , groundtruth , matches , matched_labels ) cls_targets = self . _create_classification_targets ( groundtruth_labels , matches , matched_labels ) groundtruth_weights = self . gather ( groundtruth_weights , matches ) reg_weights = self . _create_regression_weights ( groundtruth_weights , matched_labels ) cls_weights = self . _create_classification_weights ( groundtruth_weights , matched_labels ) y_true = { BoxField . LABELS : tf . cast ( cls_targets , tf . int32 ), BoxField . BOXES : tf . cast ( reg_targets , self . dtype ) } weights = { BoxField . LABELS : tf . cast ( cls_weights , self . dtype ), BoxField . BOXES : tf . cast ( reg_weights , self . dtype ) } return y_true , weights gather def gather ( self , tensor , indices ) View Source def gather ( self , tensor , indices ) : indices = get_full_indices ( indices ) return tf . gather_nd ( tensor , indices )","title":"Target Assigner"},{"location":"reference/kerod/core/target_assigner/#module-kerodcoretarget_assigner","text":"[Documentation taken from tensorflow/models/object_detection]. The code has been completely rewritten. Base target assigner module. The job of a TargetAssigner is, for a given set of anchors (bounding boxes) and groundtruth detections (bounding boxes), to assign classification and regression targets to each anchor as well as weights to each anchor (specifying, e.g., which anchors should not contribute to training loss). It assigns classification/regression targets by performing the following steps: Computing pairwise similarity between anchors and groundtruth boxes using a provided RegionSimilarity Calculator Computing a matching based on the similarity matrix using a provided Matcher Assigning regression targets based on the matching and a provided BoxCoder Assigning classification targets based on the matching and groundtruth labels View Source \"\"\"[Documentation taken from tensorflow/models/object_detection]. The code has been completely rewritten. Base target assigner module. The job of a TargetAssigner is, for a given set of anchors (bounding boxes) and groundtruth detections (bounding boxes), to assign classification and regression targets to each anchor as well as weights to each anchor (specifying, e.g., which anchors should not contribute to training loss). It assigns classification/regression targets by performing the following steps: 1. Computing pairwise similarity between anchors and groundtruth boxes using a provided RegionSimilarity Calculator 2. Computing a matching based on the similarity matrix using a provided Matcher 3. Assigning regression targets based on the matching and a provided BoxCoder 4. Assigning classification targets based on the matching and groundtruth labels \"\"\" from typing import Callable import tensorflow as tf from tensorflow.keras import backend as K from kerod.core.matcher import Matcher from kerod.core.standard_fields import BoxField from kerod.utils import item_assignment , get_full_indices class TargetAssigner : \"\"\"Target assigner to compute classification and regression targets.\"\"\" def __init__ ( self , similarity_calc : Callable , matcher : Matcher , box_encoder : Callable , negative_class_weight = 0. , positive_class_weight = 1. , dtype = None ): \"\"\"Construct Object Detection Target Assigner. Arguments: - *similarity_calc*: a method wich allow to compute a similarity between two batch of boxes - *matcher*: an od.core.Matcher used to match groundtruth to anchors. - *box_encoder*: a method which allow to encode matching groundtruth boxes with respect to anchors. - *negative_class_weight*: A negative_class can be an unmatched anchors or a padded boxes. All negative classes will have a associated set to this corresponding value for the classification target. - *positive_class_weight*: A positive_class is a matched foreground object \"\"\" self . _similarity_calc = similarity_calc self . _matcher = matcher self . _box_encoder = box_encoder if dtype is None : dtype = K . floatx () self . dtype = dtype self . negative_class_weight = tf . constant ( negative_class_weight , dtype = dtype ) self . positive_class_weight = tf . constant ( positive_class_weight , dtype = dtype ) @property def box_encoder ( self ): return self . _box_encoder def assign ( self , anchors : dict , groundtruth : dict ): \"\"\"Assign classification and regression targets to each anchor. For a given set of anchors and groundtruth detections, match anchors to groundtruth and assign classification and regression targets to each anchor as well as weights based on the resulting match (specifying, e.g., which anchors should not contribute to training loss). Anchors that are not matched to anything are given a classification target of self._unmatched_cls_target which can be specified via the constructor. Arguments: - *anchors*: a dict representing a batch of M anchors 1. BoxField.BOXES: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. - *groundtruth*: a dict representing a batch of M groundtruth boxes 1. BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape 2. BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] 3. BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] Returns: - *y_true*: A dict with : - *BoxField.LABELS*: a tensor with shape [batch_size, num_anchors] - *BoxField.BOXES*: a tensor with shape [batch_size, num_anchors, box_code_dimension] - *weights*: A dict with: - *BoxField.LABELS*: a tensor with shape [batch_size, num_anchors], - *BoxField.BOXES*: a tensor with shape [batch_size, num_anchors], \"\"\" shape = tf . shape ( groundtruth [ BoxField . BOXES ]) batch_size = shape [ 0 ] num_gt_boxes = shape [ 1 ] groundtruth_labels = groundtruth . get ( BoxField . LABELS ) groundtruth_weights = groundtruth . get ( BoxField . WEIGHTS ) if groundtruth_weights is None : groundtruth_weights = tf . ones ([ batch_size , num_gt_boxes ], self . dtype ) match_quality_matrix = self . _similarity_calc ( groundtruth , anchors ) matches , matched_labels = self . _matcher ( match_quality_matrix , groundtruth [ BoxField . NUM_BOXES ]) reg_targets = self . _create_regression_targets ( anchors , groundtruth , matches , matched_labels ) cls_targets = self . _create_classification_targets ( groundtruth_labels , matches , matched_labels ) groundtruth_weights = self . gather ( groundtruth_weights , matches ) reg_weights = self . _create_regression_weights ( groundtruth_weights , matched_labels ) cls_weights = self . _create_classification_weights ( groundtruth_weights , matched_labels ) y_true = { BoxField . LABELS : tf . cast ( cls_targets , tf . int32 ), BoxField . BOXES : tf . cast ( reg_targets , self . dtype ) } weights = { BoxField . LABELS : tf . cast ( cls_weights , self . dtype ), BoxField . BOXES : tf . cast ( reg_weights , self . dtype ) } return y_true , weights def gather ( self , tensor , indices ): indices = get_full_indices ( indices ) return tf . gather_nd ( tensor , indices ) def _create_regression_targets ( self , anchors : dict , groundtruth : dict , matches : tf . Tensor , matched_labels : tf . Tensor ) -> tf . Tensor : \"\"\"Returns a regression target for each anchor. Arguments: - *anchors*: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. - *groundtruth*: a dict representing a batch of M groundtruth boxes 1. BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape 2. BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] 3. BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] - *matches*: a tensor of float32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int8 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: *reg_targets*: A tensor with shape [N, box_code_dimension] \"\"\" matched_gt_boxes = self . gather ( groundtruth [ BoxField . BOXES ], matches ) matched_reg_targets = self . _box_encoder ( matched_gt_boxes , anchors [ BoxField . BOXES ]) # Zero out the unmatched and ignored regression targets. unmatched_ignored_reg_targets = tf . zeros_like ( matched_reg_targets , dtype = matched_reg_targets . dtype ) matched_anchors_mask = matched_labels >= 1 reg_targets = tf . where ( matched_anchors_mask [ ... , None ], x = matched_reg_targets , y = unmatched_ignored_reg_targets ) return reg_targets def _create_classification_targets ( self , groundtruth_labels : tf . Tensor , matches : tf . Tensor , matched_labels : tf . Tensor ): \"\"\"Create classification targets for each anchor. Assign a classification target of for each anchor to the matching groundtruth label that is provided by match. Anchors that are not matched to anything are given the target self._unmatched_cls_target Arguments: - *groundtruth_labels*: a tensor of shape [num_gt_boxes, d_1, ... d_k] with labels for each of the ground_truth boxes. The subshape [d_1, ... d_k] can be empty (corresponding to scalar labels). - *matches*: a tensor of float32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: A tensor of shape [num_anchors, d_1, d_2 ... d_k], where the subshape [d_1, ..., d_k] is compatible with groundtruth_labels which has shape [num_gt_boxes, d_1, d_2, ... d_k]. \"\"\" gathered_tensor = self . gather ( groundtruth_labels , matches ) # Set all the match values inferior or equal to 0 to background_classes indicator = matched_labels <= 0 gathered_tensor = item_assignment ( gathered_tensor , indicator , 0 ) return gathered_tensor def _create_regression_weights ( self , groundtruth_weights : tf . Tensor , matched_labels : tf . Tensor ): \"\"\"Set regression weight for each anchor. Only positive anchors are set to contribute to the regression loss, so this method returns a weight of 1 for every positive anchor and 0 for every negative anchor. Arguments: - *groundtruth_weights*: a float tensor of shape [M] indicating the weight to assign to all anchors match to a particular groundtruth box. - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: A tensor of shape [batch_size, num_anchors] representing the box regression weights. \"\"\" indicator = matched_labels > 0 weights = tf . where ( indicator , groundtruth_weights , 0 ) return weights def _create_classification_weights ( self , groundtruth_weights : tf . Tensor , matched_labels : tf . Tensor ): \"\"\"Create classification weights for each anchor. Positive (matched) anchors are associated with a weight of positive_class_weight and negative (unmatched) anchors are associated with a weight of negative_class_weight. When anchors are ignored, weights are set to zero. By default, both positive/negative weights are set to 1.0, but they can be adjusted to handle class imbalance (which is almost always the case in object detection). Arguments: - *groundtruth_weights*: a float tensor of shape [M] indicating the weight to assign to all anchors match to a particular groundtruth box. - *match_labels*: a tensor of int8 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: A tensor of shape [batch_size, num_anchors] representing classification weights. \"\"\" indicator = matched_labels < 0 weights = tf . where ( indicator , self . negative_class_weight , groundtruth_weights ) indicator = matched_labels == 0 weights = tf . where ( indicator , self . positive_class_weight , weights ) return weights","title":"Module kerod.core.target_assigner"},{"location":"reference/kerod/core/target_assigner/#classes","text":"","title":"Classes"},{"location":"reference/kerod/core/target_assigner/#targetassigner","text":"class TargetAssigner ( similarity_calc : Callable , matcher : kerod . core . matcher . Matcher , box_encoder : Callable , negative_class_weight = 0.0 , positive_class_weight = 1.0 , dtype = None )","title":"TargetAssigner"},{"location":"reference/kerod/core/target_assigner/#methods","text":"","title":"Methods"},{"location":"reference/kerod/core/target_assigner/#assign","text":"def assign ( self , anchors : dict , groundtruth : dict ) Assign classification and regression targets to each anchor. For a given set of anchors and groundtruth detections, match anchors to groundtruth and assign classification and regression targets to each anchor as well as weights based on the resulting match (specifying, e.g., which anchors should not contribute to training loss). Anchors that are not matched to anything are given a classification target of self._unmatched_cls_target which can be specified via the constructor. Arguments: anchors : a dict representing a batch of M anchors BoxField.BOXES: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. groundtruth : a dict representing a batch of M groundtruth boxes BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] Returns: y_true : A dict with : BoxField.LABELS : a tensor with shape [batch_size, num_anchors] BoxField.BOXES : a tensor with shape [batch_size, num_anchors, box_code_dimension] weights : A dict with: BoxField.LABELS : a tensor with shape [batch_size, num_anchors], BoxField.BOXES : a tensor with shape [batch_size, num_anchors], View Source def assign ( self , anchors : dict , groundtruth : dict ) : \"\"\"Assign classification and regression targets to each anchor. For a given set of anchors and groundtruth detections, match anchors to groundtruth and assign classification and regression targets to each anchor as well as weights based on the resulting match (specifying, e.g., which anchors should not contribute to training loss). Anchors that are not matched to anything are given a classification target of self._unmatched_cls_target which can be specified via the constructor. Arguments: - *anchors*: a dict representing a batch of M anchors 1. BoxField.BOXES: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. - *groundtruth*: a dict representing a batch of M groundtruth boxes 1. BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape 2. BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] 3. BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] Returns: - *y_true*: A dict with : - *BoxField.LABELS*: a tensor with shape [batch_size, num_anchors] - *BoxField.BOXES*: a tensor with shape [batch_size, num_anchors, box_code_dimension] - *weights*: A dict with: - *BoxField.LABELS*: a tensor with shape [batch_size, num_anchors], - *BoxField.BOXES*: a tensor with shape [batch_size, num_anchors], \"\"\" shape = tf . shape ( groundtruth [ BoxField.BOXES ] ) batch_size = shape [ 0 ] num_gt_boxes = shape [ 1 ] groundtruth_labels = groundtruth . get ( BoxField . LABELS ) groundtruth_weights = groundtruth . get ( BoxField . WEIGHTS ) if groundtruth_weights is None : groundtruth_weights = tf . ones ( [ batch_size, num_gt_boxes ] , self . dtype ) match_quality_matrix = self . _similarity_calc ( groundtruth , anchors ) matches , matched_labels = self . _matcher ( match_quality_matrix , groundtruth [ BoxField.NUM_BOXES ] ) reg_targets = self . _create_regression_targets ( anchors , groundtruth , matches , matched_labels ) cls_targets = self . _create_classification_targets ( groundtruth_labels , matches , matched_labels ) groundtruth_weights = self . gather ( groundtruth_weights , matches ) reg_weights = self . _create_regression_weights ( groundtruth_weights , matched_labels ) cls_weights = self . _create_classification_weights ( groundtruth_weights , matched_labels ) y_true = { BoxField . LABELS : tf . cast ( cls_targets , tf . int32 ), BoxField . BOXES : tf . cast ( reg_targets , self . dtype ) } weights = { BoxField . LABELS : tf . cast ( cls_weights , self . dtype ), BoxField . BOXES : tf . cast ( reg_weights , self . dtype ) } return y_true , weights","title":"assign"},{"location":"reference/kerod/core/target_assigner/#gather","text":"def gather ( self , tensor , indices ) View Source def gather ( self , tensor , indices ) : indices = get_full_indices ( indices ) return tf . gather_nd ( tensor , indices )","title":"gather"},{"location":"reference/kerod/dataset/","text":"Module kerod.dataset None None Sub-modules kerod.dataset.augmentation kerod.dataset.preprocessing kerod.dataset.utils","title":"Index"},{"location":"reference/kerod/dataset/#module-keroddataset","text":"None None","title":"Module kerod.dataset"},{"location":"reference/kerod/dataset/#sub-modules","text":"kerod.dataset.augmentation kerod.dataset.preprocessing kerod.dataset.utils","title":"Sub-modules"},{"location":"reference/kerod/dataset/augmentation/","text":"Module kerod.dataset.augmentation None None View Source from typing import Dict import tensorflow as tf from kerod.core import box_ops from kerod.core.standard_fields import BoxField from kerod.dataset.utils import filter_bad_area def random_horizontal_flip ( image : tf . Tensor , boxes : tf . Tensor , seed = None ): \"\"\"Randomly flips the image and detections horizontally. The probability of flipping the image is 50%. Arguments: - *image*: rank 3 float32 tensor with shape [height, width, channels]. - *boxes*: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Returns: - *image*: image which is the same shape as input image. - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. \"\"\" with tf . name_scope ( 'RandomHorizontalFlip' ): uniform_random = tf . random . uniform ([], 0 , 1.0 , seed = seed ) if uniform_random > .5 : image = tf . image . flip_left_right ( image ) boxes = box_ops . flip_left_right ( boxes ) return image , boxes def _random_crop ( value : tf . Tensor , size : tf . Tensor , seed = None ) -> ( tf . Tensor , tf . Tensor ): \"\"\"Randomly crops a tensor to a given size in a deterministic manner. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *value*: Input tensor to crop. - *size*: 1-D tensor with size the rank of `value`. - *seed*: A shape [2] Tensor, the seed to the random number generator. Must have Returns: - *crop*: A cropped tensor of the same rank as `value` and shape `size`. - *top_left_corner*: A 2-D tensor of int and shape [1, (y, x)] \"\"\" with tf . name_scope ( \"RandomCrop\" ): value = tf . convert_to_tensor ( value , name = \"value\" ) size = tf . convert_to_tensor ( size , dtype = tf . int32 , name = \"size\" ) shape = tf . shape ( value ) limit = shape - size + 1 offset = tf . random . uniform ( tf . shape ( shape ), dtype = size . dtype , maxval = size . dtype . max , seed = seed ) % limit return tf . slice ( value , offset , size ), offset [: 2 ] def random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ], seed = None ) -> ( tf . Tensor , tf . Tensor ): \"\"\"Randomly crops a tensor to a given size in a deterministic manner. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *crop*: A cropped tensor of the same rank as `value` and shape `size`. - *groundtruths*: Diction 1. bbox: 2-D float32 tensor containing the bounding boxes -> [N_crop <=N, 4]. If cropped, the boxes with no area or outside the crop or removed. 2. label: A tensor of shape [N_crop <= N, ] \"\"\" boxes = groundtruths [ BoxField . BOXES ] crop , top_left_corner = _random_crop ( image , size , seed = seed ) shape = tf . cast ( tf . shape ( image )[: 2 ], boxes . dtype ) size = tf . tile ( tf . constant ( size [: 2 ], boxes . dtype )[ None ], ( 1 , 2 )) # scale the boxes to the size of the image boxes *= tf . tile ( shape [ tf . newaxis ], ( 1 , 2 )) top_left_corner = tf . tile ( top_left_corner [ tf . newaxis ], ( 1 , 2 )) top_left_corner = tf . cast ( top_left_corner , boxes . dtype ) # Translate according to top_left_corner # top_left_corner is now (y=0, x=0) cropped_boxes = boxes - top_left_corner # MinClip to 0 and MaxClip to size all the coords outside the crop. cropped_boxes = tf . maximum ( cropped_boxes , tf . cast ( 0 , boxes . dtype )) cropped_boxes = tf . minimum ( cropped_boxes , size ) # Renomarlized to have the boxes between 0 and 1 cropped_boxes = cropped_boxes / size # Copy groundtruths cropped_gt = { key : val for key , val in groundtruths . items ()} cropped_gt [ BoxField . BOXES ] = cropped_boxes return crop , filter_bad_area ( cropped_gt ) def random_random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ], seed = None ) -> ( tf . Tensor , tf . Tensor ): \"\"\"Will `randomly` perform a random crop of a tensor to a given size. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop or not. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *image*: Either a cropped tensor or the same image of the same rank as `value` and shape `size`. - *boxes*: 2-D float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form, meaning their coordinates vary between [0, 1]. If cropped, the boxes with no area or outside the crop or removed. \"\"\" with tf . name_scope ( 'RandomRandomCrop' ): uniform_random = tf . random . uniform ([], 0 , 1.0 , seed = seed ) if uniform_random > .5 : image , groundtruths = random_crop ( image , size , groundtruths , seed = seed ) return image , groundtruths Functions random_crop def random_crop ( image : tensorflow . python . framework . ops . Tensor , size : tensorflow . python . framework . ops . Tensor , groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ], seed = None ) -> ( < class ' tensorflow . python . framework . ops . Tensor '>, <class ' tensorflow . python . framework . ops . Tensor '>) Randomly crops a tensor to a given size in a deterministic manner. Slices a shape size portion out of value at a uniformly chosen offset. Requires value.shape >= size . If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with size = [crop_height, crop_width, 3] . Guarantees the same results given the same seed independent of how many times the function is called, and independent of global seed settings (e.g. tf.random.set_seed ). Arguments: image : Input tensor to crop. size : 1-D tensor with size the rank of image . groundtruths : A dict with the following keys: bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. label: A tensorf of shape [N, ] seed : A shape [2] Tensor, the seed to the random number generator. Returns: crop : A cropped tensor of the same rank as value and shape size . groundtruths : Diction bbox: 2-D float32 tensor containing the bounding boxes -> [N_crop <=N, 4]. If cropped, the boxes with no area or outside the crop or removed. label: A tensor of shape [N_crop <= N, ] View Source def random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ] , seed = None ) -> ( tf . Tensor , tf . Tensor ) : \" \"\" Randomly crops a tensor to a given size in a deterministic manner. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *crop*: A cropped tensor of the same rank as `value` and shape `size`. - *groundtruths*: Diction 1. bbox: 2-D float32 tensor containing the bounding boxes -> [N_crop <=N, 4]. If cropped, the boxes with no area or outside the crop or removed. 2. label: A tensor of shape [N_crop <= N, ] \"\" \" boxes = groundtruths [ BoxField . BOXES ] crop , top_left_corner = _random_crop ( image , size , seed = seed ) shape = tf . cast ( tf . shape ( image ) [ : 2 ] , boxes . dtype ) size = tf . tile ( tf . constant ( size [ : 2 ] , boxes . dtype ) [ None ] , ( 1 , 2 )) # scale the boxes to the size of the image boxes *= tf . tile ( shape [ tf . newaxis ] , ( 1 , 2 )) top_left_corner = tf . tile ( top_left_corner [ tf . newaxis ] , ( 1 , 2 )) top_left_corner = tf . cast ( top_left_corner , boxes . dtype ) # Translate according to top_left_corner # top_left_corner is now (y=0, x=0) cropped_boxes = boxes - top_left_corner # MinClip to 0 and MaxClip to size all the coords outside the crop. cropped_boxes = tf . maximum ( cropped_boxes , tf . cast ( 0 , boxes . dtype )) cropped_boxes = tf . minimum ( cropped_boxes , size ) # Renomarlized to have the boxes between 0 and 1 cropped_boxes = cropped_boxes / size # Copy groundtruths cropped_gt = { key : val for key , val in groundtruths . items () } cropped_gt [ BoxField . BOXES ] = cropped_boxes return crop , filter_bad_area ( cropped_gt ) random_horizontal_flip def random_horizontal_flip ( image : tensorflow . python . framework . ops . Tensor , boxes : tensorflow . python . framework . ops . Tensor , seed = None ) Randomly flips the image and detections horizontally. The probability of flipping the image is 50%. Arguments: image : rank 3 float32 tensor with shape [height, width, channels]. boxes : rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Returns: image : image which is the same shape as input image. boxes : rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. View Source def random_horizontal_flip ( image : tf . Tensor , boxes : tf . Tensor , seed = None ): \"\"\"Randomly flips the image and detections horizontally. The probability of flipping the image is 50%. Arguments: - *image*: rank 3 float32 tensor with shape [height, width, channels]. - *boxes*: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Returns: - *image*: image which is the same shape as input image. - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. \"\"\" with tf . name_scope ( 'RandomHorizontalFlip' ): uniform_random = tf . random . uniform ([], 0 , 1.0 , seed = seed ) if uniform_random > . 5 : image = tf . image . flip_left_right ( image ) boxes = box_ops . flip_left_right ( boxes ) return image , boxes random_random_crop def random_random_crop ( image : tensorflow . python . framework . ops . Tensor , size : tensorflow . python . framework . ops . Tensor , groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ], seed = None ) -> ( < class ' tensorflow . python . framework . ops . Tensor '>, <class ' tensorflow . python . framework . ops . Tensor '>) Will randomly perform a random crop of a tensor to a given size. Slices a shape size portion out of value at a uniformly chosen offset. Requires value.shape >= size . If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with size = [crop_height, crop_width, 3] . Guarantees the same results given the same seed independent of how many times the function is called, and independent of global seed settings (e.g. tf.random.set_seed ). Arguments: image : Input tensor to crop or not. size : 1-D tensor with size the rank of image . groundtruths : A dict with the following keys: bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. label: A tensorf of shape [N, ] seed : A shape [2] Tensor, the seed to the random number generator. Returns: image : Either a cropped tensor or the same image of the same rank as value and shape size . boxes : 2-D float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form, meaning their coordinates vary between [0, 1]. If cropped, the boxes with no area or outside the crop or removed. View Source def random_random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ] , seed = None ) -> ( tf . Tensor , tf . Tensor ) : \" \"\" Will `randomly` perform a random crop of a tensor to a given size. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop or not. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *image*: Either a cropped tensor or the same image of the same rank as `value` and shape `size`. - *boxes*: 2-D float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form, meaning their coordinates vary between [0, 1]. If cropped, the boxes with no area or outside the crop or removed. \"\" \" with tf . name_scope ( 'RandomRandomCrop' ) : uniform_random = tf . random . uniform ( [] , 0 , 1.0 , seed = seed ) if uniform_random > .5 : image , groundtruths = random_crop ( image , size , groundtruths , seed = seed ) return image , groundtruths","title":"Augmentation"},{"location":"reference/kerod/dataset/augmentation/#module-keroddatasetaugmentation","text":"None None View Source from typing import Dict import tensorflow as tf from kerod.core import box_ops from kerod.core.standard_fields import BoxField from kerod.dataset.utils import filter_bad_area def random_horizontal_flip ( image : tf . Tensor , boxes : tf . Tensor , seed = None ): \"\"\"Randomly flips the image and detections horizontally. The probability of flipping the image is 50%. Arguments: - *image*: rank 3 float32 tensor with shape [height, width, channels]. - *boxes*: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Returns: - *image*: image which is the same shape as input image. - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. \"\"\" with tf . name_scope ( 'RandomHorizontalFlip' ): uniform_random = tf . random . uniform ([], 0 , 1.0 , seed = seed ) if uniform_random > .5 : image = tf . image . flip_left_right ( image ) boxes = box_ops . flip_left_right ( boxes ) return image , boxes def _random_crop ( value : tf . Tensor , size : tf . Tensor , seed = None ) -> ( tf . Tensor , tf . Tensor ): \"\"\"Randomly crops a tensor to a given size in a deterministic manner. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *value*: Input tensor to crop. - *size*: 1-D tensor with size the rank of `value`. - *seed*: A shape [2] Tensor, the seed to the random number generator. Must have Returns: - *crop*: A cropped tensor of the same rank as `value` and shape `size`. - *top_left_corner*: A 2-D tensor of int and shape [1, (y, x)] \"\"\" with tf . name_scope ( \"RandomCrop\" ): value = tf . convert_to_tensor ( value , name = \"value\" ) size = tf . convert_to_tensor ( size , dtype = tf . int32 , name = \"size\" ) shape = tf . shape ( value ) limit = shape - size + 1 offset = tf . random . uniform ( tf . shape ( shape ), dtype = size . dtype , maxval = size . dtype . max , seed = seed ) % limit return tf . slice ( value , offset , size ), offset [: 2 ] def random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ], seed = None ) -> ( tf . Tensor , tf . Tensor ): \"\"\"Randomly crops a tensor to a given size in a deterministic manner. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *crop*: A cropped tensor of the same rank as `value` and shape `size`. - *groundtruths*: Diction 1. bbox: 2-D float32 tensor containing the bounding boxes -> [N_crop <=N, 4]. If cropped, the boxes with no area or outside the crop or removed. 2. label: A tensor of shape [N_crop <= N, ] \"\"\" boxes = groundtruths [ BoxField . BOXES ] crop , top_left_corner = _random_crop ( image , size , seed = seed ) shape = tf . cast ( tf . shape ( image )[: 2 ], boxes . dtype ) size = tf . tile ( tf . constant ( size [: 2 ], boxes . dtype )[ None ], ( 1 , 2 )) # scale the boxes to the size of the image boxes *= tf . tile ( shape [ tf . newaxis ], ( 1 , 2 )) top_left_corner = tf . tile ( top_left_corner [ tf . newaxis ], ( 1 , 2 )) top_left_corner = tf . cast ( top_left_corner , boxes . dtype ) # Translate according to top_left_corner # top_left_corner is now (y=0, x=0) cropped_boxes = boxes - top_left_corner # MinClip to 0 and MaxClip to size all the coords outside the crop. cropped_boxes = tf . maximum ( cropped_boxes , tf . cast ( 0 , boxes . dtype )) cropped_boxes = tf . minimum ( cropped_boxes , size ) # Renomarlized to have the boxes between 0 and 1 cropped_boxes = cropped_boxes / size # Copy groundtruths cropped_gt = { key : val for key , val in groundtruths . items ()} cropped_gt [ BoxField . BOXES ] = cropped_boxes return crop , filter_bad_area ( cropped_gt ) def random_random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ], seed = None ) -> ( tf . Tensor , tf . Tensor ): \"\"\"Will `randomly` perform a random crop of a tensor to a given size. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop or not. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *image*: Either a cropped tensor or the same image of the same rank as `value` and shape `size`. - *boxes*: 2-D float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form, meaning their coordinates vary between [0, 1]. If cropped, the boxes with no area or outside the crop or removed. \"\"\" with tf . name_scope ( 'RandomRandomCrop' ): uniform_random = tf . random . uniform ([], 0 , 1.0 , seed = seed ) if uniform_random > .5 : image , groundtruths = random_crop ( image , size , groundtruths , seed = seed ) return image , groundtruths","title":"Module kerod.dataset.augmentation"},{"location":"reference/kerod/dataset/augmentation/#functions","text":"","title":"Functions"},{"location":"reference/kerod/dataset/augmentation/#random_crop","text":"def random_crop ( image : tensorflow . python . framework . ops . Tensor , size : tensorflow . python . framework . ops . Tensor , groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ], seed = None ) -> ( < class ' tensorflow . python . framework . ops . Tensor '>, <class ' tensorflow . python . framework . ops . Tensor '>) Randomly crops a tensor to a given size in a deterministic manner. Slices a shape size portion out of value at a uniformly chosen offset. Requires value.shape >= size . If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with size = [crop_height, crop_width, 3] . Guarantees the same results given the same seed independent of how many times the function is called, and independent of global seed settings (e.g. tf.random.set_seed ). Arguments: image : Input tensor to crop. size : 1-D tensor with size the rank of image . groundtruths : A dict with the following keys: bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. label: A tensorf of shape [N, ] seed : A shape [2] Tensor, the seed to the random number generator. Returns: crop : A cropped tensor of the same rank as value and shape size . groundtruths : Diction bbox: 2-D float32 tensor containing the bounding boxes -> [N_crop <=N, 4]. If cropped, the boxes with no area or outside the crop or removed. label: A tensor of shape [N_crop <= N, ] View Source def random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ] , seed = None ) -> ( tf . Tensor , tf . Tensor ) : \" \"\" Randomly crops a tensor to a given size in a deterministic manner. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *crop*: A cropped tensor of the same rank as `value` and shape `size`. - *groundtruths*: Diction 1. bbox: 2-D float32 tensor containing the bounding boxes -> [N_crop <=N, 4]. If cropped, the boxes with no area or outside the crop or removed. 2. label: A tensor of shape [N_crop <= N, ] \"\" \" boxes = groundtruths [ BoxField . BOXES ] crop , top_left_corner = _random_crop ( image , size , seed = seed ) shape = tf . cast ( tf . shape ( image ) [ : 2 ] , boxes . dtype ) size = tf . tile ( tf . constant ( size [ : 2 ] , boxes . dtype ) [ None ] , ( 1 , 2 )) # scale the boxes to the size of the image boxes *= tf . tile ( shape [ tf . newaxis ] , ( 1 , 2 )) top_left_corner = tf . tile ( top_left_corner [ tf . newaxis ] , ( 1 , 2 )) top_left_corner = tf . cast ( top_left_corner , boxes . dtype ) # Translate according to top_left_corner # top_left_corner is now (y=0, x=0) cropped_boxes = boxes - top_left_corner # MinClip to 0 and MaxClip to size all the coords outside the crop. cropped_boxes = tf . maximum ( cropped_boxes , tf . cast ( 0 , boxes . dtype )) cropped_boxes = tf . minimum ( cropped_boxes , size ) # Renomarlized to have the boxes between 0 and 1 cropped_boxes = cropped_boxes / size # Copy groundtruths cropped_gt = { key : val for key , val in groundtruths . items () } cropped_gt [ BoxField . BOXES ] = cropped_boxes return crop , filter_bad_area ( cropped_gt )","title":"random_crop"},{"location":"reference/kerod/dataset/augmentation/#random_horizontal_flip","text":"def random_horizontal_flip ( image : tensorflow . python . framework . ops . Tensor , boxes : tensorflow . python . framework . ops . Tensor , seed = None ) Randomly flips the image and detections horizontally. The probability of flipping the image is 50%. Arguments: image : rank 3 float32 tensor with shape [height, width, channels]. boxes : rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Returns: image : image which is the same shape as input image. boxes : rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. View Source def random_horizontal_flip ( image : tf . Tensor , boxes : tf . Tensor , seed = None ): \"\"\"Randomly flips the image and detections horizontally. The probability of flipping the image is 50%. Arguments: - *image*: rank 3 float32 tensor with shape [height, width, channels]. - *boxes*: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Returns: - *image*: image which is the same shape as input image. - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. \"\"\" with tf . name_scope ( 'RandomHorizontalFlip' ): uniform_random = tf . random . uniform ([], 0 , 1.0 , seed = seed ) if uniform_random > . 5 : image = tf . image . flip_left_right ( image ) boxes = box_ops . flip_left_right ( boxes ) return image , boxes","title":"random_horizontal_flip"},{"location":"reference/kerod/dataset/augmentation/#random_random_crop","text":"def random_random_crop ( image : tensorflow . python . framework . ops . Tensor , size : tensorflow . python . framework . ops . Tensor , groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ], seed = None ) -> ( < class ' tensorflow . python . framework . ops . Tensor '>, <class ' tensorflow . python . framework . ops . Tensor '>) Will randomly perform a random crop of a tensor to a given size. Slices a shape size portion out of value at a uniformly chosen offset. Requires value.shape >= size . If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with size = [crop_height, crop_width, 3] . Guarantees the same results given the same seed independent of how many times the function is called, and independent of global seed settings (e.g. tf.random.set_seed ). Arguments: image : Input tensor to crop or not. size : 1-D tensor with size the rank of image . groundtruths : A dict with the following keys: bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. label: A tensorf of shape [N, ] seed : A shape [2] Tensor, the seed to the random number generator. Returns: image : Either a cropped tensor or the same image of the same rank as value and shape size . boxes : 2-D float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form, meaning their coordinates vary between [0, 1]. If cropped, the boxes with no area or outside the crop or removed. View Source def random_random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ] , seed = None ) -> ( tf . Tensor , tf . Tensor ) : \" \"\" Will `randomly` perform a random crop of a tensor to a given size. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop or not. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *image*: Either a cropped tensor or the same image of the same rank as `value` and shape `size`. - *boxes*: 2-D float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form, meaning their coordinates vary between [0, 1]. If cropped, the boxes with no area or outside the crop or removed. \"\" \" with tf . name_scope ( 'RandomRandomCrop' ) : uniform_random = tf . random . uniform ( [] , 0 , 1.0 , seed = seed ) if uniform_random > .5 : image , groundtruths = random_crop ( image , size , groundtruths , seed = seed ) return image , groundtruths","title":"random_random_crop"},{"location":"reference/kerod/dataset/preprocessing/","text":"Module kerod.dataset.preprocessing None None View Source import tensorflow as tf from kerod . core import constants from kerod . core . standard_fields import BoxField , DatasetField from kerod . dataset . utils import filter_crowded_boxes , filter_bad_area from kerod . dataset import augmentation as aug def resize_to_min_dim ( image , short_edge_length , max_dimension ) : \"\"\"Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Arguments : - *image*: A np.array of size [height, width, channels]. - *short_edge_length*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *resized_image*: The input image resized with the aspect_ratio preserved in float32 Raises: ValueError: If the max_dimension is above `kerod.core.constants.MAX_IMAGE_SIZE` \"\"\" if max_dimension > constants . MAX_IMAGE_DIMENSION : raise ValueError ( f \"The max_dimension can only be inferior or equal to {constants.MAX_IMAGE_DIMENSION}\" ) shape = tf . shape ( image ) height = tf . cast ( shape [ 0 ], tf . float32 ) width = tf . cast ( shape [ 1 ], tf . float32 ) im_size_min = tf . minimum ( height , width ) im_size_max = tf . maximum ( height , width ) scale = short_edge_length / im_size_min # Prevent the biggest axis from being more than MAX_SIZE if tf . math . round ( scale * im_size_max ) > max_dimension: scale = max_dimension / im_size_max target_height = tf . cast ( height * scale , dtype = tf . int32 ) target_width = tf . cast ( width * scale , dtype = tf . int32 ) return tf . image . resize ( tf . expand_dims ( image , axis = 0 ), size = [ target_height , target_width ], method = tf . image . ResizeMethod . BILINEAR )[ 0 ] def preprocess ( inputs , bgr = True , horizontal_flip = True , random_crop_size = None , padded_mask = False ) : \"\"\"This operations performs a classical preprocessing operations for localization datasets: - COCO - Pascal Voc You can download easily those dataset using [tensorflow dataset](https://www.tensorflow.org/datasets/catalog/overview). Arguments: - *inputs*: It can be either a [FeaturesDict](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/FeaturesDict) or a dict. but it should have the following structures. ```python inputs = FeaturesDict({ 'image': Image(shape=(None, None, 3), dtype=tf.uint8), 'objects': Sequence({ 'area': Tensor(shape=(), dtype=tf.int64), # area 'bbox': BBoxFeature(shape=(4,), dtype=tf.float32), # The values are between 0 and 1 'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=80), }), }) ``` - *bgr*: Convert your input image to BGR (od.model.faster_rcnn.FasterRcnnFPNResnet50 needs it). If you have open your image with `tf.image.decode_image` will open an image in RGB. However, OpenCV will open it in BGR by default. - *horizontal_flip*: Activate the random horizontal flip. - *random_crop_size*: 1-D tensor with size the rank of `image` (e.g: (400, 600, 0)). - *padded_mask*: If set to true return a mask of 1 of the image. When padded we will know which parts is from the original image. Returns: - *inputs*: 1. image: A 3D tensor of float32 and shape [None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. 3. images_padding_mask: If padded_mask set to true return a 2D tensor of int8 and shape [None, None, 3]. Mask of the image if a padding is performed we will know where the original image was. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape (). It is usefull to unpad the data in case of a batched training \"\"\" image = inputs [ 'image' ][ : , : , ::- 1 ] if bgr else inputs [ 'image' ] image = tf . cast ( image , tf . float32 ) targets = inputs [ 'objects' ] if horizontal_flip: image , targets [ BoxField . BOXES ] = aug . random_horizontal_flip ( image , targets [ BoxField . BOXES ]) if random_crop_size is not None : if tf . shape ( image )[ 0 ] < random_crop_size [ 0 ] or tf . shape ( image )[ 1 ] < random_crop_size [ 1 ] : image = resize_to_min_dim ( image , max ( random_crop_size ), 1333.0 ) image , targets = aug . random_random_crop ( image , random_crop_size , targets ) if 'is_crowd' in targets : targets = filter_crowded_boxes ( targets ) targets = filter_bad_area ( targets ) image = resize_to_min_dim ( image , 800.0 , 1333.0 ) image_information = tf . cast ( tf . shape ( image )[ : 2 ], dtype = tf . float32 ) inputs = { DatasetField . IMAGES : image , DatasetField . IMAGES_INFO : image_information } if padded_mask: inputs [ DatasetField . IMAGES_PMASK ] = tf . ones (( tf . shape ( image )[ 0 ], tf . shape ( image )[ 1 ]), dtype = tf . int8 ) ground_truths = { BoxField . BOXES : targets [ BoxField . BOXES ] * tf . tile ( image_information [ tf . newaxis ], [ 1 , 2 ]), BoxField . LABELS : tf . cast ( targets [ BoxField . LABELS ], tf . int32 ), BoxField . NUM_BOXES : tf . shape ( targets [ BoxField . LABELS ]), BoxField . WEIGHTS : tf . fill ( tf . shape ( targets [ BoxField . LABELS ]), 1.0 ) } return inputs , ground_truths def expand_dims_for_single_batch ( inputs , ground_truths ) : \"\"\"In order to train your model you need to add a batch dimension to the output of the preprocess function. For a single batch operation this method is faster: - `expand_dims`: ```python ds_train = tfds.load(name=\" voc \", split=\" train \", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` > Execution time: 0.002636657891998766 - `batch` ```python ds_train = tfds.load(name=\" voc \", split=\" train \", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.batch(1) ``` > Execution time: 0.004332915792008862 - `padded_batch` ```python ds_train = tfds.load(name=\" voc \", split=\" train \", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.padded_batch(batch_size, padded_shapes=padded_shapes) ``` > Execution time: 0.0055130551019974515 Returns: - *inputs*: The features and the ground_truths are mixed together 1. DatasetField.IMAGES: A 3D tensor of float32 and shape [None, None, 3] 2. DatasetField.IMAGES_INFO: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [1, num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [1, num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape [1, 1]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [1] \"\"\" inputs = { DatasetField . IMAGES : inputs [ DatasetField . IMAGES ][ None ], DatasetField . IMAGES_INFO : inputs [ DatasetField . IMAGES_INFO ][ None ] } ground_truths = { BoxField . BOXES : ground_truths [ BoxField . BOXES ][ None ], BoxField . LABELS : ground_truths [ BoxField . LABELS ][ None ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ][ None ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ][ None ] } return inputs , ground_truths Functions expand_dims_for_single_batch def expand_dims_for_single_batch ( inputs , ground_truths ) In order to train your model you need to add a batch dimension to the output of the preprocess function. For a single batch operation this method is faster: expand_dims : ds_train = tfds . load ( name = \"voc\" , split = \"train\" , shuffle_files = True ) ds_train = ds_train . map ( preprocess , num_parallel_calls = tf . data . experimental . AUTOTUNE ) ds_train = ds_train . map ( expand_dims_for_single_batch , num_parallel_calls = tf . data . experimental . AUTOTUNE ) Execution time: 0.002636657891998766 batch ds_train = tfds . load ( name = \"voc\" , split = \"train\" , shuffle_files = True ) ds_train = ds_train . map ( preprocess , num_parallel_calls = tf . data . experimental . AUTOTUNE ) ds_train = ds_train . batch ( 1 ) Execution time: 0.004332915792008862 padded_batch ds_train = tfds . load ( name = \"voc\" , split = \"train\" , shuffle_files = True ) ds_train = ds_train . map ( preprocess , num_parallel_calls = tf . data . experimental . AUTOTUNE ) ds_train = ds_train . padded_batch ( batch_size , padded_shapes = padded_shapes ) Execution time: 0.0055130551019974515 Returns: inputs : The features and the ground_truths are mixed together DatasetField.IMAGES: A 3D tensor of float32 and shape [None, None, 3] DatasetField.IMAGES_INFO: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a padded_batch operations. The models needs those information in order to clip the boxes to the proper dimension. ground_truths : BoxField.BOXES: A tensor of shape [1, num_boxes, (y1, x1, y2, x2)] and resized to the image shape BoxField.LABELS: A tensor of shape [1, num_boxes, ] BoxField.NUM_BOXES: A tensor of shape [1, 1]. It is usefull to unpad the data in case of a batched training BoxField.WEIGHTS: A tensor of shape [1] View Source def expand_dims_for_single_batch ( inputs , ground_truths ): \"\"\"In order to train your model you need to add a batch dimension to the output of the preprocess function. For a single batch operation this method is faster: - `expand_dims`: ```python ds_train = tfds.load(name=\"voc\", split=\"train\", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` > Execution time: 0.002636657891998766 - `batch` ```python ds_train = tfds.load(name=\"voc\", split=\"train\", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.batch(1) ``` > Execution time: 0.004332915792008862 - `padded_batch` ```python ds_train = tfds.load(name=\"voc\", split=\"train\", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.padded_batch(batch_size, padded_shapes=padded_shapes) ``` > Execution time: 0.0055130551019974515 Returns: - *inputs*: The features and the ground_truths are mixed together 1. DatasetField.IMAGES: A 3D tensor of float32 and shape [None, None, 3] 2. DatasetField.IMAGES_INFO: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [1, num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [1, num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape [1, 1]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [1] \"\"\" inputs = { DatasetField . IMAGES : inputs [ DatasetField . IMAGES ][ None ], DatasetField . IMAGES_INFO : inputs [ DatasetField . IMAGES_INFO ][ None ] } ground_truths = { BoxField . BOXES : ground_truths [ BoxField . BOXES ][ None ], BoxField . LABELS : ground_truths [ BoxField . LABELS ][ None ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ][ None ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ][ None ] } return inputs , ground_truths preprocess def preprocess ( inputs , bgr = True , horizontal_flip = True , random_crop_size = None , padded_mask = False ) This operations performs a classical preprocessing operations for localization datasets: COCO Pascal Voc You can download easily those dataset using tensorflow dataset . Arguments: inputs : It can be either a FeaturesDict or a dict. but it should have the following structures. inputs = FeaturesDict ({ 'image' : Image ( shape = ( None , None , 3 ), dtype = tf . uint8 ), 'objects' : Sequence ({ 'area' : Tensor ( shape = (), dtype = tf . int64 ), # area 'bbox' : BBoxFeature ( shape = ( 4 ,), dtype = tf . float32 ), # The values are between 0 and 1 'label' : ClassLabel ( shape = (), dtype = tf . int64 , num_classes = 80 ), }), }) bgr : Convert your input image to BGR (od.model.faster_rcnn.FasterRcnnFPNResnet50 needs it). If you have open your image with tf.image.decode_image will open an image in RGB. However, OpenCV will open it in BGR by default. horizontal_flip : Activate the random horizontal flip. random_crop_size : 1-D tensor with size the rank of image (e.g: (400, 600, 0)). padded_mask : If set to true return a mask of 1 of the image. When padded we will know which parts is from the original image. Returns: inputs : image: A 3D tensor of float32 and shape [None, None, 3] image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a padded_batch operations. The models needs those information in order to clip the boxes to the proper dimension. images_padding_mask: If padded_mask set to true return a 2D tensor of int8 and shape [None, None, 3]. Mask of the image if a padding is performed we will know where the original image was. ground_truths : BoxField.BOXES: A tensor of shape [num_boxes, (y1, x1, y2, x2)] and resized to the image shape BoxField.LABELS: A tensor of shape [num_boxes, ] BoxField.NUM_BOXES: A tensor of shape (). It is usefull to unpad the data in case of a batched training View Source def preprocess ( inputs , bgr = True , horizontal_flip = True , random_crop_size = None , padded_mask = False ) : \"\"\"This operations performs a classical preprocessing operations for localization datasets: - COCO - Pascal Voc You can download easily those dataset using [tensorflow dataset](https://www.tensorflow.org/datasets/catalog/overview). Arguments: - *inputs*: It can be either a [FeaturesDict](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/FeaturesDict) or a dict. but it should have the following structures. ```python inputs = FeaturesDict({ 'image': Image(shape=(None, None, 3), dtype=tf.uint8), 'objects': Sequence({ 'area': Tensor(shape=(), dtype=tf.int64), # area 'bbox': BBoxFeature(shape=(4,), dtype=tf.float32), # The values are between 0 and 1 'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=80), }), }) ``` - *bgr*: Convert your input image to BGR (od.model.faster_rcnn.FasterRcnnFPNResnet50 needs it). If you have open your image with `tf.image.decode_image` will open an image in RGB. However, OpenCV will open it in BGR by default. - *horizontal_flip*: Activate the random horizontal flip. - *random_crop_size*: 1-D tensor with size the rank of `image` (e.g: (400, 600, 0)). - *padded_mask*: If set to true return a mask of 1 of the image. When padded we will know which parts is from the original image. Returns: - *inputs*: 1. image: A 3D tensor of float32 and shape [None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. 3. images_padding_mask: If padded_mask set to true return a 2D tensor of int8 and shape [None, None, 3]. Mask of the image if a padding is performed we will know where the original image was. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape (). It is usefull to unpad the data in case of a batched training \"\"\" image = inputs [ 'image' ][ : , : , ::- 1 ] if bgr else inputs [ 'image' ] image = tf . cast ( image , tf . float32 ) targets = inputs [ 'objects' ] if horizontal_flip: image , targets [ BoxField . BOXES ] = aug . random_horizontal_flip ( image , targets [ BoxField . BOXES ]) if random_crop_size is not None : if tf . shape ( image )[ 0 ] < random_crop_size [ 0 ] or tf . shape ( image )[ 1 ] < random_crop_size [ 1 ] : image = resize_to_min_dim ( image , max ( random_crop_size ), 1333.0 ) image , targets = aug . random_random_crop ( image , random_crop_size , targets ) if 'is_crowd' in targets : targets = filter_crowded_boxes ( targets ) targets = filter_bad_area ( targets ) image = resize_to_min_dim ( image , 800.0 , 1333.0 ) image_information = tf . cast ( tf . shape ( image )[ : 2 ], dtype = tf . float32 ) inputs = { DatasetField . IMAGES : image , DatasetField . IMAGES_INFO : image_information } if padded_mask: inputs [ DatasetField . IMAGES_PMASK ] = tf . ones (( tf . shape ( image )[ 0 ], tf . shape ( image )[ 1 ]), dtype = tf . int8 ) ground_truths = { BoxField . BOXES : targets [ BoxField . BOXES ] * tf . tile ( image_information [ tf . newaxis ], [ 1 , 2 ]), BoxField . LABELS : tf . cast ( targets [ BoxField . LABELS ], tf . int32 ), BoxField . NUM_BOXES : tf . shape ( targets [ BoxField . LABELS ]), BoxField . WEIGHTS : tf . fill ( tf . shape ( targets [ BoxField . LABELS ]), 1.0 ) } return inputs , ground_truths resize_to_min_dim def resize_to_min_dim ( image , short_edge_length , max_dimension ) Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Arguments : image : A np.array of size [height, width, channels]. short_edge_length : minimum image dimension. max_dimension : If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - resized_image : The input image resized with the aspect_ratio preserved in float32 Raises: ValueError: If the max_dimension is above kerod.core.constants.MAX_IMAGE_SIZE View Source def resize_to_min_dim ( image , short_edge_length , max_dimension ): \"\"\"Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Arguments : - *image*: A np.array of size [height, width, channels]. - *short_edge_length*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *resized_image*: The input image resized with the aspect_ratio preserved in float32 Raises: ValueError: If the max_dimension is above `kerod.core.constants.MAX_IMAGE_SIZE` \"\"\" if max_dimension > constants . MAX_IMAGE_DIMENSION : raise ValueError ( f \"The max_dimension can only be inferior or equal to {constants.MAX_IMAGE_DIMENSION}\" ) shape = tf . shape ( image ) height = tf . cast ( shape [ 0 ], tf . float32 ) width = tf . cast ( shape [ 1 ], tf . float32 ) im_size_min = tf . minimum ( height , width ) im_size_max = tf . maximum ( height , width ) scale = short_edge_length / im_size_min # Prevent the biggest axis from being more than MAX_SIZE if tf . math . round ( scale * im_size_max ) > max_dimension : scale = max_dimension / im_size_max target_height = tf . cast ( height * scale , dtype = tf . int32 ) target_width = tf . cast ( width * scale , dtype = tf . int32 ) return tf . image . resize ( tf . expand_dims ( image , axis = 0 ), size = [ target_height , target_width ], method = tf . image . ResizeMethod . BILINEAR )[ 0 ]","title":"Preprocessing"},{"location":"reference/kerod/dataset/preprocessing/#module-keroddatasetpreprocessing","text":"None None View Source import tensorflow as tf from kerod . core import constants from kerod . core . standard_fields import BoxField , DatasetField from kerod . dataset . utils import filter_crowded_boxes , filter_bad_area from kerod . dataset import augmentation as aug def resize_to_min_dim ( image , short_edge_length , max_dimension ) : \"\"\"Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Arguments : - *image*: A np.array of size [height, width, channels]. - *short_edge_length*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *resized_image*: The input image resized with the aspect_ratio preserved in float32 Raises: ValueError: If the max_dimension is above `kerod.core.constants.MAX_IMAGE_SIZE` \"\"\" if max_dimension > constants . MAX_IMAGE_DIMENSION : raise ValueError ( f \"The max_dimension can only be inferior or equal to {constants.MAX_IMAGE_DIMENSION}\" ) shape = tf . shape ( image ) height = tf . cast ( shape [ 0 ], tf . float32 ) width = tf . cast ( shape [ 1 ], tf . float32 ) im_size_min = tf . minimum ( height , width ) im_size_max = tf . maximum ( height , width ) scale = short_edge_length / im_size_min # Prevent the biggest axis from being more than MAX_SIZE if tf . math . round ( scale * im_size_max ) > max_dimension: scale = max_dimension / im_size_max target_height = tf . cast ( height * scale , dtype = tf . int32 ) target_width = tf . cast ( width * scale , dtype = tf . int32 ) return tf . image . resize ( tf . expand_dims ( image , axis = 0 ), size = [ target_height , target_width ], method = tf . image . ResizeMethod . BILINEAR )[ 0 ] def preprocess ( inputs , bgr = True , horizontal_flip = True , random_crop_size = None , padded_mask = False ) : \"\"\"This operations performs a classical preprocessing operations for localization datasets: - COCO - Pascal Voc You can download easily those dataset using [tensorflow dataset](https://www.tensorflow.org/datasets/catalog/overview). Arguments: - *inputs*: It can be either a [FeaturesDict](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/FeaturesDict) or a dict. but it should have the following structures. ```python inputs = FeaturesDict({ 'image': Image(shape=(None, None, 3), dtype=tf.uint8), 'objects': Sequence({ 'area': Tensor(shape=(), dtype=tf.int64), # area 'bbox': BBoxFeature(shape=(4,), dtype=tf.float32), # The values are between 0 and 1 'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=80), }), }) ``` - *bgr*: Convert your input image to BGR (od.model.faster_rcnn.FasterRcnnFPNResnet50 needs it). If you have open your image with `tf.image.decode_image` will open an image in RGB. However, OpenCV will open it in BGR by default. - *horizontal_flip*: Activate the random horizontal flip. - *random_crop_size*: 1-D tensor with size the rank of `image` (e.g: (400, 600, 0)). - *padded_mask*: If set to true return a mask of 1 of the image. When padded we will know which parts is from the original image. Returns: - *inputs*: 1. image: A 3D tensor of float32 and shape [None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. 3. images_padding_mask: If padded_mask set to true return a 2D tensor of int8 and shape [None, None, 3]. Mask of the image if a padding is performed we will know where the original image was. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape (). It is usefull to unpad the data in case of a batched training \"\"\" image = inputs [ 'image' ][ : , : , ::- 1 ] if bgr else inputs [ 'image' ] image = tf . cast ( image , tf . float32 ) targets = inputs [ 'objects' ] if horizontal_flip: image , targets [ BoxField . BOXES ] = aug . random_horizontal_flip ( image , targets [ BoxField . BOXES ]) if random_crop_size is not None : if tf . shape ( image )[ 0 ] < random_crop_size [ 0 ] or tf . shape ( image )[ 1 ] < random_crop_size [ 1 ] : image = resize_to_min_dim ( image , max ( random_crop_size ), 1333.0 ) image , targets = aug . random_random_crop ( image , random_crop_size , targets ) if 'is_crowd' in targets : targets = filter_crowded_boxes ( targets ) targets = filter_bad_area ( targets ) image = resize_to_min_dim ( image , 800.0 , 1333.0 ) image_information = tf . cast ( tf . shape ( image )[ : 2 ], dtype = tf . float32 ) inputs = { DatasetField . IMAGES : image , DatasetField . IMAGES_INFO : image_information } if padded_mask: inputs [ DatasetField . IMAGES_PMASK ] = tf . ones (( tf . shape ( image )[ 0 ], tf . shape ( image )[ 1 ]), dtype = tf . int8 ) ground_truths = { BoxField . BOXES : targets [ BoxField . BOXES ] * tf . tile ( image_information [ tf . newaxis ], [ 1 , 2 ]), BoxField . LABELS : tf . cast ( targets [ BoxField . LABELS ], tf . int32 ), BoxField . NUM_BOXES : tf . shape ( targets [ BoxField . LABELS ]), BoxField . WEIGHTS : tf . fill ( tf . shape ( targets [ BoxField . LABELS ]), 1.0 ) } return inputs , ground_truths def expand_dims_for_single_batch ( inputs , ground_truths ) : \"\"\"In order to train your model you need to add a batch dimension to the output of the preprocess function. For a single batch operation this method is faster: - `expand_dims`: ```python ds_train = tfds.load(name=\" voc \", split=\" train \", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` > Execution time: 0.002636657891998766 - `batch` ```python ds_train = tfds.load(name=\" voc \", split=\" train \", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.batch(1) ``` > Execution time: 0.004332915792008862 - `padded_batch` ```python ds_train = tfds.load(name=\" voc \", split=\" train \", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.padded_batch(batch_size, padded_shapes=padded_shapes) ``` > Execution time: 0.0055130551019974515 Returns: - *inputs*: The features and the ground_truths are mixed together 1. DatasetField.IMAGES: A 3D tensor of float32 and shape [None, None, 3] 2. DatasetField.IMAGES_INFO: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [1, num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [1, num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape [1, 1]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [1] \"\"\" inputs = { DatasetField . IMAGES : inputs [ DatasetField . IMAGES ][ None ], DatasetField . IMAGES_INFO : inputs [ DatasetField . IMAGES_INFO ][ None ] } ground_truths = { BoxField . BOXES : ground_truths [ BoxField . BOXES ][ None ], BoxField . LABELS : ground_truths [ BoxField . LABELS ][ None ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ][ None ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ][ None ] } return inputs , ground_truths","title":"Module kerod.dataset.preprocessing"},{"location":"reference/kerod/dataset/preprocessing/#functions","text":"","title":"Functions"},{"location":"reference/kerod/dataset/preprocessing/#expand_dims_for_single_batch","text":"def expand_dims_for_single_batch ( inputs , ground_truths ) In order to train your model you need to add a batch dimension to the output of the preprocess function. For a single batch operation this method is faster: expand_dims : ds_train = tfds . load ( name = \"voc\" , split = \"train\" , shuffle_files = True ) ds_train = ds_train . map ( preprocess , num_parallel_calls = tf . data . experimental . AUTOTUNE ) ds_train = ds_train . map ( expand_dims_for_single_batch , num_parallel_calls = tf . data . experimental . AUTOTUNE ) Execution time: 0.002636657891998766 batch ds_train = tfds . load ( name = \"voc\" , split = \"train\" , shuffle_files = True ) ds_train = ds_train . map ( preprocess , num_parallel_calls = tf . data . experimental . AUTOTUNE ) ds_train = ds_train . batch ( 1 ) Execution time: 0.004332915792008862 padded_batch ds_train = tfds . load ( name = \"voc\" , split = \"train\" , shuffle_files = True ) ds_train = ds_train . map ( preprocess , num_parallel_calls = tf . data . experimental . AUTOTUNE ) ds_train = ds_train . padded_batch ( batch_size , padded_shapes = padded_shapes ) Execution time: 0.0055130551019974515 Returns: inputs : The features and the ground_truths are mixed together DatasetField.IMAGES: A 3D tensor of float32 and shape [None, None, 3] DatasetField.IMAGES_INFO: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a padded_batch operations. The models needs those information in order to clip the boxes to the proper dimension. ground_truths : BoxField.BOXES: A tensor of shape [1, num_boxes, (y1, x1, y2, x2)] and resized to the image shape BoxField.LABELS: A tensor of shape [1, num_boxes, ] BoxField.NUM_BOXES: A tensor of shape [1, 1]. It is usefull to unpad the data in case of a batched training BoxField.WEIGHTS: A tensor of shape [1] View Source def expand_dims_for_single_batch ( inputs , ground_truths ): \"\"\"In order to train your model you need to add a batch dimension to the output of the preprocess function. For a single batch operation this method is faster: - `expand_dims`: ```python ds_train = tfds.load(name=\"voc\", split=\"train\", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` > Execution time: 0.002636657891998766 - `batch` ```python ds_train = tfds.load(name=\"voc\", split=\"train\", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.batch(1) ``` > Execution time: 0.004332915792008862 - `padded_batch` ```python ds_train = tfds.load(name=\"voc\", split=\"train\", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.padded_batch(batch_size, padded_shapes=padded_shapes) ``` > Execution time: 0.0055130551019974515 Returns: - *inputs*: The features and the ground_truths are mixed together 1. DatasetField.IMAGES: A 3D tensor of float32 and shape [None, None, 3] 2. DatasetField.IMAGES_INFO: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [1, num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [1, num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape [1, 1]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [1] \"\"\" inputs = { DatasetField . IMAGES : inputs [ DatasetField . IMAGES ][ None ], DatasetField . IMAGES_INFO : inputs [ DatasetField . IMAGES_INFO ][ None ] } ground_truths = { BoxField . BOXES : ground_truths [ BoxField . BOXES ][ None ], BoxField . LABELS : ground_truths [ BoxField . LABELS ][ None ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ][ None ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ][ None ] } return inputs , ground_truths","title":"expand_dims_for_single_batch"},{"location":"reference/kerod/dataset/preprocessing/#preprocess","text":"def preprocess ( inputs , bgr = True , horizontal_flip = True , random_crop_size = None , padded_mask = False ) This operations performs a classical preprocessing operations for localization datasets: COCO Pascal Voc You can download easily those dataset using tensorflow dataset . Arguments: inputs : It can be either a FeaturesDict or a dict. but it should have the following structures. inputs = FeaturesDict ({ 'image' : Image ( shape = ( None , None , 3 ), dtype = tf . uint8 ), 'objects' : Sequence ({ 'area' : Tensor ( shape = (), dtype = tf . int64 ), # area 'bbox' : BBoxFeature ( shape = ( 4 ,), dtype = tf . float32 ), # The values are between 0 and 1 'label' : ClassLabel ( shape = (), dtype = tf . int64 , num_classes = 80 ), }), }) bgr : Convert your input image to BGR (od.model.faster_rcnn.FasterRcnnFPNResnet50 needs it). If you have open your image with tf.image.decode_image will open an image in RGB. However, OpenCV will open it in BGR by default. horizontal_flip : Activate the random horizontal flip. random_crop_size : 1-D tensor with size the rank of image (e.g: (400, 600, 0)). padded_mask : If set to true return a mask of 1 of the image. When padded we will know which parts is from the original image. Returns: inputs : image: A 3D tensor of float32 and shape [None, None, 3] image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a padded_batch operations. The models needs those information in order to clip the boxes to the proper dimension. images_padding_mask: If padded_mask set to true return a 2D tensor of int8 and shape [None, None, 3]. Mask of the image if a padding is performed we will know where the original image was. ground_truths : BoxField.BOXES: A tensor of shape [num_boxes, (y1, x1, y2, x2)] and resized to the image shape BoxField.LABELS: A tensor of shape [num_boxes, ] BoxField.NUM_BOXES: A tensor of shape (). It is usefull to unpad the data in case of a batched training View Source def preprocess ( inputs , bgr = True , horizontal_flip = True , random_crop_size = None , padded_mask = False ) : \"\"\"This operations performs a classical preprocessing operations for localization datasets: - COCO - Pascal Voc You can download easily those dataset using [tensorflow dataset](https://www.tensorflow.org/datasets/catalog/overview). Arguments: - *inputs*: It can be either a [FeaturesDict](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/FeaturesDict) or a dict. but it should have the following structures. ```python inputs = FeaturesDict({ 'image': Image(shape=(None, None, 3), dtype=tf.uint8), 'objects': Sequence({ 'area': Tensor(shape=(), dtype=tf.int64), # area 'bbox': BBoxFeature(shape=(4,), dtype=tf.float32), # The values are between 0 and 1 'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=80), }), }) ``` - *bgr*: Convert your input image to BGR (od.model.faster_rcnn.FasterRcnnFPNResnet50 needs it). If you have open your image with `tf.image.decode_image` will open an image in RGB. However, OpenCV will open it in BGR by default. - *horizontal_flip*: Activate the random horizontal flip. - *random_crop_size*: 1-D tensor with size the rank of `image` (e.g: (400, 600, 0)). - *padded_mask*: If set to true return a mask of 1 of the image. When padded we will know which parts is from the original image. Returns: - *inputs*: 1. image: A 3D tensor of float32 and shape [None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. 3. images_padding_mask: If padded_mask set to true return a 2D tensor of int8 and shape [None, None, 3]. Mask of the image if a padding is performed we will know where the original image was. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape (). It is usefull to unpad the data in case of a batched training \"\"\" image = inputs [ 'image' ][ : , : , ::- 1 ] if bgr else inputs [ 'image' ] image = tf . cast ( image , tf . float32 ) targets = inputs [ 'objects' ] if horizontal_flip: image , targets [ BoxField . BOXES ] = aug . random_horizontal_flip ( image , targets [ BoxField . BOXES ]) if random_crop_size is not None : if tf . shape ( image )[ 0 ] < random_crop_size [ 0 ] or tf . shape ( image )[ 1 ] < random_crop_size [ 1 ] : image = resize_to_min_dim ( image , max ( random_crop_size ), 1333.0 ) image , targets = aug . random_random_crop ( image , random_crop_size , targets ) if 'is_crowd' in targets : targets = filter_crowded_boxes ( targets ) targets = filter_bad_area ( targets ) image = resize_to_min_dim ( image , 800.0 , 1333.0 ) image_information = tf . cast ( tf . shape ( image )[ : 2 ], dtype = tf . float32 ) inputs = { DatasetField . IMAGES : image , DatasetField . IMAGES_INFO : image_information } if padded_mask: inputs [ DatasetField . IMAGES_PMASK ] = tf . ones (( tf . shape ( image )[ 0 ], tf . shape ( image )[ 1 ]), dtype = tf . int8 ) ground_truths = { BoxField . BOXES : targets [ BoxField . BOXES ] * tf . tile ( image_information [ tf . newaxis ], [ 1 , 2 ]), BoxField . LABELS : tf . cast ( targets [ BoxField . LABELS ], tf . int32 ), BoxField . NUM_BOXES : tf . shape ( targets [ BoxField . LABELS ]), BoxField . WEIGHTS : tf . fill ( tf . shape ( targets [ BoxField . LABELS ]), 1.0 ) } return inputs , ground_truths","title":"preprocess"},{"location":"reference/kerod/dataset/preprocessing/#resize_to_min_dim","text":"def resize_to_min_dim ( image , short_edge_length , max_dimension ) Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Arguments : image : A np.array of size [height, width, channels]. short_edge_length : minimum image dimension. max_dimension : If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - resized_image : The input image resized with the aspect_ratio preserved in float32 Raises: ValueError: If the max_dimension is above kerod.core.constants.MAX_IMAGE_SIZE View Source def resize_to_min_dim ( image , short_edge_length , max_dimension ): \"\"\"Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Arguments : - *image*: A np.array of size [height, width, channels]. - *short_edge_length*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *resized_image*: The input image resized with the aspect_ratio preserved in float32 Raises: ValueError: If the max_dimension is above `kerod.core.constants.MAX_IMAGE_SIZE` \"\"\" if max_dimension > constants . MAX_IMAGE_DIMENSION : raise ValueError ( f \"The max_dimension can only be inferior or equal to {constants.MAX_IMAGE_DIMENSION}\" ) shape = tf . shape ( image ) height = tf . cast ( shape [ 0 ], tf . float32 ) width = tf . cast ( shape [ 1 ], tf . float32 ) im_size_min = tf . minimum ( height , width ) im_size_max = tf . maximum ( height , width ) scale = short_edge_length / im_size_min # Prevent the biggest axis from being more than MAX_SIZE if tf . math . round ( scale * im_size_max ) > max_dimension : scale = max_dimension / im_size_max target_height = tf . cast ( height * scale , dtype = tf . int32 ) target_width = tf . cast ( width * scale , dtype = tf . int32 ) return tf . image . resize ( tf . expand_dims ( image , axis = 0 ), size = [ target_height , target_width ], method = tf . image . ResizeMethod . BILINEAR )[ 0 ]","title":"resize_to_min_dim"},{"location":"reference/kerod/dataset/utils/","text":"Module kerod.dataset.utils None None View Source from typing import Dict import tensorflow as tf from kerod.core.box_ops import compute_area from kerod.core.standard_fields import BoxField def _filter ( _dict , _filter ): keys = { BoxField . BOXES , BoxField . LABELS , BoxField . MASKS , 'is_crowd' } filtered_dict = {} for key in _dict . keys (): if key in keys : filtered_dict [ key ] = tf . gather_nd ( _dict [ key ], _filter ) else : filtered_dict [ key ] = _dict [ key ] return filtered_dict def filter_crowded_boxes ( groundtruths : Dict [ str , tf . Tensor ]) -> Dict [ str , tf . Tensor ]: \"\"\"Coco has boxes flagged as crowded which are not used during the training. This function will discard them. Arguments: - *groundtruths*: A dict with the following keys 1. boxes: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. labels: A tensor of shape [num_boxes, ] 3. crowd: Boolean tensor which indicates if the boxes is crowded are not. Crowded means that the boxes contains multiple entities which are to difficult to localize one by one. `True` is for crowded box. Returns: - *groundtruths*: Filtered groundtruths \"\"\" ind_uncrowded_boxes = tf . where ( tf . equal ( groundtruths [ 'is_crowd' ], False )) return _filter ( groundtruths , ind_uncrowded_boxes ) def filter_bad_area ( groundtruths : Dict [ str , tf . Tensor ]) -> Dict [ str , tf . Tensor ]: \"\"\"Remove all the boxes that have an area less or equal to 0. Arguments: - *groundtruths*: A dict with the following keys: 1. bbox: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. label: A tensor of shape [num_boxes, ] Returns: - *groundtruths*: All the groundtruths which match have not been filtered. \"\"\" area = compute_area ( groundtruths [ BoxField . BOXES ]) filter_area = tf . where ( area > 0 ) return _filter ( groundtruths , filter_area ) Functions filter_bad_area def filter_bad_area ( groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> Dict [ str , tensorflow . python . framework . ops . Tensor ] Remove all the boxes that have an area less or equal to 0. Arguments: groundtruths : A dict with the following keys: bbox: A tensor of shape [num_boxes, (y1, x1, y2, x2)] label: A tensor of shape [num_boxes, ] Returns: groundtruths : All the groundtruths which match have not been filtered. View Source def filter_bad_area ( groundtruths : Dict [ str , tf . Tensor ]) -> Dict [ str , tf . Tensor ] : \"\"\"Remove all the boxes that have an area less or equal to 0. Arguments: - *groundtruths*: A dict with the following keys: 1. bbox: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. label: A tensor of shape [num_boxes, ] Returns: - *groundtruths*: All the groundtruths which match have not been filtered. \"\"\" area = compute_area ( groundtruths [ BoxField . BOXES ]) filter_area = tf . where ( area > 0 ) return _filter ( groundtruths , filter_area ) filter_crowded_boxes def filter_crowded_boxes ( groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> Dict [ str , tensorflow . python . framework . ops . Tensor ] Coco has boxes flagged as crowded which are not used during the training. This function will discard them. Arguments: groundtruths : A dict with the following keys boxes: A tensor of shape [num_boxes, (y1, x1, y2, x2)] labels: A tensor of shape [num_boxes, ] crowd: Boolean tensor which indicates if the boxes is crowded are not. Crowded means that the boxes contains multiple entities which are to difficult to localize one by one. True is for crowded box. Returns: groundtruths : Filtered groundtruths View Source def filter_crowded_boxes ( groundtruths : Dict [ str , tf . Tensor ] ) -> Dict [ str , tf . Tensor ] : \" \"\" Coco has boxes flagged as crowded which are not used during the training. This function will discard them. Arguments: - *groundtruths*: A dict with the following keys 1. boxes: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. labels: A tensor of shape [num_boxes, ] 3. crowd: Boolean tensor which indicates if the boxes is crowded are not. Crowded means that the boxes contains multiple entities which are to difficult to localize one by one. `True` is for crowded box. Returns: - *groundtruths*: Filtered groundtruths \"\" \" ind_uncrowded_boxes = tf . where ( tf . equal ( groundtruths [ 'is_crowd' ] , False )) return _filter ( groundtruths , ind_uncrowded_boxes )","title":"Utils"},{"location":"reference/kerod/dataset/utils/#module-keroddatasetutils","text":"None None View Source from typing import Dict import tensorflow as tf from kerod.core.box_ops import compute_area from kerod.core.standard_fields import BoxField def _filter ( _dict , _filter ): keys = { BoxField . BOXES , BoxField . LABELS , BoxField . MASKS , 'is_crowd' } filtered_dict = {} for key in _dict . keys (): if key in keys : filtered_dict [ key ] = tf . gather_nd ( _dict [ key ], _filter ) else : filtered_dict [ key ] = _dict [ key ] return filtered_dict def filter_crowded_boxes ( groundtruths : Dict [ str , tf . Tensor ]) -> Dict [ str , tf . Tensor ]: \"\"\"Coco has boxes flagged as crowded which are not used during the training. This function will discard them. Arguments: - *groundtruths*: A dict with the following keys 1. boxes: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. labels: A tensor of shape [num_boxes, ] 3. crowd: Boolean tensor which indicates if the boxes is crowded are not. Crowded means that the boxes contains multiple entities which are to difficult to localize one by one. `True` is for crowded box. Returns: - *groundtruths*: Filtered groundtruths \"\"\" ind_uncrowded_boxes = tf . where ( tf . equal ( groundtruths [ 'is_crowd' ], False )) return _filter ( groundtruths , ind_uncrowded_boxes ) def filter_bad_area ( groundtruths : Dict [ str , tf . Tensor ]) -> Dict [ str , tf . Tensor ]: \"\"\"Remove all the boxes that have an area less or equal to 0. Arguments: - *groundtruths*: A dict with the following keys: 1. bbox: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. label: A tensor of shape [num_boxes, ] Returns: - *groundtruths*: All the groundtruths which match have not been filtered. \"\"\" area = compute_area ( groundtruths [ BoxField . BOXES ]) filter_area = tf . where ( area > 0 ) return _filter ( groundtruths , filter_area )","title":"Module kerod.dataset.utils"},{"location":"reference/kerod/dataset/utils/#functions","text":"","title":"Functions"},{"location":"reference/kerod/dataset/utils/#filter_bad_area","text":"def filter_bad_area ( groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> Dict [ str , tensorflow . python . framework . ops . Tensor ] Remove all the boxes that have an area less or equal to 0. Arguments: groundtruths : A dict with the following keys: bbox: A tensor of shape [num_boxes, (y1, x1, y2, x2)] label: A tensor of shape [num_boxes, ] Returns: groundtruths : All the groundtruths which match have not been filtered. View Source def filter_bad_area ( groundtruths : Dict [ str , tf . Tensor ]) -> Dict [ str , tf . Tensor ] : \"\"\"Remove all the boxes that have an area less or equal to 0. Arguments: - *groundtruths*: A dict with the following keys: 1. bbox: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. label: A tensor of shape [num_boxes, ] Returns: - *groundtruths*: All the groundtruths which match have not been filtered. \"\"\" area = compute_area ( groundtruths [ BoxField . BOXES ]) filter_area = tf . where ( area > 0 ) return _filter ( groundtruths , filter_area )","title":"filter_bad_area"},{"location":"reference/kerod/dataset/utils/#filter_crowded_boxes","text":"def filter_crowded_boxes ( groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> Dict [ str , tensorflow . python . framework . ops . Tensor ] Coco has boxes flagged as crowded which are not used during the training. This function will discard them. Arguments: groundtruths : A dict with the following keys boxes: A tensor of shape [num_boxes, (y1, x1, y2, x2)] labels: A tensor of shape [num_boxes, ] crowd: Boolean tensor which indicates if the boxes is crowded are not. Crowded means that the boxes contains multiple entities which are to difficult to localize one by one. True is for crowded box. Returns: groundtruths : Filtered groundtruths View Source def filter_crowded_boxes ( groundtruths : Dict [ str , tf . Tensor ] ) -> Dict [ str , tf . Tensor ] : \" \"\" Coco has boxes flagged as crowded which are not used during the training. This function will discard them. Arguments: - *groundtruths*: A dict with the following keys 1. boxes: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. labels: A tensor of shape [num_boxes, ] 3. crowd: Boolean tensor which indicates if the boxes is crowded are not. Crowded means that the boxes contains multiple entities which are to difficult to localize one by one. `True` is for crowded box. Returns: - *groundtruths*: Filtered groundtruths \"\" \" ind_uncrowded_boxes = tf . where ( tf . equal ( groundtruths [ 'is_crowd' ] , False )) return _filter ( groundtruths , ind_uncrowded_boxes )","title":"filter_crowded_boxes"},{"location":"reference/kerod/layers/","text":"Module kerod.layers None None View Source from kerod.layers.anchors import Anchors from kerod.layers.attentions import MultiHeadAttention from kerod.layers.positional_encoding import ( PositionEmbeddingLearned , PositionEmbeddingSine ) from kerod.layers.smca.reference_points import SMCAReferencePoints from kerod.layers.smca.weight_map import DynamicalWeightMaps from kerod.layers.transformer import ( DecoderLayer , EncoderLayer , Transformer ) from kerod.layers.detection.fast_rcnn import FastRCNN from kerod.layers.detection.rpn import RegionProposalNetwork Sub-modules kerod.layers.anchors kerod.layers.attentions kerod.layers.detection kerod.layers.patches kerod.layers.positional_encoding kerod.layers.post_processing kerod.layers.smca kerod.layers.transformer","title":"Index"},{"location":"reference/kerod/layers/#module-kerodlayers","text":"None None View Source from kerod.layers.anchors import Anchors from kerod.layers.attentions import MultiHeadAttention from kerod.layers.positional_encoding import ( PositionEmbeddingLearned , PositionEmbeddingSine ) from kerod.layers.smca.reference_points import SMCAReferencePoints from kerod.layers.smca.weight_map import DynamicalWeightMaps from kerod.layers.transformer import ( DecoderLayer , EncoderLayer , Transformer ) from kerod.layers.detection.fast_rcnn import FastRCNN from kerod.layers.detection.rpn import RegionProposalNetwork","title":"Module kerod.layers"},{"location":"reference/kerod/layers/#sub-modules","text":"kerod.layers.anchors kerod.layers.attentions kerod.layers.detection kerod.layers.patches kerod.layers.positional_encoding kerod.layers.post_processing kerod.layers.smca kerod.layers.transformer","title":"Sub-modules"},{"location":"reference/kerod/layers/anchors/","text":"Module kerod.layers.anchors None None View Source import tensorflow as tf from kerod.core.constants import MAX_IMAGE_DIMENSION from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} def generate_anchors ( stride : int , scales : tf . Tensor , ratios : tf . Tensor , max_size : int ): \"\"\"Will generate a determistic grid based on the input images dimension. The anchors will be generated once and for all on the biggest possible image. At each forward according to the shape of the tensor we can extract the corresponding anchors. Arguments: stride: Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales: The scale of the anchors e.g: 8, 16, 32 ratios: The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size: Maximum size of the input image. The anchors will computed once and for all. Returns: A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. \"\"\" ratios , scales = tf . meshgrid ( ratios , scales ) scales = tf . reshape ( scales , [ - 1 ]) ratios = tf . reshape ( ratios , [ - 1 ]) ratio_sqrts = tf . sqrt ( ratios ) widths = scales / ratio_sqrts heights = ratios * widths base_anchors = tf . stack ([ - widths , - heights , widths , heights ], axis =- 1 ) * 0.5 max_stride = tf . math . ceil ( max_size / stride ) y_centers = tf . cast ( tf . range ( max_stride ), dtype = scales . dtype ) * stride x_centers = tf . cast ( tf . range ( max_stride ), dtype = scales . dtype ) * stride x_centers , y_centers = tf . meshgrid ( x_centers , y_centers ) shifts = tf . stack ([ x_centers , y_centers , x_centers , y_centers ], axis =- 1 ) anchors = tf . expand_dims ( base_anchors , 0 ) + tf . expand_dims ( shifts , 2 ) return tf . gather ( anchors , [ 1 , 0 , 3 , 2 ], axis =- 1 ) class Anchors ( tf . keras . layers . Layer ): \"\"\"Will generate a determistic grid and store it in memory to avoid recompute it at each run. At each forward according to the shape of the tensor we can extract the corresponding anchors. Arguments: stride: Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales: The scale of the anchors e.g: 8, 16, 32 ratios: The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size: Maximum size of the input image. The anchors will computed once and for all. Call arguments: inputs: A tensor of shape [batch_size, height, widht, channel] Call returns: tf.Tensor: A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. \"\"\" def __init__ ( self , stride , scales , ratios , ** kwargs ): super () . __init__ ( ** kwargs ) self . _stride = stride self . _scales = scales self . _ratios = ratios self . _anchors = generate_anchors ( stride , tf . constant ([ scales ], self . _compute_dtype ), tf . constant ( ratios , self . _compute_dtype ), max_size = MAX_IMAGE_DIMENSION ) def call ( self , inputs ): \"\"\"Return anchors based on the shape of the input tensors Arguments: inputs: A tensor of shape [batch_size, height, widht, channel] Returns: tf.Tensor: A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. \"\"\" shape = tf . shape ( inputs ) height , width = shape [ 1 ], shape [ 2 ] anchors = self . _anchors [: height , : width ] return tf . reshape ( anchors , ( - 1 , 4 )) def get_config ( self ): config = super () . get_config () config [ 'stride' ] = self . _stride config [ 'scales' ] = self . _scales config [ 'ratios' ] = self . _ratios return config remove_unwanted_doc ( Anchors , __pdoc__ ) Variables MAX_IMAGE_DIMENSION Functions generate_anchors def generate_anchors ( stride : int , scales : tensorflow . python . framework . ops . Tensor , ratios : tensorflow . python . framework . ops . Tensor , max_size : int ) Will generate a determistic grid based on the input images dimension. The anchors will be generated once and for all on the biggest possible image. At each forward according to the shape of the tensor we can extract the corresponding anchors. Parameters: Name Description stride Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales The scale of the anchors e.g: 8, 16, 32 ratios The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size Maximum size of the input image. The anchors will computed once and for all. Returns: Type Description None A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. View Source def generate_anchors ( stride : int , scales : tf . Tensor , ratios : tf . Tensor , max_size : int ) : \"\"\" Will generate a determistic grid based on the input images dimension. The anchors will be generated once and for all on the biggest possible image . At each forward according to the shape of the tensor we can extract the corresponding anchors . Arguments : stride : Downscaling ratio compared to the original image . At stride 16 your original image will be 16 times bigger than your actual tensor scales : The scale of the anchors e . g : 8 , 16 , 32 ratios : The ratios are the different shapes that you want to apply on your anchors . e . g : ( 0 . 5 , 1 , 2 ) max_size : Maximum size of the input image . The anchors will computed once and for all . Returns : A tensor of shape [ num_scales * num_ratios * height * width , 4 ]. The anchors have the format [ y_min , x_min , y_max , x_max ]. \"\"\" ratios , scales = tf . meshgrid ( ratios , scales ) scales = tf . reshape ( scales , [ - 1 ] ) ratios = tf . reshape ( ratios , [ - 1 ] ) ratio_sqrts = tf . sqrt ( ratios ) widths = scales / ratio_sqrts heights = ratios * widths base_anchors = tf . stack ( [ - widths , - heights , widths , heights ], axis =- 1 ) * 0 . 5 max_stride = tf . math . ceil ( max_size / stride ) y_centers = tf . cast ( tf . range ( max_stride ) , dtype = scales . dtype ) * stride x_centers = tf . cast ( tf . range ( max_stride ) , dtype = scales . dtype ) * stride x_centers , y_centers = tf . meshgrid ( x_centers , y_centers ) shifts = tf . stack ( [ x_centers , y_centers , x_centers , y_centers ], axis =- 1 ) anchors = tf . expand_dims ( base_anchors , 0 ) + tf . expand_dims ( shifts , 2 ) return tf . gather ( anchors , [ 1 , 0 , 3 , 2 ], axis =- 1 ) Classes Anchors class Anchors ( stride , scales , ratios , ** kwargs ) At each forward according to the shape of the tensor we can extract the corresponding anchors. Arguments Name Description stride Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales The scale of the anchors e.g: 8, 16, 32 ratios The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size Maximum size of the input image. The anchors will computed once and for all. Call arguments Name Description inputs A tensor of shape [batch_size, height, widht, channel] Call returns Type Description tf.Tensor A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , inputs ) Return anchors based on the shape of the input tensors Parameters: Name Description inputs A tensor of shape [batch_size, height, widht, channel] Returns: Type Description tf.Tensor A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. View Source def call ( self , inputs ) : \"\"\" Return anchors based on the shape of the input tensors Arguments : inputs : A tensor of shape [ batch_size , height , widht , channel ] Returns : tf . Tensor : A tensor of shape [ num_scales * num_ratios * height * width , 4 ]. The anchors have the format [ y_min , x_min , y_max , x_max ]. \"\"\" shape = tf . shape ( inputs ) height , width = shape [ 1 ], shape [ 2 ] anchors = self . _anchors [: height , : width ] return tf . reshape ( anchors , ( - 1 , 4 ))","title":"Anchors"},{"location":"reference/kerod/layers/anchors/#module-kerodlayersanchors","text":"None None View Source import tensorflow as tf from kerod.core.constants import MAX_IMAGE_DIMENSION from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} def generate_anchors ( stride : int , scales : tf . Tensor , ratios : tf . Tensor , max_size : int ): \"\"\"Will generate a determistic grid based on the input images dimension. The anchors will be generated once and for all on the biggest possible image. At each forward according to the shape of the tensor we can extract the corresponding anchors. Arguments: stride: Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales: The scale of the anchors e.g: 8, 16, 32 ratios: The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size: Maximum size of the input image. The anchors will computed once and for all. Returns: A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. \"\"\" ratios , scales = tf . meshgrid ( ratios , scales ) scales = tf . reshape ( scales , [ - 1 ]) ratios = tf . reshape ( ratios , [ - 1 ]) ratio_sqrts = tf . sqrt ( ratios ) widths = scales / ratio_sqrts heights = ratios * widths base_anchors = tf . stack ([ - widths , - heights , widths , heights ], axis =- 1 ) * 0.5 max_stride = tf . math . ceil ( max_size / stride ) y_centers = tf . cast ( tf . range ( max_stride ), dtype = scales . dtype ) * stride x_centers = tf . cast ( tf . range ( max_stride ), dtype = scales . dtype ) * stride x_centers , y_centers = tf . meshgrid ( x_centers , y_centers ) shifts = tf . stack ([ x_centers , y_centers , x_centers , y_centers ], axis =- 1 ) anchors = tf . expand_dims ( base_anchors , 0 ) + tf . expand_dims ( shifts , 2 ) return tf . gather ( anchors , [ 1 , 0 , 3 , 2 ], axis =- 1 ) class Anchors ( tf . keras . layers . Layer ): \"\"\"Will generate a determistic grid and store it in memory to avoid recompute it at each run. At each forward according to the shape of the tensor we can extract the corresponding anchors. Arguments: stride: Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales: The scale of the anchors e.g: 8, 16, 32 ratios: The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size: Maximum size of the input image. The anchors will computed once and for all. Call arguments: inputs: A tensor of shape [batch_size, height, widht, channel] Call returns: tf.Tensor: A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. \"\"\" def __init__ ( self , stride , scales , ratios , ** kwargs ): super () . __init__ ( ** kwargs ) self . _stride = stride self . _scales = scales self . _ratios = ratios self . _anchors = generate_anchors ( stride , tf . constant ([ scales ], self . _compute_dtype ), tf . constant ( ratios , self . _compute_dtype ), max_size = MAX_IMAGE_DIMENSION ) def call ( self , inputs ): \"\"\"Return anchors based on the shape of the input tensors Arguments: inputs: A tensor of shape [batch_size, height, widht, channel] Returns: tf.Tensor: A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. \"\"\" shape = tf . shape ( inputs ) height , width = shape [ 1 ], shape [ 2 ] anchors = self . _anchors [: height , : width ] return tf . reshape ( anchors , ( - 1 , 4 )) def get_config ( self ): config = super () . get_config () config [ 'stride' ] = self . _stride config [ 'scales' ] = self . _scales config [ 'ratios' ] = self . _ratios return config remove_unwanted_doc ( Anchors , __pdoc__ )","title":"Module kerod.layers.anchors"},{"location":"reference/kerod/layers/anchors/#variables","text":"MAX_IMAGE_DIMENSION","title":"Variables"},{"location":"reference/kerod/layers/anchors/#functions","text":"","title":"Functions"},{"location":"reference/kerod/layers/anchors/#generate_anchors","text":"def generate_anchors ( stride : int , scales : tensorflow . python . framework . ops . Tensor , ratios : tensorflow . python . framework . ops . Tensor , max_size : int ) Will generate a determistic grid based on the input images dimension. The anchors will be generated once and for all on the biggest possible image. At each forward according to the shape of the tensor we can extract the corresponding anchors. Parameters: Name Description stride Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales The scale of the anchors e.g: 8, 16, 32 ratios The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size Maximum size of the input image. The anchors will computed once and for all. Returns: Type Description None A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. View Source def generate_anchors ( stride : int , scales : tf . Tensor , ratios : tf . Tensor , max_size : int ) : \"\"\" Will generate a determistic grid based on the input images dimension. The anchors will be generated once and for all on the biggest possible image . At each forward according to the shape of the tensor we can extract the corresponding anchors . Arguments : stride : Downscaling ratio compared to the original image . At stride 16 your original image will be 16 times bigger than your actual tensor scales : The scale of the anchors e . g : 8 , 16 , 32 ratios : The ratios are the different shapes that you want to apply on your anchors . e . g : ( 0 . 5 , 1 , 2 ) max_size : Maximum size of the input image . The anchors will computed once and for all . Returns : A tensor of shape [ num_scales * num_ratios * height * width , 4 ]. The anchors have the format [ y_min , x_min , y_max , x_max ]. \"\"\" ratios , scales = tf . meshgrid ( ratios , scales ) scales = tf . reshape ( scales , [ - 1 ] ) ratios = tf . reshape ( ratios , [ - 1 ] ) ratio_sqrts = tf . sqrt ( ratios ) widths = scales / ratio_sqrts heights = ratios * widths base_anchors = tf . stack ( [ - widths , - heights , widths , heights ], axis =- 1 ) * 0 . 5 max_stride = tf . math . ceil ( max_size / stride ) y_centers = tf . cast ( tf . range ( max_stride ) , dtype = scales . dtype ) * stride x_centers = tf . cast ( tf . range ( max_stride ) , dtype = scales . dtype ) * stride x_centers , y_centers = tf . meshgrid ( x_centers , y_centers ) shifts = tf . stack ( [ x_centers , y_centers , x_centers , y_centers ], axis =- 1 ) anchors = tf . expand_dims ( base_anchors , 0 ) + tf . expand_dims ( shifts , 2 ) return tf . gather ( anchors , [ 1 , 0 , 3 , 2 ], axis =- 1 )","title":"generate_anchors"},{"location":"reference/kerod/layers/anchors/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/anchors/#anchors","text":"class Anchors ( stride , scales , ratios , ** kwargs ) At each forward according to the shape of the tensor we can extract the corresponding anchors.","title":"Anchors"},{"location":"reference/kerod/layers/anchors/#arguments","text":"Name Description stride Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales The scale of the anchors e.g: 8, 16, 32 ratios The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size Maximum size of the input image. The anchors will computed once and for all.","title":"Arguments"},{"location":"reference/kerod/layers/anchors/#call-arguments","text":"Name Description inputs A tensor of shape [batch_size, height, widht, channel]","title":"Call arguments"},{"location":"reference/kerod/layers/anchors/#call-returns","text":"Type Description tf.Tensor A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max].","title":"Call returns"},{"location":"reference/kerod/layers/anchors/#ancestors-in-mro","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/anchors/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/anchors/#call","text":"def call ( self , inputs ) Return anchors based on the shape of the input tensors Parameters: Name Description inputs A tensor of shape [batch_size, height, widht, channel] Returns: Type Description tf.Tensor A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. View Source def call ( self , inputs ) : \"\"\" Return anchors based on the shape of the input tensors Arguments : inputs : A tensor of shape [ batch_size , height , widht , channel ] Returns : tf . Tensor : A tensor of shape [ num_scales * num_ratios * height * width , 4 ]. The anchors have the format [ y_min , x_min , y_max , x_max ]. \"\"\" shape = tf . shape ( inputs ) height , width = shape [ 1 ], shape [ 2 ] anchors = self . _anchors [: height , : width ] return tf . reshape ( anchors , ( - 1 , 4 ))","title":"call"},{"location":"reference/kerod/layers/attentions/","text":"Module kerod.layers.attentions None None View Source import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class MultiHeadAttention ( tf . keras . layers . Layer ): \"\"\"Allows the model to jointly attend to information from different representation subspaces. See reference: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) Arguments: d_model: The number of expected features in the decoder inputs num_heads: The number of heads in the multiheadattention models. dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. attention_axes: axes over which the attention is applied. `None` means attention over all axes, but batch, heads, and features. Call arguments: value: A 3-D tensor of shape [batch_size, seq_len, depth_v] key: A 3-D tensor of shape [batch_size, seq_len, depth] query: A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. attn_mask: A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Call returns: tf.Tensor: A 3-D tensor of shape [batch_size, seq_len_q, d_model] \"\"\" def __init__ ( self , d_model : int , num_heads : int , dropout_rate = 0. , attention_axes =- 1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_heads = num_heads self . d_model = d_model assert d_model % self . num_heads == 0 self . depth = d_model // self . num_heads self . query = tf . keras . layers . Dense ( d_model ) self . key = tf . keras . layers . Dense ( d_model ) self . value = tf . keras . layers . Dense ( d_model ) self . dense = tf . keras . layers . Dense ( d_model ) self . dropout = tf . keras . layers . Dropout ( dropout_rate ) self . softmax = tf . keras . layers . Softmax ( axis = attention_axes ) def split_heads ( self , tgt : tf . Tensor , batch_size : int ): \"\"\"Split the last dimension into (num_heads, depth). Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth) \"\"\" tgt = tf . reshape ( tgt , ( batch_size , - 1 , self . num_heads , self . depth )) return tf . transpose ( tgt , perm = [ 0 , 2 , 1 , 3 ]) def call ( self , value , key , query , key_padding_mask = None , attn_mask = None , training = None ): \"\"\" Args: value: A 3-D tensor of shape [batch_size, seq_len, depth_v] key: A 3-D tensor of shape [batch_size, seq_len, depth] query: A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. attn_mask: A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Returns: tf.Tensor: A 3-D tensor of shape [batch_size, seq_len_q, d_model] \"\"\" batch_size = tf . shape ( query )[ 0 ] # (batch_size, num_heads, seq_len_q, depth) query = self . split_heads ( self . query ( query ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) key = self . split_heads ( self . key ( key ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) value = self . split_heads ( self . value ( value ), batch_size ) # scaled dot product attention # (batch_size, nh, seq_len_q, depth) x (batch_size, nh, depth, seq_len_k) # = (batch_size, nh, seq_len_q, seq_len_k) matmul_qk = tf . matmul ( query , key , transpose_b = True ) # Here we normalize by depth_k suppose K and Q are two matrices # with mean=0 and var=1. After QK^T will have a matrix with # mean=0 and var= 1 * depth_k. QK^T/sqrt(depth_k) => mean=0 and var=1 scaled_attention_logits = matmul_qk / tf . math . sqrt ( tf . cast ( self . depth , self . compute_dtype )) if attn_mask is not None : scaled_attention_logits += attn_mask if key_padding_mask is not None : # Apply -inf if the pixels is a padding # False means padded so we take: not key_padding_mask scaled_attention_logits = tf . where ( ~ key_padding_mask [:, None , None ], tf . zeros_like ( scaled_attention_logits ) + float ( '-inf' ), scaled_attention_logits ) # softmax is normalized on the last axis (seq_len_k) so that the scores # add up to 1. # (..., seq_len_q, seq_len_k) attention_weights = self . softmax ( scaled_attention_logits ) attention_weights = self . dropout ( attention_weights , training = training ) scaled_attention = tf . matmul ( attention_weights , value ) # (batch_size, seq_len_q, nh, depth) scaled_attention = tf . transpose ( scaled_attention , perm = [ 0 , 2 , 1 , 3 ]) # (batch_size, seq_len_q, d_model) concat_attention = tf . reshape ( scaled_attention , ( batch_size , - 1 , self . d_model )) return self . dense ( concat_attention ) remove_unwanted_doc ( MultiHeadAttention , __pdoc__ ) Classes MultiHeadAttention class MultiHeadAttention ( d_model : int , num_heads : int , dropout_rate = 0.0 , attention_axes =- 1 , ** kwargs ) See reference: Attention Is All You Need Arguments Name Description d_model The number of expected features in the decoder inputs num_heads The number of heads in the multiheadattention models. dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. attention_axes axes over which the attention is applied. None means attention over all axes, but batch, heads, and features. Call arguments Name Description value A 3-D tensor of shape [batch_size, seq_len, depth_v] key A 3-D tensor of shape [batch_size, seq_len, depth] query A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of True will be ignored while the position with the value of False will be unchanged. attn_mask A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Call returns Type Description tf.Tensor A 3-D tensor of shape [batch_size, seq_len_q, d_model] Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , value , key , query , key_padding_mask = None , attn_mask = None , training = None ) Parameters: Name Description value A 3-D tensor of shape [batch_size, seq_len, depth_v] key A 3-D tensor of shape [batch_size, seq_len, depth] query A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of True will be ignored while the position with the value of False will be unchanged. attn_mask A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Returns: Type Description tf.Tensor A 3-D tensor of shape [batch_size, seq_len_q, d_model] View Source def call ( self , value , key , query , key_padding_mask = None , attn_mask = None , training = None ) : \" \"\" Args: value: A 3-D tensor of shape [batch_size, seq_len, depth_v] key: A 3-D tensor of shape [batch_size, seq_len, depth] query: A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. attn_mask: A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Returns: tf.Tensor: A 3-D tensor of shape [batch_size, seq_len_q, d_model] \"\" \" batch_size = tf . shape ( query ) [ 0 ] # (batch_size, num_heads, seq_len_q, depth) query = self . split_heads ( self . query ( query ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) key = self . split_heads ( self . key ( key ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) value = self . split_heads ( self . value ( value ), batch_size ) # scaled dot product attention # (batch_size, nh, seq_len_q, depth) x (batch_size, nh, depth, seq_len_k) # = (batch_size, nh, seq_len_q, seq_len_k) matmul_qk = tf . matmul ( query , key , transpose_b = True ) # Here we normalize by depth_k suppose K and Q are two matrices # with mean=0 and var=1. After QK^T will have a matrix with # mean=0 and var= 1 * depth_k. QK^T/sqrt(depth_k) => mean=0 and var=1 scaled_attention_logits = matmul_qk / tf . math . sqrt ( tf . cast ( self . depth , self . compute_dtype )) if attn_mask is not None : scaled_attention_logits += attn_mask if key_padding_mask is not None : # Apply -inf if the pixels is a padding # False means padded so we take: not key_padding_mask scaled_attention_logits = tf . where ( ~ key_padding_mask [ : , None , None ] , tf . zeros_like ( scaled_attention_logits ) + float ( '-inf' ), scaled_attention_logits ) # softmax is normalized on the last axis (seq_len_k) so that the scores # add up to 1. # (..., seq_len_q, seq_len_k) attention_weights = self . softmax ( scaled_attention_logits ) attention_weights = self . dropout ( attention_weights , training = training ) scaled_attention = tf . matmul ( attention_weights , value ) # (batch_size, seq_len_q, nh, depth) scaled_attention = tf . transpose ( scaled_attention , perm = [ 0 , 2 , 1 , 3 ] ) # (batch_size, seq_len_q, d_model) concat_attention = tf . reshape ( scaled_attention , ( batch_size , - 1 , self . d_model )) return self . dense ( concat_attention ) split_heads def split_heads ( self , tgt : tensorflow . python . framework . ops . Tensor , batch_size : int ) Split the last dimension into (num_heads, depth). Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth) View Source def split_heads ( self , tgt : tf . Tensor , batch_size : int ) : \"\"\" Split the last dimension into (num_heads, depth). Transpose the result such that the shape is ( batch_size , num_heads , seq_len , depth ) \"\"\" tgt = tf . reshape ( tgt , ( batch_size , - 1 , self . num_heads , self . depth )) return tf . transpose ( tgt , perm = [ 0 , 2 , 1 , 3 ] )","title":"Attentions"},{"location":"reference/kerod/layers/attentions/#module-kerodlayersattentions","text":"None None View Source import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class MultiHeadAttention ( tf . keras . layers . Layer ): \"\"\"Allows the model to jointly attend to information from different representation subspaces. See reference: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) Arguments: d_model: The number of expected features in the decoder inputs num_heads: The number of heads in the multiheadattention models. dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. attention_axes: axes over which the attention is applied. `None` means attention over all axes, but batch, heads, and features. Call arguments: value: A 3-D tensor of shape [batch_size, seq_len, depth_v] key: A 3-D tensor of shape [batch_size, seq_len, depth] query: A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. attn_mask: A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Call returns: tf.Tensor: A 3-D tensor of shape [batch_size, seq_len_q, d_model] \"\"\" def __init__ ( self , d_model : int , num_heads : int , dropout_rate = 0. , attention_axes =- 1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_heads = num_heads self . d_model = d_model assert d_model % self . num_heads == 0 self . depth = d_model // self . num_heads self . query = tf . keras . layers . Dense ( d_model ) self . key = tf . keras . layers . Dense ( d_model ) self . value = tf . keras . layers . Dense ( d_model ) self . dense = tf . keras . layers . Dense ( d_model ) self . dropout = tf . keras . layers . Dropout ( dropout_rate ) self . softmax = tf . keras . layers . Softmax ( axis = attention_axes ) def split_heads ( self , tgt : tf . Tensor , batch_size : int ): \"\"\"Split the last dimension into (num_heads, depth). Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth) \"\"\" tgt = tf . reshape ( tgt , ( batch_size , - 1 , self . num_heads , self . depth )) return tf . transpose ( tgt , perm = [ 0 , 2 , 1 , 3 ]) def call ( self , value , key , query , key_padding_mask = None , attn_mask = None , training = None ): \"\"\" Args: value: A 3-D tensor of shape [batch_size, seq_len, depth_v] key: A 3-D tensor of shape [batch_size, seq_len, depth] query: A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. attn_mask: A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Returns: tf.Tensor: A 3-D tensor of shape [batch_size, seq_len_q, d_model] \"\"\" batch_size = tf . shape ( query )[ 0 ] # (batch_size, num_heads, seq_len_q, depth) query = self . split_heads ( self . query ( query ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) key = self . split_heads ( self . key ( key ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) value = self . split_heads ( self . value ( value ), batch_size ) # scaled dot product attention # (batch_size, nh, seq_len_q, depth) x (batch_size, nh, depth, seq_len_k) # = (batch_size, nh, seq_len_q, seq_len_k) matmul_qk = tf . matmul ( query , key , transpose_b = True ) # Here we normalize by depth_k suppose K and Q are two matrices # with mean=0 and var=1. After QK^T will have a matrix with # mean=0 and var= 1 * depth_k. QK^T/sqrt(depth_k) => mean=0 and var=1 scaled_attention_logits = matmul_qk / tf . math . sqrt ( tf . cast ( self . depth , self . compute_dtype )) if attn_mask is not None : scaled_attention_logits += attn_mask if key_padding_mask is not None : # Apply -inf if the pixels is a padding # False means padded so we take: not key_padding_mask scaled_attention_logits = tf . where ( ~ key_padding_mask [:, None , None ], tf . zeros_like ( scaled_attention_logits ) + float ( '-inf' ), scaled_attention_logits ) # softmax is normalized on the last axis (seq_len_k) so that the scores # add up to 1. # (..., seq_len_q, seq_len_k) attention_weights = self . softmax ( scaled_attention_logits ) attention_weights = self . dropout ( attention_weights , training = training ) scaled_attention = tf . matmul ( attention_weights , value ) # (batch_size, seq_len_q, nh, depth) scaled_attention = tf . transpose ( scaled_attention , perm = [ 0 , 2 , 1 , 3 ]) # (batch_size, seq_len_q, d_model) concat_attention = tf . reshape ( scaled_attention , ( batch_size , - 1 , self . d_model )) return self . dense ( concat_attention ) remove_unwanted_doc ( MultiHeadAttention , __pdoc__ )","title":"Module kerod.layers.attentions"},{"location":"reference/kerod/layers/attentions/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/attentions/#multiheadattention","text":"class MultiHeadAttention ( d_model : int , num_heads : int , dropout_rate = 0.0 , attention_axes =- 1 , ** kwargs ) See reference: Attention Is All You Need","title":"MultiHeadAttention"},{"location":"reference/kerod/layers/attentions/#arguments","text":"Name Description d_model The number of expected features in the decoder inputs num_heads The number of heads in the multiheadattention models. dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. attention_axes axes over which the attention is applied. None means attention over all axes, but batch, heads, and features.","title":"Arguments"},{"location":"reference/kerod/layers/attentions/#call-arguments","text":"Name Description value A 3-D tensor of shape [batch_size, seq_len, depth_v] key A 3-D tensor of shape [batch_size, seq_len, depth] query A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of True will be ignored while the position with the value of False will be unchanged. attn_mask A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight.","title":"Call arguments"},{"location":"reference/kerod/layers/attentions/#call-returns","text":"Type Description tf.Tensor A 3-D tensor of shape [batch_size, seq_len_q, d_model]","title":"Call returns"},{"location":"reference/kerod/layers/attentions/#ancestors-in-mro","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/attentions/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/attentions/#call","text":"def call ( self , value , key , query , key_padding_mask = None , attn_mask = None , training = None ) Parameters: Name Description value A 3-D tensor of shape [batch_size, seq_len, depth_v] key A 3-D tensor of shape [batch_size, seq_len, depth] query A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of True will be ignored while the position with the value of False will be unchanged. attn_mask A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Returns: Type Description tf.Tensor A 3-D tensor of shape [batch_size, seq_len_q, d_model] View Source def call ( self , value , key , query , key_padding_mask = None , attn_mask = None , training = None ) : \" \"\" Args: value: A 3-D tensor of shape [batch_size, seq_len, depth_v] key: A 3-D tensor of shape [batch_size, seq_len, depth] query: A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. attn_mask: A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Returns: tf.Tensor: A 3-D tensor of shape [batch_size, seq_len_q, d_model] \"\" \" batch_size = tf . shape ( query ) [ 0 ] # (batch_size, num_heads, seq_len_q, depth) query = self . split_heads ( self . query ( query ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) key = self . split_heads ( self . key ( key ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) value = self . split_heads ( self . value ( value ), batch_size ) # scaled dot product attention # (batch_size, nh, seq_len_q, depth) x (batch_size, nh, depth, seq_len_k) # = (batch_size, nh, seq_len_q, seq_len_k) matmul_qk = tf . matmul ( query , key , transpose_b = True ) # Here we normalize by depth_k suppose K and Q are two matrices # with mean=0 and var=1. After QK^T will have a matrix with # mean=0 and var= 1 * depth_k. QK^T/sqrt(depth_k) => mean=0 and var=1 scaled_attention_logits = matmul_qk / tf . math . sqrt ( tf . cast ( self . depth , self . compute_dtype )) if attn_mask is not None : scaled_attention_logits += attn_mask if key_padding_mask is not None : # Apply -inf if the pixels is a padding # False means padded so we take: not key_padding_mask scaled_attention_logits = tf . where ( ~ key_padding_mask [ : , None , None ] , tf . zeros_like ( scaled_attention_logits ) + float ( '-inf' ), scaled_attention_logits ) # softmax is normalized on the last axis (seq_len_k) so that the scores # add up to 1. # (..., seq_len_q, seq_len_k) attention_weights = self . softmax ( scaled_attention_logits ) attention_weights = self . dropout ( attention_weights , training = training ) scaled_attention = tf . matmul ( attention_weights , value ) # (batch_size, seq_len_q, nh, depth) scaled_attention = tf . transpose ( scaled_attention , perm = [ 0 , 2 , 1 , 3 ] ) # (batch_size, seq_len_q, d_model) concat_attention = tf . reshape ( scaled_attention , ( batch_size , - 1 , self . d_model )) return self . dense ( concat_attention )","title":"call"},{"location":"reference/kerod/layers/attentions/#split_heads","text":"def split_heads ( self , tgt : tensorflow . python . framework . ops . Tensor , batch_size : int ) Split the last dimension into (num_heads, depth). Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth) View Source def split_heads ( self , tgt : tf . Tensor , batch_size : int ) : \"\"\" Split the last dimension into (num_heads, depth). Transpose the result such that the shape is ( batch_size , num_heads , seq_len , depth ) \"\"\" tgt = tf . reshape ( tgt , ( batch_size , - 1 , self . num_heads , self . depth )) return tf . transpose ( tgt , perm = [ 0 , 2 , 1 , 3 ] )","title":"split_heads"},{"location":"reference/kerod/layers/patches/","text":"Module kerod.layers.patches None None View Source import tensorflow as tf class Patches ( tf . keras . layers . Layer ): \"\"\"Extract `patches` from `images`. This op collects patches from the input image, as if applying a convolution. All extracted patches are stacked in the depth (last) dimension of the output. Argument: patch_size: Inputs: images: A 3-D tensor of shape [batch_size, height, width, nb_channel] Output: patches: A 3-D tensor of shape [batch_size, ] \"\"\" def __init__ ( self , patch_size : int ): super () . __init__ () self . patch_size = patch_size def call ( self , images ): batch_size = tf . shape ( images )[ 0 ] patches = tf . image . extract_patches ( images = images , sizes = [ 1 , self . patch_size , self . patch_size , 1 ], strides = [ 1 , self . patch_size , self . patch_size , 1 ], rates = [ 1 , 1 , 1 , 1 ], padding = \"VALID\" , ) patch_dims = patches . shape [ - 1 ] patches = tf . reshape ( patches , [ batch_size , - 1 , patch_dims ]) return patches Classes Patches class Patches ( patch_size : int ) This op collects patches from the input image, as if applying a convolution. All extracted patches are stacked in the depth (last) dimension of the output. Arguments Name Description patch_size Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector Static methods from_config def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Description config A Python dictionary, typically the output of get_config. Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config ) with_name_scope def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Description method The method to wrap. Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope ) Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( f 'Unknown keyword arguments: {kwargs.keys()}. ' 'Expected `aggregation`.' ) from_metric_obj = hasattr ( value , '_metric_obj' ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' , stacklevel = 2 ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Args: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? elif 'getter' not in kwargs : # When `getter` is specified, it's possibly fine for `initializer` to be # None since it's up to the custom `getter` to raise error in case it # indeed needs `initializer`. raise ValueError ( f 'An initializer for variable {name} of type ' f '{dtype.base_dtype} is required for layer ' f '{self.name}. Received: {initializer}.' ) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( '`caching_device` does not work with mixed precision API. Ignoring ' 'user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable apply def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Args: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' , stacklevel = 2 ) return self . __call__ ( inputs , * args , ** kwargs ) build def build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. It is invoked automatically before the first execution of call() . This is typically used to create the weights of Layer subclasses (at the discretion of the subclass implementer). Parameters: Name Description input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). View Source @tf.__internal__.tracking.no_automatic_dependency_tracking @generic_utils.default def build ( self , input_shape ) : \" \"\" Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. It is invoked automatically before the first execution of `call()`. This is typically used to create the weights of `Layer` subclasses (at the discretion of the subclass implementer). Args: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). \"\" \" # Only record the build input shapes of overridden build methods. if not hasattr ( self . build , '_is_default' ) : self . _build_input_shape = input_shape self . built = True call def call ( self , images ) View Source def call ( self , images ) : batch_size = tf . shape ( images ) [ 0 ] patches = tf . image . extract_patches ( images = images , sizes = [ 1 , self . patch_size , self . patch_size , 1 ], strides = [ 1 , self . patch_size , self . patch_size , 1 ], rates = [ 1 , 1 , 1 , 1 ], padding = \" VALID \" , ) patch_dims = patches . shape [ - 1 ] patches = tf . reshape ( patches , [ batch_size , - 1 , patch_dims ] ) return patches compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + '_scratch_graph' with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported. ' f 'Received: {s}.' ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` ' f 'on layer {self.name}' ', but the layer isn \\' t built. ' 'You can build it manually via: ' f '`{self.name}.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights ) finalize_state def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass get_config def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Python dictionary. \"\" \" all_args = tf_inspect . getfullargspec ( self . __init__ ). args config = { 'name' : self . name , 'trainable' : self . trainable , } if hasattr ( self , '_batch_input_shape' ) : config [ 'batch_input_shape' ] = self . _batch_input_shape config [ 'dtype' ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , 'dynamic' ) : # Only include `dynamic` in the `config` if it is `True` if self . dynamic : config [ 'dynamic' ] = self . dynamic elif 'dynamic' in all_args : all_args . remove ( 'dynamic' ) expected_args = config . keys () # Finds all arguments in the `__init__` that are not in the config: extra_args = [ arg for arg in all_args if arg not in expected_args ] # Check that either the only argument in the `__init__` is `self`, # or that `get_config` has been overridden: if len ( extra_args ) > 1 and hasattr ( self . get_config , '_is_default' ) : raise NotImplementedError ( textwrap . dedent ( f \" \"\" Layer {self.__class__.__name__} has arguments {extra_args} in `__init__` and therefore must override `get_config()`. Example: class CustomLayer(keras.layers.Layer): def __init__(self, arg1, arg2): super().__init__() self.arg1 = arg1 self.arg2 = arg2 def get_config(self): config = super().get_config() config.update({{ \" arg1 \": self.arg1, \" arg2 \": self.arg2, }}) return config \"\" \" )) return config get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' ) get_losses_for def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Args: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' , stacklevel = 2 ) return self . losses get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' ) get_updates_for def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Args: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' , stacklevel = 2 ) return self . updates get_weights def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set _weights ( self , weights ) : \" \"\" Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Args: weights: a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\" \" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( 'You called `set_weights(weights)` on layer \"%s\" ' 'with a weight list of length %s, but the layer was ' 'expecting %s weights. Provided weights: %s...' % ( self . name , len ( weights ), expected_num_weights , str ( weights ) [ : 50 ] )) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set _weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , 'shape' ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f 'Layer {self.name} weight shape {ref_shape} ' 'is not compatible with provided weight ' f 'shape {weight_shape}.' ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"Patches"},{"location":"reference/kerod/layers/patches/#module-kerodlayerspatches","text":"None None View Source import tensorflow as tf class Patches ( tf . keras . layers . Layer ): \"\"\"Extract `patches` from `images`. This op collects patches from the input image, as if applying a convolution. All extracted patches are stacked in the depth (last) dimension of the output. Argument: patch_size: Inputs: images: A 3-D tensor of shape [batch_size, height, width, nb_channel] Output: patches: A 3-D tensor of shape [batch_size, ] \"\"\" def __init__ ( self , patch_size : int ): super () . __init__ () self . patch_size = patch_size def call ( self , images ): batch_size = tf . shape ( images )[ 0 ] patches = tf . image . extract_patches ( images = images , sizes = [ 1 , self . patch_size , self . patch_size , 1 ], strides = [ 1 , self . patch_size , self . patch_size , 1 ], rates = [ 1 , 1 , 1 , 1 ], padding = \"VALID\" , ) patch_dims = patches . shape [ - 1 ] patches = tf . reshape ( patches , [ batch_size , - 1 , patch_dims ]) return patches","title":"Module kerod.layers.patches"},{"location":"reference/kerod/layers/patches/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/patches/#patches","text":"class Patches ( patch_size : int ) This op collects patches from the input image, as if applying a convolution. All extracted patches are stacked in the depth (last) dimension of the output.","title":"Patches"},{"location":"reference/kerod/layers/patches/#arguments","text":"Name Description patch_size","title":"Arguments"},{"location":"reference/kerod/layers/patches/#ancestors-in-mro","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/patches/#static-methods","text":"","title":"Static methods"},{"location":"reference/kerod/layers/patches/#from_config","text":"def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Description config A Python dictionary, typically the output of get_config. Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config )","title":"from_config"},{"location":"reference/kerod/layers/patches/#with_name_scope","text":"def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Description method The method to wrap. Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope )","title":"with_name_scope"},{"location":"reference/kerod/layers/patches/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/patches/#add_loss","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/kerod/layers/patches/#add_metric","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( f 'Unknown keyword arguments: {kwargs.keys()}. ' 'Expected `aggregation`.' ) from_metric_obj = hasattr ( value , '_metric_obj' ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/kerod/layers/patches/#add_update","text":"def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable","title":"add_update"},{"location":"reference/kerod/layers/patches/#add_variable","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' , stacklevel = 2 ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/kerod/layers/patches/#add_weight","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Args: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? elif 'getter' not in kwargs : # When `getter` is specified, it's possibly fine for `initializer` to be # None since it's up to the custom `getter` to raise error in case it # indeed needs `initializer`. raise ValueError ( f 'An initializer for variable {name} of type ' f '{dtype.base_dtype} is required for layer ' f '{self.name}. Received: {initializer}.' ) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( '`caching_device` does not work with mixed precision API. Ignoring ' 'user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/kerod/layers/patches/#apply","text":"def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Args: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' , stacklevel = 2 ) return self . __call__ ( inputs , * args , ** kwargs )","title":"apply"},{"location":"reference/kerod/layers/patches/#build","text":"def build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. It is invoked automatically before the first execution of call() . This is typically used to create the weights of Layer subclasses (at the discretion of the subclass implementer). Parameters: Name Description input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). View Source @tf.__internal__.tracking.no_automatic_dependency_tracking @generic_utils.default def build ( self , input_shape ) : \" \"\" Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. It is invoked automatically before the first execution of `call()`. This is typically used to create the weights of `Layer` subclasses (at the discretion of the subclass implementer). Args: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). \"\" \" # Only record the build input shapes of overridden build methods. if not hasattr ( self . build , '_is_default' ) : self . _build_input_shape = input_shape self . built = True","title":"build"},{"location":"reference/kerod/layers/patches/#call","text":"def call ( self , images ) View Source def call ( self , images ) : batch_size = tf . shape ( images ) [ 0 ] patches = tf . image . extract_patches ( images = images , sizes = [ 1 , self . patch_size , self . patch_size , 1 ], strides = [ 1 , self . patch_size , self . patch_size , 1 ], rates = [ 1 , 1 , 1 , 1 ], padding = \" VALID \" , ) patch_dims = patches . shape [ - 1 ] patches = tf . reshape ( patches , [ batch_size , - 1 , patch_dims ] ) return patches","title":"call"},{"location":"reference/kerod/layers/patches/#compute_mask","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/kerod/layers/patches/#compute_output_shape","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + '_scratch_graph' with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/kerod/layers/patches/#compute_output_signature","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported. ' f 'Received: {s}.' ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/kerod/layers/patches/#count_params","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` ' f 'on layer {self.name}' ', but the layer isn \\' t built. ' 'You can build it manually via: ' f '`{self.name}.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/kerod/layers/patches/#finalize_state","text":"def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass","title":"finalize_state"},{"location":"reference/kerod/layers/patches/#get_config","text":"def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Python dictionary. \"\" \" all_args = tf_inspect . getfullargspec ( self . __init__ ). args config = { 'name' : self . name , 'trainable' : self . trainable , } if hasattr ( self , '_batch_input_shape' ) : config [ 'batch_input_shape' ] = self . _batch_input_shape config [ 'dtype' ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , 'dynamic' ) : # Only include `dynamic` in the `config` if it is `True` if self . dynamic : config [ 'dynamic' ] = self . dynamic elif 'dynamic' in all_args : all_args . remove ( 'dynamic' ) expected_args = config . keys () # Finds all arguments in the `__init__` that are not in the config: extra_args = [ arg for arg in all_args if arg not in expected_args ] # Check that either the only argument in the `__init__` is `self`, # or that `get_config` has been overridden: if len ( extra_args ) > 1 and hasattr ( self . get_config , '_is_default' ) : raise NotImplementedError ( textwrap . dedent ( f \" \"\" Layer {self.__class__.__name__} has arguments {extra_args} in `__init__` and therefore must override `get_config()`. Example: class CustomLayer(keras.layers.Layer): def __init__(self, arg1, arg2): super().__init__() self.arg1 = arg1 self.arg2 = arg2 def get_config(self): config = super().get_config() config.update({{ \" arg1 \": self.arg1, \" arg2 \": self.arg2, }}) return config \"\" \" )) return config","title":"get_config"},{"location":"reference/kerod/layers/patches/#get_input_at","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' )","title":"get_input_at"},{"location":"reference/kerod/layers/patches/#get_input_mask_at","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None )","title":"get_input_mask_at"},{"location":"reference/kerod/layers/patches/#get_input_shape_at","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' )","title":"get_input_shape_at"},{"location":"reference/kerod/layers/patches/#get_losses_for","text":"def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Args: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' , stacklevel = 2 ) return self . losses","title":"get_losses_for"},{"location":"reference/kerod/layers/patches/#get_output_at","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' )","title":"get_output_at"},{"location":"reference/kerod/layers/patches/#get_output_mask_at","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None )","title":"get_output_mask_at"},{"location":"reference/kerod/layers/patches/#get_output_shape_at","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' )","title":"get_output_shape_at"},{"location":"reference/kerod/layers/patches/#get_updates_for","text":"def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Args: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' , stacklevel = 2 ) return self . updates","title":"get_updates_for"},{"location":"reference/kerod/layers/patches/#get_weights","text":"def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights )","title":"get_weights"},{"location":"reference/kerod/layers/patches/#set_weights","text":"def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set _weights ( self , weights ) : \" \"\" Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Args: weights: a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\" \" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( 'You called `set_weights(weights)` on layer \"%s\" ' 'with a weight list of length %s, but the layer was ' 'expecting %s weights. Provided weights: %s...' % ( self . name , len ( weights ), expected_num_weights , str ( weights ) [ : 50 ] )) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set _weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , 'shape' ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f 'Layer {self.name} weight shape {ref_shape} ' 'is not compatible with provided weight ' f 'shape {weight_shape}.' ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"set_weights"},{"location":"reference/kerod/layers/positional_encoding/","text":"Module kerod.layers.positional_encoding None None View Source import math import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class PositionEmbeddingLearned ( tf . keras . layers . Layer ): \"\"\"Absolute pos embedding, learned. Arguments: output_dim: Dimension of the dense embedding. Call arguments: inputs: A 4-D Tensor of shape [batch_size, h, w, channel] Call returns: tf.Tensor: The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] \"\"\" def __init__ ( self , output_dim = 512 , ** kwargs ): super () . __init__ ( ** kwargs ) if output_dim % 2 != 0 : raise ValueError ( \"x an y embedding will be concatened to form a single vector \" f \"of shape output_dim. Please use a multiple of 2 (e.g { output_dim } )\" ) self . dim = int ( output_dim / 2 ) self . row_embed = tf . keras . layers . Embedding ( 50 , self . dim ) self . col_embed = tf . keras . layers . Embedding ( 50 , self . dim ) def call ( self , inputs ): \"\"\"Based on the shape of the input tensor return a positional embedding. Arguments: inputs: A 4-D Tensor of shape [batch_size, h, w, channel] Returns: tf.Tensor: The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] \"\"\" batch_size , h , w = tf . shape ( inputs )[ 0 ], tf . shape ( inputs )[ 1 ], tf . shape ( inputs )[ 2 ] i = tf . range ( w ) j = tf . range ( h ) x_emb = self . col_embed ( i ) y_emb = self . row_embed ( j ) single_img_emb = tf . concat ([ tf . tile ( x_emb [ None ], ( h , 1 , 1 )), tf . tile ( y_emb [:, None ], ( 1 , w , 1 )), ], axis =- 1 ) batch_emb = tf . tile ( single_img_emb [ None ], ( batch_size , 1 , 1 , 1 )) return batch_emb class PositionEmbeddingSine ( tf . keras . layers . Layer ): \"\"\" This is a more standard version of the position embedding, very similar to the one used by the Attention is all you need paper, generalized to work on images. ```python import matplotlib.pyplot as plt from kerod.layers.positional_encoding import PositionEmbeddingSine dim = 128 embedding = PositionEmbeddingSine(dim) pos_encoding = embedding(tf.ones((1, 10, 10))) plt.pcolormesh(tf.reshape(pos_encoding, (1, -1, dim))[0], cmap='RdBu') plt.xlabel('Depth') plt.xlim((0, dim)) plt.ylabel('Position') plt.colorbar() plt.show() ``` ![Visualization Positional Encoding](https://raw.githubusercontent.com/EmGarr/kerod/master/ressources/2d_pos_encoding.png) Arguments: output_dim: Dimension of the dense embedding. Call arguments: masks: A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image Call returns: tf.Tensor: The encoding a tensor of float and shape [batch_size, w, h, output_dim] \"\"\" def __init__ ( self , output_dim = 64 , temperature = 10000 ): super () . __init__ () self . temperature = temperature self . scale = 2 * math . pi if output_dim % 2 != 0 : raise ValueError ( \"x an y embedding will be concatened to form a single vector \" f \"of shape output_dim. Please use a multiple of 2 (e.g { output_dim } )\" ) self . dim = int ( output_dim / 2 ) dim_t = tf . range ( self . dim , dtype = tf . float32 ) self . dim_t = self . temperature ** ( 2 * ( dim_t // 2 ) / self . dim ) def call ( self , masks ): \"\"\"From a masks tensor compute the positional encoding Arguments: masks: A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image Returns: tf.Tensor: The encoding a tensor of float and shape [batch_size, w, h, output_dim] \"\"\" masks = tf . cast ( masks , self . compute_dtype ) y_embed = tf . math . cumsum ( masks , axis = 1 ) x_embed = tf . math . cumsum ( masks , axis = 2 ) # Normalize x_embed and y_embed by the maximum values of the cumsum eps = 1e-6 y_embed = y_embed / ( y_embed [:, - 1 :, :] + eps ) * self . scale x_embed = x_embed / ( x_embed [:, :, - 1 :] + eps ) * self . scale pos_x = x_embed [ ... , None ] / self . dim_t pos_y = y_embed [ ... , None ] / self . dim_t pos_x = tf . stack ([ tf . math . sin ( pos_x [ ... , 0 :: 2 ]), tf . math . cos ( pos_x [ ... , 1 :: 2 ]), ], axis = 4 ) pos_y = tf . stack ([ tf . math . sin ( pos_y [ ... , 0 :: 2 ]), tf . math . cos ( pos_y [ ... , 1 :: 2 ]), ], axis = 4 ) batch_size , h , w = tf . shape ( masks )[ 0 ], tf . shape ( masks )[ 1 ], tf . shape ( masks )[ 2 ] pos_x = tf . reshape ( pos_x , ( batch_size , h , w , - 1 )) pos_y = tf . reshape ( pos_y , ( batch_size , h , w , - 1 )) pos_emb = tf . concat ([ pos_y , pos_x ], axis =- 1 ) return pos_emb remove_unwanted_doc ( PositionEmbeddingLearned , __pdoc__ ) remove_unwanted_doc ( PositionEmbeddingSine , __pdoc__ ) Classes PositionEmbeddingLearned class PositionEmbeddingLearned ( output_dim = 512 , ** kwargs ) Arguments Name Description output_dim Dimension of the dense embedding. Call arguments Name Description inputs A 4-D Tensor of shape [batch_size, h, w, channel] Call returns Type Description tf.Tensor The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , inputs ) Based on the shape of the input tensor return a positional embedding. Parameters: Name Description inputs A 4-D Tensor of shape [batch_size, h, w, channel] Returns: Type Description tf.Tensor The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] View Source def call ( self , inputs ) : \"\"\"Based on the shape of the input tensor return a positional embedding. Arguments: inputs: A 4-D Tensor of shape [batch_size, h, w, channel] Returns: tf.Tensor: The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] \"\"\" batch_size , h , w = tf . shape ( inputs ) [ 0 ] , tf . shape ( inputs ) [ 1 ] , tf . shape ( inputs ) [ 2 ] i = tf . range ( w ) j = tf . range ( h ) x_emb = self . col_embed ( i ) y_emb = self . row_embed ( j ) single_img_emb = tf . concat ( [ tf.tile(x_emb[None ] , ( h , 1 , 1 )), tf . tile ( y_emb [ :, None ] , ( 1 , w , 1 )), ] , axis =- 1 ) batch_emb = tf . tile ( single_img_emb [ None ] , ( batch_size , 1 , 1 , 1 )) return batch_emb PositionEmbeddingSine class PositionEmbeddingSine ( output_dim = 64 , temperature = 10000 ) used by the Attention is all you need paper, generalized to work on images. import matplotlib.pyplot as plt from kerod.layers.positional_encoding import PositionEmbeddingSine dim = 128 embedding = PositionEmbeddingSine ( dim ) pos_encoding = embedding ( tf . ones (( 1 , 10 , 10 ))) plt . pcolormesh ( tf . reshape ( pos_encoding , ( 1 , - 1 , dim ))[ 0 ], cmap = 'RdBu' ) plt . xlabel ( 'Depth' ) plt . xlim (( 0 , dim )) plt . ylabel ( 'Position' ) plt . colorbar () plt . show () Arguments Name Description output_dim Dimension of the dense embedding. Call arguments Name Description masks A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image Call returns Type Description tf.Tensor The encoding a tensor of float and shape [batch_size, w, h, output_dim] Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , masks ) From a masks tensor compute the positional encoding Parameters: Name Description masks A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image Returns: Type Description tf.Tensor The encoding a tensor of float and shape [batch_size, w, h, output_dim] View Source def call ( self , masks ) : \"\"\" From a masks tensor compute the positional encoding Arguments : masks : A tensor of bool and shape [ batch_size , w , h ] where False means padding and True pixel from the image Returns : tf . Tensor : The encoding a tensor of float and shape [ batch_size , w , h , output_dim ] \"\"\" masks = tf . cast ( masks , self . compute_dtype ) y_embed = tf . math . cumsum ( masks , axis = 1 ) x_embed = tf . math . cumsum ( masks , axis = 2 ) # Normalize x_embed and y_embed by the maximum values of the cumsum eps = 1 e - 6 y_embed = y_embed / ( y_embed [:, - 1 :, :] + eps ) * self . scale x_embed = x_embed / ( x_embed [:, :, - 1 :] + eps ) * self . scale pos_x = x_embed [..., None ] / self . dim_t pos_y = y_embed [..., None ] / self . dim_t pos_x = tf . stack ( [ tf . math . sin ( pos_x [..., 0 :: 2 ] ) , tf . math . cos ( pos_x [..., 1 :: 2 ] ) , ], axis = 4 ) pos_y = tf . stack ( [ tf . math . sin ( pos_y [..., 0 :: 2 ] ) , tf . math . cos ( pos_y [..., 1 :: 2 ] ) , ], axis = 4 ) batch_size , h , w = tf . shape ( masks ) [ 0 ], tf . shape ( masks ) [ 1 ], tf . shape ( masks ) [ 2 ] pos_x = tf . reshape ( pos_x , ( batch_size , h , w , - 1 )) pos_y = tf . reshape ( pos_y , ( batch_size , h , w , - 1 )) pos_emb = tf . concat ( [ pos_y , pos_x ], axis =- 1 ) return pos_emb","title":"Positional Encoding"},{"location":"reference/kerod/layers/positional_encoding/#module-kerodlayerspositional_encoding","text":"None None View Source import math import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class PositionEmbeddingLearned ( tf . keras . layers . Layer ): \"\"\"Absolute pos embedding, learned. Arguments: output_dim: Dimension of the dense embedding. Call arguments: inputs: A 4-D Tensor of shape [batch_size, h, w, channel] Call returns: tf.Tensor: The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] \"\"\" def __init__ ( self , output_dim = 512 , ** kwargs ): super () . __init__ ( ** kwargs ) if output_dim % 2 != 0 : raise ValueError ( \"x an y embedding will be concatened to form a single vector \" f \"of shape output_dim. Please use a multiple of 2 (e.g { output_dim } )\" ) self . dim = int ( output_dim / 2 ) self . row_embed = tf . keras . layers . Embedding ( 50 , self . dim ) self . col_embed = tf . keras . layers . Embedding ( 50 , self . dim ) def call ( self , inputs ): \"\"\"Based on the shape of the input tensor return a positional embedding. Arguments: inputs: A 4-D Tensor of shape [batch_size, h, w, channel] Returns: tf.Tensor: The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] \"\"\" batch_size , h , w = tf . shape ( inputs )[ 0 ], tf . shape ( inputs )[ 1 ], tf . shape ( inputs )[ 2 ] i = tf . range ( w ) j = tf . range ( h ) x_emb = self . col_embed ( i ) y_emb = self . row_embed ( j ) single_img_emb = tf . concat ([ tf . tile ( x_emb [ None ], ( h , 1 , 1 )), tf . tile ( y_emb [:, None ], ( 1 , w , 1 )), ], axis =- 1 ) batch_emb = tf . tile ( single_img_emb [ None ], ( batch_size , 1 , 1 , 1 )) return batch_emb class PositionEmbeddingSine ( tf . keras . layers . Layer ): \"\"\" This is a more standard version of the position embedding, very similar to the one used by the Attention is all you need paper, generalized to work on images. ```python import matplotlib.pyplot as plt from kerod.layers.positional_encoding import PositionEmbeddingSine dim = 128 embedding = PositionEmbeddingSine(dim) pos_encoding = embedding(tf.ones((1, 10, 10))) plt.pcolormesh(tf.reshape(pos_encoding, (1, -1, dim))[0], cmap='RdBu') plt.xlabel('Depth') plt.xlim((0, dim)) plt.ylabel('Position') plt.colorbar() plt.show() ``` ![Visualization Positional Encoding](https://raw.githubusercontent.com/EmGarr/kerod/master/ressources/2d_pos_encoding.png) Arguments: output_dim: Dimension of the dense embedding. Call arguments: masks: A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image Call returns: tf.Tensor: The encoding a tensor of float and shape [batch_size, w, h, output_dim] \"\"\" def __init__ ( self , output_dim = 64 , temperature = 10000 ): super () . __init__ () self . temperature = temperature self . scale = 2 * math . pi if output_dim % 2 != 0 : raise ValueError ( \"x an y embedding will be concatened to form a single vector \" f \"of shape output_dim. Please use a multiple of 2 (e.g { output_dim } )\" ) self . dim = int ( output_dim / 2 ) dim_t = tf . range ( self . dim , dtype = tf . float32 ) self . dim_t = self . temperature ** ( 2 * ( dim_t // 2 ) / self . dim ) def call ( self , masks ): \"\"\"From a masks tensor compute the positional encoding Arguments: masks: A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image Returns: tf.Tensor: The encoding a tensor of float and shape [batch_size, w, h, output_dim] \"\"\" masks = tf . cast ( masks , self . compute_dtype ) y_embed = tf . math . cumsum ( masks , axis = 1 ) x_embed = tf . math . cumsum ( masks , axis = 2 ) # Normalize x_embed and y_embed by the maximum values of the cumsum eps = 1e-6 y_embed = y_embed / ( y_embed [:, - 1 :, :] + eps ) * self . scale x_embed = x_embed / ( x_embed [:, :, - 1 :] + eps ) * self . scale pos_x = x_embed [ ... , None ] / self . dim_t pos_y = y_embed [ ... , None ] / self . dim_t pos_x = tf . stack ([ tf . math . sin ( pos_x [ ... , 0 :: 2 ]), tf . math . cos ( pos_x [ ... , 1 :: 2 ]), ], axis = 4 ) pos_y = tf . stack ([ tf . math . sin ( pos_y [ ... , 0 :: 2 ]), tf . math . cos ( pos_y [ ... , 1 :: 2 ]), ], axis = 4 ) batch_size , h , w = tf . shape ( masks )[ 0 ], tf . shape ( masks )[ 1 ], tf . shape ( masks )[ 2 ] pos_x = tf . reshape ( pos_x , ( batch_size , h , w , - 1 )) pos_y = tf . reshape ( pos_y , ( batch_size , h , w , - 1 )) pos_emb = tf . concat ([ pos_y , pos_x ], axis =- 1 ) return pos_emb remove_unwanted_doc ( PositionEmbeddingLearned , __pdoc__ ) remove_unwanted_doc ( PositionEmbeddingSine , __pdoc__ )","title":"Module kerod.layers.positional_encoding"},{"location":"reference/kerod/layers/positional_encoding/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/positional_encoding/#positionembeddinglearned","text":"class PositionEmbeddingLearned ( output_dim = 512 , ** kwargs )","title":"PositionEmbeddingLearned"},{"location":"reference/kerod/layers/positional_encoding/#arguments","text":"Name Description output_dim Dimension of the dense embedding.","title":"Arguments"},{"location":"reference/kerod/layers/positional_encoding/#call-arguments","text":"Name Description inputs A 4-D Tensor of shape [batch_size, h, w, channel]","title":"Call arguments"},{"location":"reference/kerod/layers/positional_encoding/#call-returns","text":"Type Description tf.Tensor The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim]","title":"Call returns"},{"location":"reference/kerod/layers/positional_encoding/#ancestors-in-mro","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/positional_encoding/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/positional_encoding/#call","text":"def call ( self , inputs ) Based on the shape of the input tensor return a positional embedding. Parameters: Name Description inputs A 4-D Tensor of shape [batch_size, h, w, channel] Returns: Type Description tf.Tensor The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] View Source def call ( self , inputs ) : \"\"\"Based on the shape of the input tensor return a positional embedding. Arguments: inputs: A 4-D Tensor of shape [batch_size, h, w, channel] Returns: tf.Tensor: The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] \"\"\" batch_size , h , w = tf . shape ( inputs ) [ 0 ] , tf . shape ( inputs ) [ 1 ] , tf . shape ( inputs ) [ 2 ] i = tf . range ( w ) j = tf . range ( h ) x_emb = self . col_embed ( i ) y_emb = self . row_embed ( j ) single_img_emb = tf . concat ( [ tf.tile(x_emb[None ] , ( h , 1 , 1 )), tf . tile ( y_emb [ :, None ] , ( 1 , w , 1 )), ] , axis =- 1 ) batch_emb = tf . tile ( single_img_emb [ None ] , ( batch_size , 1 , 1 , 1 )) return batch_emb","title":"call"},{"location":"reference/kerod/layers/positional_encoding/#positionembeddingsine","text":"class PositionEmbeddingSine ( output_dim = 64 , temperature = 10000 ) used by the Attention is all you need paper, generalized to work on images. import matplotlib.pyplot as plt from kerod.layers.positional_encoding import PositionEmbeddingSine dim = 128 embedding = PositionEmbeddingSine ( dim ) pos_encoding = embedding ( tf . ones (( 1 , 10 , 10 ))) plt . pcolormesh ( tf . reshape ( pos_encoding , ( 1 , - 1 , dim ))[ 0 ], cmap = 'RdBu' ) plt . xlabel ( 'Depth' ) plt . xlim (( 0 , dim )) plt . ylabel ( 'Position' ) plt . colorbar () plt . show ()","title":"PositionEmbeddingSine"},{"location":"reference/kerod/layers/positional_encoding/#arguments_1","text":"Name Description output_dim Dimension of the dense embedding.","title":"Arguments"},{"location":"reference/kerod/layers/positional_encoding/#call-arguments_1","text":"Name Description masks A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image","title":"Call arguments"},{"location":"reference/kerod/layers/positional_encoding/#call-returns_1","text":"Type Description tf.Tensor The encoding a tensor of float and shape [batch_size, w, h, output_dim]","title":"Call returns"},{"location":"reference/kerod/layers/positional_encoding/#ancestors-in-mro_1","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/positional_encoding/#methods_1","text":"","title":"Methods"},{"location":"reference/kerod/layers/positional_encoding/#call_1","text":"def call ( self , masks ) From a masks tensor compute the positional encoding Parameters: Name Description masks A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image Returns: Type Description tf.Tensor The encoding a tensor of float and shape [batch_size, w, h, output_dim] View Source def call ( self , masks ) : \"\"\" From a masks tensor compute the positional encoding Arguments : masks : A tensor of bool and shape [ batch_size , w , h ] where False means padding and True pixel from the image Returns : tf . Tensor : The encoding a tensor of float and shape [ batch_size , w , h , output_dim ] \"\"\" masks = tf . cast ( masks , self . compute_dtype ) y_embed = tf . math . cumsum ( masks , axis = 1 ) x_embed = tf . math . cumsum ( masks , axis = 2 ) # Normalize x_embed and y_embed by the maximum values of the cumsum eps = 1 e - 6 y_embed = y_embed / ( y_embed [:, - 1 :, :] + eps ) * self . scale x_embed = x_embed / ( x_embed [:, :, - 1 :] + eps ) * self . scale pos_x = x_embed [..., None ] / self . dim_t pos_y = y_embed [..., None ] / self . dim_t pos_x = tf . stack ( [ tf . math . sin ( pos_x [..., 0 :: 2 ] ) , tf . math . cos ( pos_x [..., 1 :: 2 ] ) , ], axis = 4 ) pos_y = tf . stack ( [ tf . math . sin ( pos_y [..., 0 :: 2 ] ) , tf . math . cos ( pos_y [..., 1 :: 2 ] ) , ], axis = 4 ) batch_size , h , w = tf . shape ( masks ) [ 0 ], tf . shape ( masks ) [ 1 ], tf . shape ( masks ) [ 2 ] pos_x = tf . reshape ( pos_x , ( batch_size , h , w , - 1 )) pos_y = tf . reshape ( pos_y , ( batch_size , h , w , - 1 )) pos_emb = tf . concat ( [ pos_y , pos_x ], axis =- 1 ) return pos_emb","title":"call"},{"location":"reference/kerod/layers/transformer/","text":"Module kerod.layers.transformer None None View Source import tensorflow as tf from kerod.layers import MultiHeadAttention from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class EncoderLayer ( tf . keras . layers . Layer ): \"\"\"Will build a TransformerEncoderLayer according to the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) Arguments: d_model: The number of expected features in the encoder inputs num_heads: The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the EncoderLayer. dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments: src: A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, dim] positional encoding of the encoder key_padding_mask: [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Call returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" def __init__ ( self , d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . mha = MultiHeadAttention ( d_model , num_heads , dropout_rate = dropout_rate ) self . ffn = tf . keras . Sequential ([ tf . keras . layers . Dense ( dim_feedforward , activation = 'relu' ), tf . keras . layers . Dropout ( dropout_rate ), tf . keras . layers . Dense ( d_model ) ]) self . layernorm1 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . layernorm2 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . dropout1 = tf . keras . layers . Dropout ( dropout_rate ) self . dropout2 = tf . keras . layers . Dropout ( dropout_rate ) def call ( self , src , pos_emb , key_padding_mask = None , training = None ): \"\"\"Forward of the EncoderLayer Args: src: A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed: A 3-D Tensor of float32 and shape [batch_size, M, dim] positional encoding of the encoder key_padding_mask: [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" x_pos_emb = src + pos_emb attn_output = self . mha ( src , x_pos_emb , x_pos_emb , key_padding_mask = key_padding_mask , training = training ) # (batch_size, input_seq_len, d_model) attn_output = self . dropout1 ( attn_output , training = training ) out1 = self . layernorm1 ( src + attn_output ) # (batch_size, input_seq_len, d_model) ffn_output = self . ffn ( out1 ) # (batch_size, input_seq_len, d_model) ffn_output = self . dropout2 ( ffn_output , training = training ) out2 = self . layernorm2 ( out1 + ffn_output ) # (batch_size, input_seq_len, d_model) return out2 class DecoderLayer ( tf . keras . layers . Layer ): \"\"\"Will build a TransformerDecoderLayer according to the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) Arguments: d_model: The number of expected features in the encoder inputs num_heads: The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the DecoderLayer. dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments: dec_out: A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory: A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries: A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn). Call returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" def __init__ ( self , d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . mha1 = MultiHeadAttention ( d_model , num_heads , dropout_rate = dropout_rate ) self . mha2 = MultiHeadAttention ( d_model , num_heads , dropout_rate = dropout_rate ) self . ffn = tf . keras . Sequential ([ tf . keras . layers . Dense ( dim_feedforward , activation = 'relu' ), tf . keras . layers . Dropout ( dropout_rate ), tf . keras . layers . Dense ( d_model ) ]) self . layernorm1 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . layernorm2 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . layernorm3 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . dropout1 = tf . keras . layers . Dropout ( dropout_rate ) self . dropout2 = tf . keras . layers . Dropout ( dropout_rate ) self . dropout3 = tf . keras . layers . Dropout ( dropout_rate ) def call ( self , dec_out , memory , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ): \"\"\" Forward of the DecoderLayer Arguments: dec_out: A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory: A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries: A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. the positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" tgt_object_queries = dec_out + object_queries # (batch_size, M, d_model) self_attn = self . mha1 ( dec_out , tgt_object_queries , tgt_object_queries , training = training ) self_attn = self . dropout1 ( self_attn , training = training ) self_attn = self . layernorm1 ( self_attn + dec_out ) # (batch_size, M, d_model) co_attn = self . mha2 ( memory , memory + pos_embed , self_attn + object_queries , key_padding_mask = key_padding_mask , attn_mask = coattn_mask , training = training ) co_attn = self . dropout2 ( co_attn , training = training ) co_attn = self . layernorm2 ( co_attn + self_attn ) # (batch_size, M, d_model) ffn_output = self . ffn ( co_attn ) # (batch_size, M, d_model) ffn_output = self . dropout3 ( ffn_output , training = training ) out3 = self . layernorm3 ( ffn_output + co_attn ) # (batch_size, M, d_model) return out3 class Transformer ( tf . keras . layers . Layer ): \"\"\"Will build a Transformer according to the paper [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). Args: num_layers: the number of sub-layers in the decoder and the encoder. d_model: The number of expected features in the encoder/decoder inputs num_heads: The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the EncoderLayer and DecoderLayer dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments: flatten_tensor: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask: A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Call returns: Tuple: - `decoder_output`: 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - `encoder_output`: 3-D float32 Tensor of shape [batch_size, batch_size, d_model] \"\"\" def __init__ ( self , num_layers = 6 , d_model = 256 , num_heads = 8 , dim_feedforward = 2048 , dropout_rate = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_heads = num_heads self . layer_norm = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . enc_layers = [ EncoderLayer ( d_model , num_heads , dim_feedforward , dropout_rate ) for _ in range ( num_layers ) ] self . dec_layers = [ DecoderLayer ( d_model , num_heads , dim_feedforward , dropout_rate ) for _ in range ( num_layers ) ] def call ( self , flatten_tensor , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ): \"\"\" Args: flatten_tensor: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask: A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Tuple: - `decoder_output`: 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - `encoder_output`: 3-D float32 Tensor of shape [batch_size, batch_size, d_model] \"\"\" memory = flatten_tensor for enc in self . enc_layers : # (batch_size, seq_len, d_model) memory = enc ( memory , pos_embed , key_padding_mask = key_padding_mask , training = training ) # At the beginning we set target to 0 # In the first decoder layer Q and K will be equal # to dec_out + object_queries=object_queries dec_out = tf . zeros_like ( object_queries ) layers_output = [] for layer in self . dec_layers : dec_out = layer ( dec_out , memory , pos_embed , object_queries , key_padding_mask = key_padding_mask , coattn_mask = coattn_mask , training = training ) dec_out = self . layer_norm ( dec_out ) if training : layers_output . append ( dec_out ) if training : return tf . concat ( layers_output , axis = 1 ), memory return dec_out , memory remove_unwanted_doc ( EncoderLayer , __pdoc__ ) remove_unwanted_doc ( DecoderLayer , __pdoc__ ) remove_unwanted_doc ( Transformer , __pdoc__ ) Classes DecoderLayer class DecoderLayer ( d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ) End-to-End Object Detection with Transformers Arguments Name Description d_model The number of expected features in the encoder inputs num_heads The number of heads in the multiheadattention models. dim_feedforward The dim of the feedforward neuralnetworks in the DecoderLayer. dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments Name Description dec_out A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of True will be ignored while the position with the value of False will be unchanged. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn). Call returns Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model] Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , dec_out , memory , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) Forward of the DecoderLayer Parameters: Name Description dec_out A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. the positions with the value of True will be ignored while the position with the value of False will be unchanged. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model] View Source def call ( self , dec_out , memory , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) : \" \"\" Forward of the DecoderLayer Arguments: dec_out: A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory: A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries: A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. the positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\" \" tgt_object_queries = dec_out + object_queries # (batch_size, M, d_model) self_attn = self . mha1 ( dec_out , tgt_object_queries , tgt_object_queries , training = training ) self_attn = self . dropout1 ( self_attn , training = training ) self_attn = self . layernorm1 ( self_attn + dec_out ) # (batch_size, M, d_model) co_attn = self . mha2 ( memory , memory + pos_embed , self_attn + object_queries , key_padding_mask = key_padding_mask , attn_mask = coattn_mask , training = training ) co_attn = self . dropout2 ( co_attn , training = training ) co_attn = self . layernorm2 ( co_attn + self_attn ) # (batch_size, M, d_model) ffn_output = self . ffn ( co_attn ) # (batch_size, M, d_model) ffn_output = self . dropout3 ( ffn_output , training = training ) out3 = self . layernorm3 ( ffn_output + co_attn ) # (batch_size, M, d_model) return out3 EncoderLayer class EncoderLayer ( d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ) End-to-End Object Detection with Transformers Arguments Name Description d_model The number of expected features in the encoder inputs num_heads The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the EncoderLayer. dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments Name Description src A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed A 3-D Tensor of float32 and shape [batch_size, N, dim] positional encoding of the encoder key_padding_mask [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Call returns Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model] Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , src , pos_emb , key_padding_mask = None , training = None ) Forward of the EncoderLayer Parameters: Name Description src A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed A 3-D Tensor of float32 and shape [batch_size, M, dim] positional encoding of the encoder key_padding_mask [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Returns: Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model] View Source def call ( self , src , pos_emb , key_padding_mask = None , training = None ) : \"\"\"Forward of the EncoderLayer Args: src: A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed: A 3-D Tensor of float32 and shape [batch_size, M, dim] positional encoding of the encoder key_padding_mask: [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" x_pos_emb = src + pos_emb attn_output = self . mha ( src , x_pos_emb , x_pos_emb , key_padding_mask = key_padding_mask , training = training ) # ( batch_size , input_seq_len , d_model ) attn_output = self . dropout1 ( attn_output , training = training ) out1 = self . layernorm1 ( src + attn_output ) # ( batch_size , input_seq_len , d_model ) ffn_output = self . ffn ( out1 ) # ( batch_size , input_seq_len , d_model ) ffn_output = self . dropout2 ( ffn_output , training = training ) out2 = self . layernorm2 ( out1 + ffn_output ) # ( batch_size , input_seq_len , d_model ) return out2 Transformer class Transformer ( num_layers = 6 , d_model = 256 , num_heads = 8 , dim_feedforward = 2048 , dropout_rate = 0.1 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . Arguments Name Description num_layers the number of sub-layers in the decoder and the encoder. d_model The number of expected features in the encoder/decoder inputs num_heads The number of heads in the multiheadattention models. dim_feedforward The dim of the feedforward neuralnetworks in the EncoderLayer and DecoderLayer dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments Name Description flatten_tensor A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Call returns Type Description Tuple - decoder_output : 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - encoder_output : 3-D float32 Tensor of shape [batch_size, batch_size, d_model] Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , flatten_tensor , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) Parameters: Name Description flatten_tensor A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Type Description Tuple - decoder_output : 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - encoder_output : 3-D float32 Tensor of shape [batch_size, batch_size, d_model] View Source def call ( self , flatten_tensor , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) : \" \"\" Args: flatten_tensor: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask: A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Tuple: - `decoder_output`: 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - `encoder_output`: 3-D float32 Tensor of shape [batch_size, batch_size, d_model] \"\" \" memory = flatten_tensor for enc in self . enc_layers : # (batch_size, seq_len, d_model) memory = enc ( memory , pos_embed , key_padding_mask = key_padding_mask , training = training ) # At the beginning we set target to 0 # In the first decoder layer Q and K will be equal # to dec_out + object_queries=object_queries dec_out = tf . zeros_like ( object_queries ) layers_output = [] for layer in self . dec_layers : dec_out = layer ( dec_out , memory , pos_embed , object_queries , key_padding_mask = key_padding_mask , coattn_mask = coattn_mask , training = training ) dec_out = self . layer_norm ( dec_out ) if training : layers_output . append ( dec_out ) if training : return tf . concat ( layers_output , axis = 1 ), memory return dec_out , memory","title":"Transformer"},{"location":"reference/kerod/layers/transformer/#module-kerodlayerstransformer","text":"None None View Source import tensorflow as tf from kerod.layers import MultiHeadAttention from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class EncoderLayer ( tf . keras . layers . Layer ): \"\"\"Will build a TransformerEncoderLayer according to the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) Arguments: d_model: The number of expected features in the encoder inputs num_heads: The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the EncoderLayer. dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments: src: A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, dim] positional encoding of the encoder key_padding_mask: [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Call returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" def __init__ ( self , d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . mha = MultiHeadAttention ( d_model , num_heads , dropout_rate = dropout_rate ) self . ffn = tf . keras . Sequential ([ tf . keras . layers . Dense ( dim_feedforward , activation = 'relu' ), tf . keras . layers . Dropout ( dropout_rate ), tf . keras . layers . Dense ( d_model ) ]) self . layernorm1 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . layernorm2 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . dropout1 = tf . keras . layers . Dropout ( dropout_rate ) self . dropout2 = tf . keras . layers . Dropout ( dropout_rate ) def call ( self , src , pos_emb , key_padding_mask = None , training = None ): \"\"\"Forward of the EncoderLayer Args: src: A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed: A 3-D Tensor of float32 and shape [batch_size, M, dim] positional encoding of the encoder key_padding_mask: [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" x_pos_emb = src + pos_emb attn_output = self . mha ( src , x_pos_emb , x_pos_emb , key_padding_mask = key_padding_mask , training = training ) # (batch_size, input_seq_len, d_model) attn_output = self . dropout1 ( attn_output , training = training ) out1 = self . layernorm1 ( src + attn_output ) # (batch_size, input_seq_len, d_model) ffn_output = self . ffn ( out1 ) # (batch_size, input_seq_len, d_model) ffn_output = self . dropout2 ( ffn_output , training = training ) out2 = self . layernorm2 ( out1 + ffn_output ) # (batch_size, input_seq_len, d_model) return out2 class DecoderLayer ( tf . keras . layers . Layer ): \"\"\"Will build a TransformerDecoderLayer according to the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) Arguments: d_model: The number of expected features in the encoder inputs num_heads: The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the DecoderLayer. dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments: dec_out: A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory: A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries: A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn). Call returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" def __init__ ( self , d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . mha1 = MultiHeadAttention ( d_model , num_heads , dropout_rate = dropout_rate ) self . mha2 = MultiHeadAttention ( d_model , num_heads , dropout_rate = dropout_rate ) self . ffn = tf . keras . Sequential ([ tf . keras . layers . Dense ( dim_feedforward , activation = 'relu' ), tf . keras . layers . Dropout ( dropout_rate ), tf . keras . layers . Dense ( d_model ) ]) self . layernorm1 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . layernorm2 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . layernorm3 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . dropout1 = tf . keras . layers . Dropout ( dropout_rate ) self . dropout2 = tf . keras . layers . Dropout ( dropout_rate ) self . dropout3 = tf . keras . layers . Dropout ( dropout_rate ) def call ( self , dec_out , memory , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ): \"\"\" Forward of the DecoderLayer Arguments: dec_out: A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory: A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries: A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. the positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" tgt_object_queries = dec_out + object_queries # (batch_size, M, d_model) self_attn = self . mha1 ( dec_out , tgt_object_queries , tgt_object_queries , training = training ) self_attn = self . dropout1 ( self_attn , training = training ) self_attn = self . layernorm1 ( self_attn + dec_out ) # (batch_size, M, d_model) co_attn = self . mha2 ( memory , memory + pos_embed , self_attn + object_queries , key_padding_mask = key_padding_mask , attn_mask = coattn_mask , training = training ) co_attn = self . dropout2 ( co_attn , training = training ) co_attn = self . layernorm2 ( co_attn + self_attn ) # (batch_size, M, d_model) ffn_output = self . ffn ( co_attn ) # (batch_size, M, d_model) ffn_output = self . dropout3 ( ffn_output , training = training ) out3 = self . layernorm3 ( ffn_output + co_attn ) # (batch_size, M, d_model) return out3 class Transformer ( tf . keras . layers . Layer ): \"\"\"Will build a Transformer according to the paper [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). Args: num_layers: the number of sub-layers in the decoder and the encoder. d_model: The number of expected features in the encoder/decoder inputs num_heads: The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the EncoderLayer and DecoderLayer dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments: flatten_tensor: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask: A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Call returns: Tuple: - `decoder_output`: 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - `encoder_output`: 3-D float32 Tensor of shape [batch_size, batch_size, d_model] \"\"\" def __init__ ( self , num_layers = 6 , d_model = 256 , num_heads = 8 , dim_feedforward = 2048 , dropout_rate = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_heads = num_heads self . layer_norm = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . enc_layers = [ EncoderLayer ( d_model , num_heads , dim_feedforward , dropout_rate ) for _ in range ( num_layers ) ] self . dec_layers = [ DecoderLayer ( d_model , num_heads , dim_feedforward , dropout_rate ) for _ in range ( num_layers ) ] def call ( self , flatten_tensor , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ): \"\"\" Args: flatten_tensor: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask: A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Tuple: - `decoder_output`: 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - `encoder_output`: 3-D float32 Tensor of shape [batch_size, batch_size, d_model] \"\"\" memory = flatten_tensor for enc in self . enc_layers : # (batch_size, seq_len, d_model) memory = enc ( memory , pos_embed , key_padding_mask = key_padding_mask , training = training ) # At the beginning we set target to 0 # In the first decoder layer Q and K will be equal # to dec_out + object_queries=object_queries dec_out = tf . zeros_like ( object_queries ) layers_output = [] for layer in self . dec_layers : dec_out = layer ( dec_out , memory , pos_embed , object_queries , key_padding_mask = key_padding_mask , coattn_mask = coattn_mask , training = training ) dec_out = self . layer_norm ( dec_out ) if training : layers_output . append ( dec_out ) if training : return tf . concat ( layers_output , axis = 1 ), memory return dec_out , memory remove_unwanted_doc ( EncoderLayer , __pdoc__ ) remove_unwanted_doc ( DecoderLayer , __pdoc__ ) remove_unwanted_doc ( Transformer , __pdoc__ )","title":"Module kerod.layers.transformer"},{"location":"reference/kerod/layers/transformer/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/transformer/#decoderlayer","text":"class DecoderLayer ( d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ) End-to-End Object Detection with Transformers","title":"DecoderLayer"},{"location":"reference/kerod/layers/transformer/#arguments","text":"Name Description d_model The number of expected features in the encoder inputs num_heads The number of heads in the multiheadattention models. dim_feedforward The dim of the feedforward neuralnetworks in the DecoderLayer. dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer.","title":"Arguments"},{"location":"reference/kerod/layers/transformer/#call-arguments","text":"Name Description dec_out A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of True will be ignored while the position with the value of False will be unchanged. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn).","title":"Call arguments"},{"location":"reference/kerod/layers/transformer/#call-returns","text":"Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model]","title":"Call returns"},{"location":"reference/kerod/layers/transformer/#ancestors-in-mro","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/transformer/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/transformer/#call","text":"def call ( self , dec_out , memory , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) Forward of the DecoderLayer Parameters: Name Description dec_out A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. the positions with the value of True will be ignored while the position with the value of False will be unchanged. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model] View Source def call ( self , dec_out , memory , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) : \" \"\" Forward of the DecoderLayer Arguments: dec_out: A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory: A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries: A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. the positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\" \" tgt_object_queries = dec_out + object_queries # (batch_size, M, d_model) self_attn = self . mha1 ( dec_out , tgt_object_queries , tgt_object_queries , training = training ) self_attn = self . dropout1 ( self_attn , training = training ) self_attn = self . layernorm1 ( self_attn + dec_out ) # (batch_size, M, d_model) co_attn = self . mha2 ( memory , memory + pos_embed , self_attn + object_queries , key_padding_mask = key_padding_mask , attn_mask = coattn_mask , training = training ) co_attn = self . dropout2 ( co_attn , training = training ) co_attn = self . layernorm2 ( co_attn + self_attn ) # (batch_size, M, d_model) ffn_output = self . ffn ( co_attn ) # (batch_size, M, d_model) ffn_output = self . dropout3 ( ffn_output , training = training ) out3 = self . layernorm3 ( ffn_output + co_attn ) # (batch_size, M, d_model) return out3","title":"call"},{"location":"reference/kerod/layers/transformer/#encoderlayer","text":"class EncoderLayer ( d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ) End-to-End Object Detection with Transformers","title":"EncoderLayer"},{"location":"reference/kerod/layers/transformer/#arguments_1","text":"Name Description d_model The number of expected features in the encoder inputs num_heads The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the EncoderLayer. dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer.","title":"Arguments"},{"location":"reference/kerod/layers/transformer/#call-arguments_1","text":"Name Description src A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed A 3-D Tensor of float32 and shape [batch_size, N, dim] positional encoding of the encoder key_padding_mask [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image.","title":"Call arguments"},{"location":"reference/kerod/layers/transformer/#call-returns_1","text":"Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model]","title":"Call returns"},{"location":"reference/kerod/layers/transformer/#ancestors-in-mro_1","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/transformer/#methods_1","text":"","title":"Methods"},{"location":"reference/kerod/layers/transformer/#call_1","text":"def call ( self , src , pos_emb , key_padding_mask = None , training = None ) Forward of the EncoderLayer Parameters: Name Description src A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed A 3-D Tensor of float32 and shape [batch_size, M, dim] positional encoding of the encoder key_padding_mask [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Returns: Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model] View Source def call ( self , src , pos_emb , key_padding_mask = None , training = None ) : \"\"\"Forward of the EncoderLayer Args: src: A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed: A 3-D Tensor of float32 and shape [batch_size, M, dim] positional encoding of the encoder key_padding_mask: [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" x_pos_emb = src + pos_emb attn_output = self . mha ( src , x_pos_emb , x_pos_emb , key_padding_mask = key_padding_mask , training = training ) # ( batch_size , input_seq_len , d_model ) attn_output = self . dropout1 ( attn_output , training = training ) out1 = self . layernorm1 ( src + attn_output ) # ( batch_size , input_seq_len , d_model ) ffn_output = self . ffn ( out1 ) # ( batch_size , input_seq_len , d_model ) ffn_output = self . dropout2 ( ffn_output , training = training ) out2 = self . layernorm2 ( out1 + ffn_output ) # ( batch_size , input_seq_len , d_model ) return out2","title":"call"},{"location":"reference/kerod/layers/transformer/#transformer","text":"class Transformer ( num_layers = 6 , d_model = 256 , num_heads = 8 , dim_feedforward = 2048 , dropout_rate = 0.1 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention .","title":"Transformer"},{"location":"reference/kerod/layers/transformer/#arguments_2","text":"Name Description num_layers the number of sub-layers in the decoder and the encoder. d_model The number of expected features in the encoder/decoder inputs num_heads The number of heads in the multiheadattention models. dim_feedforward The dim of the feedforward neuralnetworks in the EncoderLayer and DecoderLayer dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer.","title":"Arguments"},{"location":"reference/kerod/layers/transformer/#call-arguments_2","text":"Name Description flatten_tensor A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn)","title":"Call arguments"},{"location":"reference/kerod/layers/transformer/#call-returns_2","text":"Type Description Tuple - decoder_output : 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - encoder_output : 3-D float32 Tensor of shape [batch_size, batch_size, d_model]","title":"Call returns"},{"location":"reference/kerod/layers/transformer/#ancestors-in-mro_2","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/transformer/#methods_2","text":"","title":"Methods"},{"location":"reference/kerod/layers/transformer/#call_2","text":"def call ( self , flatten_tensor , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) Parameters: Name Description flatten_tensor A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Type Description Tuple - decoder_output : 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - encoder_output : 3-D float32 Tensor of shape [batch_size, batch_size, d_model] View Source def call ( self , flatten_tensor , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) : \" \"\" Args: flatten_tensor: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask: A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Tuple: - `decoder_output`: 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - `encoder_output`: 3-D float32 Tensor of shape [batch_size, batch_size, d_model] \"\" \" memory = flatten_tensor for enc in self . enc_layers : # (batch_size, seq_len, d_model) memory = enc ( memory , pos_embed , key_padding_mask = key_padding_mask , training = training ) # At the beginning we set target to 0 # In the first decoder layer Q and K will be equal # to dec_out + object_queries=object_queries dec_out = tf . zeros_like ( object_queries ) layers_output = [] for layer in self . dec_layers : dec_out = layer ( dec_out , memory , pos_embed , object_queries , key_padding_mask = key_padding_mask , coattn_mask = coattn_mask , training = training ) dec_out = self . layer_norm ( dec_out ) if training : layers_output . append ( dec_out ) if training : return tf . concat ( layers_output , axis = 1 ), memory return dec_out , memory","title":"call"},{"location":"reference/kerod/layers/detection/","text":"Module kerod.layers.detection None None Sub-modules kerod.layers.detection.abstract_detection_head kerod.layers.detection.fast_rcnn kerod.layers.detection.pooling_ops kerod.layers.detection.rpn","title":"Index"},{"location":"reference/kerod/layers/detection/#module-kerodlayersdetection","text":"None None","title":"Module kerod.layers.detection"},{"location":"reference/kerod/layers/detection/#sub-modules","text":"kerod.layers.detection.abstract_detection_head kerod.layers.detection.fast_rcnn kerod.layers.detection.pooling_ops kerod.layers.detection.rpn","title":"Sub-modules"},{"location":"reference/kerod/layers/detection/abstract_detection_head/","text":"Module kerod.layers.detection.abstract_detection_head None None View Source from typing import Dict import tensorflow as tf import tensorflow.keras.layers as KL from kerod.core.standard_fields import BoxField from kerod.utils.documentation import remove_unwanted_doc from tensorflow.keras import initializers __pdoc__ = {} class AbstractDetectionHead ( KL . Layer ): \"\"\"Abstract object detector. It encapsulates the main functions of an object detector. Arguments: num_classes: Number of classes of the classification head (e.g: Your n classes + the background class) classification_loss: An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. localization_loss: An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. segmentation_loss: An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. classification_loss_weight: A float 32 representing the weight of the loss in the total loss. localization_loss_weight: A float 32 representing the weight of the loss in the total loss. segmentation_loss_weight: A float 32 representing the weight of the loss in the total loss. multiples: How many time will you replicate the output of the head. For a rpn multiples can be the number of anchors. For a fast_rcnn multiples is 1 we just want the number of classes kernel_initializer_classification_head: Initializer for the `kernel` weights matrix of the classification head (see [initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)). kernel_initializer_box_prediction_head: Initializer for the `kernel` weights matrix of the box prediction head (see [initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)). kernel_regularizer: Regularizer function applied to the kernel weights matrix ([see keras.regularizers](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers)). use_mask: Boolean define if the segmentation_head will be used. \"\"\" def __init__ ( self , num_classes , classification_loss , localization_loss , segmentation_loss = None , classification_loss_weight = 1.0 , localization_loss_weight = 1.0 , segmentation_loss_weight = 1.0 , multiples = 1 , kernel_initializer_classification_head = None , kernel_initializer_box_prediction_head = None , kernel_regularizer = None , use_mask = False , ** kwargs ): super () . __init__ ( ** kwargs ) self . _num_classes = num_classes self . _classification_loss = classification_loss self . _localization_loss = localization_loss self . _classification_loss_weight = classification_loss_weight self . _localization_loss_weight = localization_loss_weight self . _multiples = multiples self . _kernel_initializer_classification_head = kernel_initializer_classification_head self . _kernel_initializer_box_prediction_head = kernel_initializer_box_prediction_head self . _kernel_regularizer = kernel_regularizer self . _use_mask = use_mask if self . _use_mask : self . _segmentation_loss_weight = segmentation_loss_weight self . _segmentation_loss = segmentation_loss def build ( self , input_shape ): self . _conv_classification_head = KL . Conv2D ( self . _multiples * self . _num_classes , ( 1 , 1 ), padding = 'valid' , activation = None , kernel_initializer = self . _kernel_initializer_classification_head , kernel_regularizer = self . _kernel_regularizer , name = f ' { self . name } classification_head' ) self . _conv_box_prediction_head = KL . Conv2D ( ( self . _num_classes - 1 ) * self . _multiples * 4 , ( 1 , 1 ), padding = 'valid' , activation = None , kernel_initializer = self . _kernel_initializer_box_prediction_head , kernel_regularizer = self . _kernel_regularizer , name = f ' { self . name } box_prediction_head' ) if self . _use_mask : self . _segmentation_layers = [ KL . Conv2D ( 256 , ( 3 , 3 ), padding = 'valid' , activation = 'relu' , kernel_initializer = initializers . VarianceScaling ( scale = 2. , mode = 'fan_out' ), kernel_regularizer = self . _kernel_regularizer ), KL . Conv2DTranspose ( 256 , ( 2 , 2 ), strides = ( 2 , 2 ), padding = 'valid' , activation = 'relu' , kernel_initializer = initializers . VarianceScaling ( scale = 2. , mode = 'fan_out' ), kernel_regularizer = self . _kernel_regularizer ), KL . Conv2D ( self . _num_classes , ( 3 , 3 ), padding = 'valid' , activation = 'relu' , kernel_initializer = initializers . VarianceScaling ( scale = 2. , mode = 'fan_out' ), kernel_regularizer = self . _kernel_regularizer ) ] super () . build ( input_shape ) def build_segmentation_head ( self , inputs ): \"\"\"Build the detection head Arguments: inputs: A tensor of float and shape [N, H, W, C] Returns: tf.Tensor: A tensor and shape [N, H*2, W*2, num_classes - 1] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x def build_detection_head ( self , inputs ): \"\"\" Build a detection head composed of a classification and box_detection. Arguments: inputs: A tensor of shape [batch_size, H, W, C] Returns: Tuple: classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head def compute_losses ( self , y_true : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ], weights : Dict [ str , tf . Tensor ]) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ): losses = loss ( tf . cast ( y_true [ target ], tf . float32 ), tf . cast ( y_pred [ target ], tf . float32 ), sample_weight = tf . cast ( weights [ target ], tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField . LABELS ], axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f ' { self . name } _classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f ' { self . name } _localization_loss' , aggregation = 'mean' ) self . add_loss ([ classification_loss , localization_loss ]) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f ' { self . name } _segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss } def get_config ( self ): base_config = super () . get_config () base_config [ 'num_classes' ] = self . _num_classes base_config [ 'classification_loss_weight' ] = self . _classification_loss_weight base_config [ 'localization_loss_weight' ] = self . _localization_loss_weight base_config [ 'multiples' ] = self . _multiples base_config [ 'use_mask' ] = self . _use_mask if self . _use_mask : base_config [ 'segmentation_loss_weight' ] = self . _segmentation_loss_weight return base_config remove_unwanted_doc ( AbstractDetectionHead , __pdoc__ ) Classes AbstractDetectionHead class AbstractDetectionHead ( num_classes , classification_loss , localization_loss , segmentation_loss = None , classification_loss_weight = 1.0 , localization_loss_weight = 1.0 , segmentation_loss_weight = 1.0 , multiples = 1 , kernel_initializer_classification_head = None , kernel_initializer_box_prediction_head = None , kernel_regularizer = None , use_mask = False , ** kwargs ) Arguments Name Description num_classes Number of classes of the classification head (e.g: Your n classes + the background class) classification_loss An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. localization_loss An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. segmentation_loss An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. classification_loss_weight A float 32 representing the weight of the loss in the total loss. localization_loss_weight A float 32 representing the weight of the loss in the total loss. segmentation_loss_weight A float 32 representing the weight of the loss in the total loss. multiples How many time will you replicate the output of the head. For a rpn multiples can be the number of anchors. For a fast_rcnn multiples is 1 we just want the number of classes kernel_initializer_classification_head Initializer for the kernel weights matrix of the classification head (see initializers ). kernel_initializer_box_prediction_head Initializer for the kernel weights matrix of the box prediction head (see initializers ). kernel_regularizer Regularizer function applied to the kernel weights matrix ( see keras.regularizers ). use_mask Boolean define if the segmentation_head will be used. Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector Methods build_detection_head def build_detection_head ( self , inputs ) Build a detection head composed of a classification and box_detection. Parameters: Name Description inputs A tensor of shape [batch_size, H, W, C] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_detection_head ( self , inputs ) : \"\"\" Build a detection head composed of a classification and box_detection. Arguments : inputs : A tensor of shape [ batch_size , H , W , C ] Returns : Tuple : classification_head : a tensor of shape [ batch_size , num_anchors , 2 ] localization_head : a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head build_segmentation_head def build_segmentation_head ( self , inputs ) Build the detection head Parameters: Name Description inputs A tensor of float and shape [N, H, W, C] Returns: Type Description tf.Tensor A tensor and shape [N, H 2, W 2, num_classes - 1] View Source def build_segmentation_head ( self , inputs ) : \"\"\" Build the detection head Arguments : inputs : A tensor of float and shape [ N , H , W , C ] Returns : tf . Tensor : A tensor and shape [ N , H * 2 , W * 2 , num_classes - 1 ] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x call def call ( self , inputs , * args , ** kwargs ) This is where the layer's logic lives. The call() method may not create state (except in its first invocation, wrapping the creation of variables or other resources in tf.init_scope() ). It is recommended to create state in __init__() , or the build() method that is called automatically before call() executes the first time. Parameters: Name Description inputs Input tensor, or dict/list/tuple of input tensors. The first positional inputs argument is subject to special rules: - inputs must be explicitly passed. A layer cannot have zero arguments, and inputs cannot be provided via the default value of a keyword argument. - NumPy array or Python scalar values in inputs get cast as tensors. - Keras mask metadata is only collected from inputs . - Layers are built ( build(input_shape) method) using shape info from inputs only. - input_spec compatibility is only checked against inputs . - Mixed precision input casting is only applied to inputs . If a layer has tensor arguments in *args or **kwargs , their casting behavior in mixed precision should be handled manually. - The SavedModel input specification is generated using inputs only. - Integration with various ecosystem packages like TFMOT, TFLite, TF.js, etc is only supported for inputs and not for tensors in positional and keyword arguments. *args Additional positional arguments. May contain tensors, although this is not recommended, for the reasons above. **kwargs Additional keyword arguments. May contain tensors, although this is not recommended, for the reasons above. The following optional keyword arguments are reserved: - training : Boolean scalar tensor of Python boolean indicating whether the call is meant for training or inference. - mask : Boolean input mask. If the layer's call() method takes a mask argument, its default value will be set to the mask generated for inputs by the previous layer (if input did come from a layer that generated a corresponding mask, i.e. if it came from a Keras layer with masking support). Returns: Type Description None A tensor or list/tuple of tensors. View Source @doc_controls.for_subclass_implementers def call ( self , inputs , * args , ** kwargs ) : # pylint: disable=unused-argument \" \"\" This is where the layer's logic lives. The `call()` method may not create state (except in its first invocation, wrapping the creation of variables or other resources in `tf.init_scope()`). It is recommended to create state in `__init__()`, or the `build()` method that is called automatically before `call()` executes the first time. Args: inputs: Input tensor, or dict/list/tuple of input tensors. The first positional `inputs` argument is subject to special rules: - `inputs` must be explicitly passed. A layer cannot have zero arguments, and `inputs` cannot be provided via the default value of a keyword argument. - NumPy array or Python scalar values in `inputs` get cast as tensors. - Keras mask metadata is only collected from `inputs`. - Layers are built (`build(input_shape)` method) using shape info from `inputs` only. - `input_spec` compatibility is only checked against `inputs`. - Mixed precision input casting is only applied to `inputs`. If a layer has tensor arguments in `*args` or `**kwargs`, their casting behavior in mixed precision should be handled manually. - The SavedModel input specification is generated using `inputs` only. - Integration with various ecosystem packages like TFMOT, TFLite, TF.js, etc is only supported for `inputs` and not for tensors in positional and keyword arguments. *args: Additional positional arguments. May contain tensors, although this is not recommended, for the reasons above. **kwargs: Additional keyword arguments. May contain tensors, although this is not recommended, for the reasons above. The following optional keyword arguments are reserved: - `training`: Boolean scalar tensor of Python boolean indicating whether the `call` is meant for training or inference. - `mask`: Boolean input mask. If the layer's `call()` method takes a `mask` argument, its default value will be set to the mask generated for `inputs` by the previous layer (if `input` did come from a layer that generated a corresponding mask, i.e. if it came from a Keras layer with masking support). Returns: A tensor or list/tuple of tensors. \"\" \" return inputs compute_losses def compute_losses ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], weights : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> dict Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Parameters: Name Description y_pred A dict of tensors of shape [N, nb_boxes, num_output]. y_true A dict of tensors of shape [N, nb_boxes, num_output]. weights A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: Type Description None dict : A dict of different losses View Source def compute_losses ( self , y_true : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , weights : Dict [ str, tf.Tensor ] ) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ) : losses = loss ( tf . cast ( y_true [ target ] , tf . float32 ), tf . cast ( y_pred [ target ] , tf . float32 ), sample_weight = tf . cast ( weights [ target ] , tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField.LABELS ] , axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f '{self.name}_classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f '{self.name}_localization_loss' , aggregation = 'mean' ) self . add_loss ( [ classification_loss, localization_loss ] ) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f '{self.name}_segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss }","title":"Abstract Detection Head"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#module-kerodlayersdetectionabstract_detection_head","text":"None None View Source from typing import Dict import tensorflow as tf import tensorflow.keras.layers as KL from kerod.core.standard_fields import BoxField from kerod.utils.documentation import remove_unwanted_doc from tensorflow.keras import initializers __pdoc__ = {} class AbstractDetectionHead ( KL . Layer ): \"\"\"Abstract object detector. It encapsulates the main functions of an object detector. Arguments: num_classes: Number of classes of the classification head (e.g: Your n classes + the background class) classification_loss: An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. localization_loss: An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. segmentation_loss: An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. classification_loss_weight: A float 32 representing the weight of the loss in the total loss. localization_loss_weight: A float 32 representing the weight of the loss in the total loss. segmentation_loss_weight: A float 32 representing the weight of the loss in the total loss. multiples: How many time will you replicate the output of the head. For a rpn multiples can be the number of anchors. For a fast_rcnn multiples is 1 we just want the number of classes kernel_initializer_classification_head: Initializer for the `kernel` weights matrix of the classification head (see [initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)). kernel_initializer_box_prediction_head: Initializer for the `kernel` weights matrix of the box prediction head (see [initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)). kernel_regularizer: Regularizer function applied to the kernel weights matrix ([see keras.regularizers](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers)). use_mask: Boolean define if the segmentation_head will be used. \"\"\" def __init__ ( self , num_classes , classification_loss , localization_loss , segmentation_loss = None , classification_loss_weight = 1.0 , localization_loss_weight = 1.0 , segmentation_loss_weight = 1.0 , multiples = 1 , kernel_initializer_classification_head = None , kernel_initializer_box_prediction_head = None , kernel_regularizer = None , use_mask = False , ** kwargs ): super () . __init__ ( ** kwargs ) self . _num_classes = num_classes self . _classification_loss = classification_loss self . _localization_loss = localization_loss self . _classification_loss_weight = classification_loss_weight self . _localization_loss_weight = localization_loss_weight self . _multiples = multiples self . _kernel_initializer_classification_head = kernel_initializer_classification_head self . _kernel_initializer_box_prediction_head = kernel_initializer_box_prediction_head self . _kernel_regularizer = kernel_regularizer self . _use_mask = use_mask if self . _use_mask : self . _segmentation_loss_weight = segmentation_loss_weight self . _segmentation_loss = segmentation_loss def build ( self , input_shape ): self . _conv_classification_head = KL . Conv2D ( self . _multiples * self . _num_classes , ( 1 , 1 ), padding = 'valid' , activation = None , kernel_initializer = self . _kernel_initializer_classification_head , kernel_regularizer = self . _kernel_regularizer , name = f ' { self . name } classification_head' ) self . _conv_box_prediction_head = KL . Conv2D ( ( self . _num_classes - 1 ) * self . _multiples * 4 , ( 1 , 1 ), padding = 'valid' , activation = None , kernel_initializer = self . _kernel_initializer_box_prediction_head , kernel_regularizer = self . _kernel_regularizer , name = f ' { self . name } box_prediction_head' ) if self . _use_mask : self . _segmentation_layers = [ KL . Conv2D ( 256 , ( 3 , 3 ), padding = 'valid' , activation = 'relu' , kernel_initializer = initializers . VarianceScaling ( scale = 2. , mode = 'fan_out' ), kernel_regularizer = self . _kernel_regularizer ), KL . Conv2DTranspose ( 256 , ( 2 , 2 ), strides = ( 2 , 2 ), padding = 'valid' , activation = 'relu' , kernel_initializer = initializers . VarianceScaling ( scale = 2. , mode = 'fan_out' ), kernel_regularizer = self . _kernel_regularizer ), KL . Conv2D ( self . _num_classes , ( 3 , 3 ), padding = 'valid' , activation = 'relu' , kernel_initializer = initializers . VarianceScaling ( scale = 2. , mode = 'fan_out' ), kernel_regularizer = self . _kernel_regularizer ) ] super () . build ( input_shape ) def build_segmentation_head ( self , inputs ): \"\"\"Build the detection head Arguments: inputs: A tensor of float and shape [N, H, W, C] Returns: tf.Tensor: A tensor and shape [N, H*2, W*2, num_classes - 1] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x def build_detection_head ( self , inputs ): \"\"\" Build a detection head composed of a classification and box_detection. Arguments: inputs: A tensor of shape [batch_size, H, W, C] Returns: Tuple: classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head def compute_losses ( self , y_true : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ], weights : Dict [ str , tf . Tensor ]) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ): losses = loss ( tf . cast ( y_true [ target ], tf . float32 ), tf . cast ( y_pred [ target ], tf . float32 ), sample_weight = tf . cast ( weights [ target ], tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField . LABELS ], axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f ' { self . name } _classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f ' { self . name } _localization_loss' , aggregation = 'mean' ) self . add_loss ([ classification_loss , localization_loss ]) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f ' { self . name } _segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss } def get_config ( self ): base_config = super () . get_config () base_config [ 'num_classes' ] = self . _num_classes base_config [ 'classification_loss_weight' ] = self . _classification_loss_weight base_config [ 'localization_loss_weight' ] = self . _localization_loss_weight base_config [ 'multiples' ] = self . _multiples base_config [ 'use_mask' ] = self . _use_mask if self . _use_mask : base_config [ 'segmentation_loss_weight' ] = self . _segmentation_loss_weight return base_config remove_unwanted_doc ( AbstractDetectionHead , __pdoc__ )","title":"Module kerod.layers.detection.abstract_detection_head"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#abstractdetectionhead","text":"class AbstractDetectionHead ( num_classes , classification_loss , localization_loss , segmentation_loss = None , classification_loss_weight = 1.0 , localization_loss_weight = 1.0 , segmentation_loss_weight = 1.0 , multiples = 1 , kernel_initializer_classification_head = None , kernel_initializer_box_prediction_head = None , kernel_regularizer = None , use_mask = False , ** kwargs )","title":"AbstractDetectionHead"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#arguments","text":"Name Description num_classes Number of classes of the classification head (e.g: Your n classes + the background class) classification_loss An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. localization_loss An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. segmentation_loss An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. classification_loss_weight A float 32 representing the weight of the loss in the total loss. localization_loss_weight A float 32 representing the weight of the loss in the total loss. segmentation_loss_weight A float 32 representing the weight of the loss in the total loss. multiples How many time will you replicate the output of the head. For a rpn multiples can be the number of anchors. For a fast_rcnn multiples is 1 we just want the number of classes kernel_initializer_classification_head Initializer for the kernel weights matrix of the classification head (see initializers ). kernel_initializer_box_prediction_head Initializer for the kernel weights matrix of the box prediction head (see initializers ). kernel_regularizer Regularizer function applied to the kernel weights matrix ( see keras.regularizers ). use_mask Boolean define if the segmentation_head will be used.","title":"Arguments"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#ancestors-in-mro","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#build_detection_head","text":"def build_detection_head ( self , inputs ) Build a detection head composed of a classification and box_detection. Parameters: Name Description inputs A tensor of shape [batch_size, H, W, C] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_detection_head ( self , inputs ) : \"\"\" Build a detection head composed of a classification and box_detection. Arguments : inputs : A tensor of shape [ batch_size , H , W , C ] Returns : Tuple : classification_head : a tensor of shape [ batch_size , num_anchors , 2 ] localization_head : a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head","title":"build_detection_head"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#build_segmentation_head","text":"def build_segmentation_head ( self , inputs ) Build the detection head Parameters: Name Description inputs A tensor of float and shape [N, H, W, C] Returns: Type Description tf.Tensor A tensor and shape [N, H 2, W 2, num_classes - 1] View Source def build_segmentation_head ( self , inputs ) : \"\"\" Build the detection head Arguments : inputs : A tensor of float and shape [ N , H , W , C ] Returns : tf . Tensor : A tensor and shape [ N , H * 2 , W * 2 , num_classes - 1 ] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x","title":"build_segmentation_head"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#call","text":"def call ( self , inputs , * args , ** kwargs ) This is where the layer's logic lives. The call() method may not create state (except in its first invocation, wrapping the creation of variables or other resources in tf.init_scope() ). It is recommended to create state in __init__() , or the build() method that is called automatically before call() executes the first time. Parameters: Name Description inputs Input tensor, or dict/list/tuple of input tensors. The first positional inputs argument is subject to special rules: - inputs must be explicitly passed. A layer cannot have zero arguments, and inputs cannot be provided via the default value of a keyword argument. - NumPy array or Python scalar values in inputs get cast as tensors. - Keras mask metadata is only collected from inputs . - Layers are built ( build(input_shape) method) using shape info from inputs only. - input_spec compatibility is only checked against inputs . - Mixed precision input casting is only applied to inputs . If a layer has tensor arguments in *args or **kwargs , their casting behavior in mixed precision should be handled manually. - The SavedModel input specification is generated using inputs only. - Integration with various ecosystem packages like TFMOT, TFLite, TF.js, etc is only supported for inputs and not for tensors in positional and keyword arguments. *args Additional positional arguments. May contain tensors, although this is not recommended, for the reasons above. **kwargs Additional keyword arguments. May contain tensors, although this is not recommended, for the reasons above. The following optional keyword arguments are reserved: - training : Boolean scalar tensor of Python boolean indicating whether the call is meant for training or inference. - mask : Boolean input mask. If the layer's call() method takes a mask argument, its default value will be set to the mask generated for inputs by the previous layer (if input did come from a layer that generated a corresponding mask, i.e. if it came from a Keras layer with masking support). Returns: Type Description None A tensor or list/tuple of tensors. View Source @doc_controls.for_subclass_implementers def call ( self , inputs , * args , ** kwargs ) : # pylint: disable=unused-argument \" \"\" This is where the layer's logic lives. The `call()` method may not create state (except in its first invocation, wrapping the creation of variables or other resources in `tf.init_scope()`). It is recommended to create state in `__init__()`, or the `build()` method that is called automatically before `call()` executes the first time. Args: inputs: Input tensor, or dict/list/tuple of input tensors. The first positional `inputs` argument is subject to special rules: - `inputs` must be explicitly passed. A layer cannot have zero arguments, and `inputs` cannot be provided via the default value of a keyword argument. - NumPy array or Python scalar values in `inputs` get cast as tensors. - Keras mask metadata is only collected from `inputs`. - Layers are built (`build(input_shape)` method) using shape info from `inputs` only. - `input_spec` compatibility is only checked against `inputs`. - Mixed precision input casting is only applied to `inputs`. If a layer has tensor arguments in `*args` or `**kwargs`, their casting behavior in mixed precision should be handled manually. - The SavedModel input specification is generated using `inputs` only. - Integration with various ecosystem packages like TFMOT, TFLite, TF.js, etc is only supported for `inputs` and not for tensors in positional and keyword arguments. *args: Additional positional arguments. May contain tensors, although this is not recommended, for the reasons above. **kwargs: Additional keyword arguments. May contain tensors, although this is not recommended, for the reasons above. The following optional keyword arguments are reserved: - `training`: Boolean scalar tensor of Python boolean indicating whether the `call` is meant for training or inference. - `mask`: Boolean input mask. If the layer's `call()` method takes a `mask` argument, its default value will be set to the mask generated for `inputs` by the previous layer (if `input` did come from a layer that generated a corresponding mask, i.e. if it came from a Keras layer with masking support). Returns: A tensor or list/tuple of tensors. \"\" \" return inputs","title":"call"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#compute_losses","text":"def compute_losses ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], weights : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> dict Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Parameters: Name Description y_pred A dict of tensors of shape [N, nb_boxes, num_output]. y_true A dict of tensors of shape [N, nb_boxes, num_output]. weights A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: Type Description None dict : A dict of different losses View Source def compute_losses ( self , y_true : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , weights : Dict [ str, tf.Tensor ] ) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ) : losses = loss ( tf . cast ( y_true [ target ] , tf . float32 ), tf . cast ( y_pred [ target ] , tf . float32 ), sample_weight = tf . cast ( weights [ target ] , tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField.LABELS ] , axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f '{self.name}_classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f '{self.name}_localization_loss' , aggregation = 'mean' ) self . add_loss ( [ classification_loss, localization_loss ] ) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f '{self.name}_segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss }","title":"compute_losses"},{"location":"reference/kerod/layers/detection/fast_rcnn/","text":"Module kerod.layers.detection.fast_rcnn None None View Source import functools from typing import Dict import tensorflow as tf import tensorflow . keras . layers as KL from tensorflow . keras import initializers from tensorflow . keras . losses import SparseCategoricalCrossentropy from kerod . core . box_coder import encode_boxes_faster_rcnn from kerod . core . losses import L1Loss from kerod . core . matcher import Matcher from kerod . core . sampling_ops import batch_sample_balanced_positive_negative from kerod . core . similarity import IoUSimilarity from kerod . core . standard_fields import BoxField from kerod . core . target_assigner import TargetAssigner from kerod . layers . detection . abstract_detection_head import AbstractDetectionHead from kerod . layers . detection . pooling_ops import multilevel_roi_align from kerod . utils . documentation import remove_unwanted_doc __ pdoc__ = {} class FastRCNN ( AbstractDetectionHead ) : \"\"\"Build the Fast-RCNN on top of the FPN. The parameters used are from [Feature Pyramidal Networks for Object Detection](https://arxiv.org/abs/1612.03144). Arguments: num_classes: The number of classes that predict the classification head (N+1) where N is the number of classes of your dataset and 1 is the background. Call arguments: inputs: A Tuple 1. `pyramid`: A List of tensors the output of the pyramid 2. `anchors`: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Call returns: Tuple: `classification_pred`: A logit Tensor of shape [batch_size, num_boxes, num_classes] `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" def __ init__ ( self , num_classes , **kwargs ) : super (). __ init__ ( num_classes , SparseCategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE , from_logits = True ), L1Loss ( reduction = tf . keras . losses . Reduction . NONE ), # like in tensorpack kernel_initializer_classification_head = initializers . RandomNormal ( stddev = 0.01 ), kernel_initializer_box_prediction_head = initializers . RandomNormal ( stddev = 0.001 ), **kwargs ) matcher = Matcher ([ 0.5 ], [ 0 , 1 ]) # The same scale_factors is used in decoding as well encode = functools . partial ( encode_boxes_faster_rcnn , scale_factors= ( 10.0 , 10.0 , 5.0 , 5.0 )) self . target_assigner = TargetAssigner ( IoUSimilarity (), matcher , encode , dtype = self . _ compute_dtype ) def build ( self , input_shape ) : self . denses = [ KL . Dense ( 1024 , kernel_initializer = initializers . VarianceScaling (), kernel_regularizer = self . _ kernel_regularizer , activation='relu' ) for _ in range ( 2 ) ] super (). build ( input_shape ) def call ( self , inputs ) : \"\"\"Build the computational graph of the fast RCNN HEAD. It performs a raw prediction of the FastRCNN head you can post_process them using: ```python from kerod.layers.post_processing import post_process_fast_rcnn_boxes outputs = post_process_fast_rcnn_boxes(classification_pred, localization_pred, anchors, images_information, num_classes) ``` where `images_information` is provided as input of your model and `num_classes` includes the background. Arguments: inputs: A Tuple 1. `pyramid`: A List of tensors the output of the pyramid 2. `anchors`: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Returns: Tuple: `classification_pred`: A logit Tensor of shape [batch_size, num_boxes, num_classes] `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" # Remove P6 pyramid = inputs [ 0 ][:- 1 ] anchors = inputs [ 1 ] # We can compute the original image shape regarding # TODO compute it more automatically without knowing that the last layer is stride 32 image_shape = tf . cast ( tf . shape ( pyramid [ - 1 ])[ 1 : 3 ] * 32 , dtype = self . _ compute_dtype ) boxe_tensors = multilevel_roi_align ( pyramid , anchors , image_shape , crop_size = 7 ) l = KL . Flatten ()( boxe_tensors ) for dense in self . denses : l = dense ( l ) classification_pred , localization_pred = self . build_detection_head ( tf . reshape ( l , ( - 1 , 1 , 1 , 1024 ))) batch_size = tf . shape ( anchors )[ 0 ] classification_pred = tf . reshape ( classification_pred , ( batch_size , - 1 , self . _ num_classes )) localization_pred = tf . reshape ( localization_pred , ( batch_size , - 1 , ( self . _ num_classes - 1 ) * 4 )) return classification_pred , localization_pred def sample_boxes ( self , anchors : tf . Tensor , ground_truths: Dict [ str , tf . Tensor ], sampling_size: int = 512 , sampling_positive_ratio: float = 0.25 ) : \"\"\"Perform the sampling of the target anchors. During the training a set of RoIs is detected by the RPN. However, you do not want to analyse all the set. You only want to analyse the anchors that you sampled with this method. Arguments: anchors: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 sampling_size: Desired sampling size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. sampling_positive_ratio: Desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: Tuple: 1. y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, box_code_dimension] 2. weights: A dict with: - `BoxField.LABELS`: A 2-D tensor of shape [batch_size, num_anchors], - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] Raises: ValueError: If the batch_size is None. ValueError: If the batch_size between your ground_truths and the anchors does not match. \"\"\" ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField . LABELS ] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField . BOXES ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } y_true , weights = self . target_assigner . assign ({ BoxField . BOXES : anchors }, ground_truths ) labels = y_true [ BoxField . LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField . LABELS ], sampling_size , labels , positive_fraction = sampling_positive_ratio , dtype = self . _ compute_dtype ) weights [ BoxField . LABELS ] = sample_idx * weights [ BoxField . LABELS ] weights [ BoxField . BOXES ] = sample_idx * weights [ BoxField . BOXES ] selected_boxes_idx = tf . where ( sample_idx == 1 ) batch_size = tf . shape ( sample_idx )[ 0 ] # Extract the selected anchors corresponding anchors # tf . gather_nd collaps the batch_together so we reshape with the proper batch_size anchors = tf . reshape ( tf . gather_nd ( anchors , selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField . BOXES ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField . BOXES ], selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField . LABELS ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField . LABELS ], selected_boxes_idx ), ( batch_size , - 1 )) for key in y_true . keys () : weights [ key ] = tf . reshape ( tf . gather_nd ( weights [ key ], selected_boxes_idx ), ( batch_size , - 1 )) weights [ key ] = tf . stop_gradient ( weights [ key ]) y_true [ key ] = tf . stop_gradient ( y_true [ key ]) return y_true , weights , anchors def compute_loss ( self , y_true: dict , weights : dict , classification_pred: tf . Tensor , localization_pred: tf . Tensor ) : \"\"\"Compute the loss of the FastRCNN Arguments: y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, 4] weights: A dict with: - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] classification_pred: A 3-D tensor of float and shape [batch_size, num_anchors, num_classes] localization_pred: A 3-D tensor of float and shape [batch_size, num_anchors, (num_classes - 1) * 4] Returns: Tuple: - `classification_loss`: A scalar - `localization_loss`: A scalar \"\"\" y_true_classification = tf . cast ( y_true [ BoxField . LABELS ], tf . int32 ) accuracy , fg_accuracy , false_negative = compute_fast_rcnn_metrics ( y_true_classification , classification_pred ) self . add_metric ( accuracy , name='accuracy' , aggregation='mean' ) self . add_metric ( fg_accuracy , name='fg_accuracy' , aggregation='mean' ) self . add_metric ( false_negative , name='false_negative' , aggregation='mean' ) # y_true [ BoxField . LABELS ] is just 1 and 0 we are using it as mask to extract # the corresponding target anchors batch_size = tf . shape ( classification_pred )[ 0 ] # We create a boolean mask to extract the desired localization prediction to compute # the loss one_hot_targets = tf . one_hot ( y_true_classification , self . _ num_classes , dtype = tf . int8 ) one_hot_targets = tf . reshape ( one_hot_targets , [ - 1 ]) # We need to insert a fake background classes at the position 0 localization_pred = tf . pad ( localization_pred , [[ 0 , 0 ], [ 0 , 0 ], [ 4 , 0 ]]) localization_pred = tf . reshape ( localization_pred , [ - 1 , 4 ]) extracted_localization_pred = tf . boolean_mask ( localization_pred , one_hot_targets > 0 ) extracted_localization_pred = tf . reshape ( extracted_localization_pred , ( batch_size , - 1 , 4 )) y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : extracted_localization_pred } return self . compute_losses ( y_true , y_pred , weights ) def compute_fast_rcnn_metrics ( y_true: tf . Tensor , y_pred: tf . Tensor ) : \"\"\"Useful metrics that allows to track how behave the training of the fast rcnn head. `Warning`: This function should be used if the ground_truths have been added to the RoIs. It won't work if the there are no foreground ground_truths in the sample_boxes which isn't possible if they have been added. Arguments: y_true: A one-hot encoded vector with shape [batch_size, num_sample_anchors, num_classes] y_pred: A tensor with shape [batch_size, num_sample_anchors, num_classes], representing the classification logits. Returns: Tuple: 1. `accuracy`: A scalar tensor representing the accuracy with the background classes included 2. `fg_accuracy`: A scalar tensor representing the accuracy without the background classes included 3. `false_negative`: A scalar tensor representing the ratio of boxes predicted as background instead of their respective class among the foreground example to predict. \"\"\" # compute usefull metrics # Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis=- 1 , name='label_prediction' , output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # The accuracy allows to determine if the models perform well ( background included ) accuracy = tf . reduce_mean ( correct , name='accuracy' ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) num_fg = tf . shape ( fg_inds )[ 0 ] fg_label_pred = tf . argmax ( tf . gather_nd ( y_pred , fg_inds ), axis=- 1 ) num_zero = tf . reduce_sum ( tf . cast ( tf . equal ( fg_label_pred , 0 ), tf . int32 ), name='num_zero' ) # Number of example predicted as background instead of one of our classes false_negative = tf . cast ( tf . truediv ( num_zero , num_fg ), tf . float32 , name='false_negative' ) fg_accuracy = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name='fg_accuracy' ) return accuracy , fg_accuracy , false_negative remove_unwanted_doc ( FastRCNN , __ pdoc__ ) Functions compute_fast_rcnn_metrics def compute_fast_rcnn_metrics ( y_true : tensorflow . python . framework . ops . Tensor , y_pred : tensorflow . python . framework . ops . Tensor ) Useful metrics that allows to track how behave the training of the fast rcnn head. Warning : This function should be used if the ground_truths have been added to the RoIs. It won't work if the there are no foreground ground_truths in the sample_boxes which isn't possible if they have been added. Parameters: Name Description y_true A one-hot encoded vector with shape [batch_size, num_sample_anchors, num_classes] y_pred A tensor with shape [batch_size, num_sample_anchors, num_classes], representing the classification logits. Returns: Type Description Tuple 1. accuracy : A scalar tensor representing the accuracy with the background classes included 2. fg_accuracy : A scalar tensor representing the accuracy without the background classes included 3. false_negative : A scalar tensor representing the ratio of boxes predicted as background instead of their respective class among the foreground example to predict. View Source def compute_fast_rcnn_metrics ( y_true : tf . Tensor , y_pred : tf . Tensor ) : \" \"\" Useful metrics that allows to track how behave the training of the fast rcnn head. `Warning`: This function should be used if the ground_truths have been added to the RoIs. It won't work if the there are no foreground ground_truths in the sample_boxes which isn't possible if they have been added. Arguments: y_true: A one-hot encoded vector with shape [batch_size, num_sample_anchors, num_classes] y_pred: A tensor with shape [batch_size, num_sample_anchors, num_classes], representing the classification logits. Returns: Tuple: 1. `accuracy`: A scalar tensor representing the accuracy with the background classes included 2. `fg_accuracy`: A scalar tensor representing the accuracy without the background classes included 3. `false_negative`: A scalar tensor representing the ratio of boxes predicted as background instead of their respective class among the foreground example to predict. \"\" \" # compute usefull metrics #Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis =- 1 , name = 'label_prediction' , output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # The accuracy allows to determine if the models perform well (background included) accuracy = tf . reduce_mean ( correct , name = 'accuracy' ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) num_fg = tf . shape ( fg_inds ) [ 0 ] fg_label_pred = tf . argmax ( tf . gather_nd ( y_pred , fg_inds ), axis =- 1 ) num_zero = tf . reduce_sum ( tf . cast ( tf . equal ( fg_label_pred , 0 ), tf . int32 ), name = 'num_zero' ) # Number of example predicted as background instead of one of our classes false_negative = tf . cast ( tf . truediv ( num_zero , num_fg ), tf . float32 , name = 'false_negative' ) fg_accuracy = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name = 'fg_accuracy' ) return accuracy , fg_accuracy , false_negative Classes FastRCNN class FastRCNN ( num_classes , ** kwargs ) are from Feature Pyramidal Networks for Object Detection . Arguments Name Description num_classes The number of classes that predict the classification head (N+1) where N is the number of classes of your dataset and 1 is the background. Call arguments Name Description inputs A Tuple 1. pyramid : A List of tensors the output of the pyramid 2. anchors : A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Call returns Type Description Tuple classification_pred : A logit Tensor of shape [batch_size, num_boxes, num_classes] localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] anchors : A Tensor of shape [batch_size, num_boxes, 4] Ancestors (in MRO) kerod.layers.detection.abstract_detection_head.AbstractDetectionHead keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector Methods build_detection_head def build_detection_head ( self , inputs ) Build a detection head composed of a classification and box_detection. Parameters: Name Description inputs A tensor of shape [batch_size, H, W, C] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_detection_head ( self , inputs ) : \"\"\" Build a detection head composed of a classification and box_detection. Arguments : inputs : A tensor of shape [ batch_size , H , W , C ] Returns : Tuple : classification_head : a tensor of shape [ batch_size , num_anchors , 2 ] localization_head : a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head build_segmentation_head def build_segmentation_head ( self , inputs ) Build the detection head Parameters: Name Description inputs A tensor of float and shape [N, H, W, C] Returns: Type Description tf.Tensor A tensor and shape [N, H 2, W 2, num_classes - 1] View Source def build_segmentation_head ( self , inputs ) : \"\"\" Build the detection head Arguments : inputs : A tensor of float and shape [ N , H , W , C ] Returns : tf . Tensor : A tensor and shape [ N , H * 2 , W * 2 , num_classes - 1 ] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x call def call ( self , inputs ) Build the computational graph of the fast RCNN HEAD. It performs a raw prediction of the FastRCNN head you can post_process them using: from kerod.layers.post_processing import post_process_fast_rcnn_boxes outputs = post_process_fast_rcnn_boxes ( classification_pred , localization_pred , anchors , images_information , num_classes ) where images_information is provided as input of your model and num_classes includes the background. Parameters: Name Description inputs A Tuple 1. pyramid : A List of tensors the output of the pyramid 2. anchors : A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Returns: Type Description Tuple classification_pred : A logit Tensor of shape [batch_size, num_boxes, num_classes] localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs ) : \"\"\"Build the computational graph of the fast RCNN HEAD. It performs a raw prediction of the FastRCNN head you can post_process them using: ```python from kerod.layers.post_processing import post_process_fast_rcnn_boxes outputs = post_process_fast_rcnn_boxes(classification_pred, localization_pred, anchors, images_information, num_classes) ``` where `images_information` is provided as input of your model and `num_classes` includes the background. Arguments: inputs: A Tuple 1. `pyramid`: A List of tensors the output of the pyramid 2. `anchors`: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Returns: Tuple: `classification_pred`: A logit Tensor of shape [batch_size, num_boxes, num_classes] `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" # Remove P6 pyramid = inputs [ 0 ][:- 1 ] anchors = inputs [ 1 ] # We can compute the original image shape regarding # TODO compute it more automatically without knowing that the last layer is stride 32 image_shape = tf . cast ( tf . shape ( pyramid [ - 1 ])[ 1 : 3 ] * 32 , dtype = self . _ compute_dtype ) boxe_tensors = multilevel_roi_align ( pyramid , anchors , image_shape , crop_size = 7 ) l = KL . Flatten ()( boxe_tensors ) for dense in self . denses : l = dense ( l ) classification_pred , localization_pred = self . build_detection_head ( tf . reshape ( l , ( - 1 , 1 , 1 , 1024 ))) batch_size = tf . shape ( anchors )[ 0 ] classification_pred = tf . reshape ( classification_pred , ( batch_size , - 1 , self . _ num_classes )) localization_pred = tf . reshape ( localization_pred , ( batch_size , - 1 , ( self . _ num_classes - 1 ) * 4 )) return classification_pred , localization_pred compute_loss def compute_loss ( self , y_true : dict , weights : dict , classification_pred : tensorflow . python . framework . ops . Tensor , localization_pred : tensorflow . python . framework . ops . Tensor ) Compute the loss of the FastRCNN Parameters: Name Description y_true A dict with : - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_anchors, num_classes] - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_anchors, 4] weights A dict with: - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_anchors, num_classes] - BoxField.BOXES : A 2-D tensor of shape [batch_size, num_anchors] classification_pred A 3-D tensor of float and shape [batch_size, num_anchors, num_classes] localization_pred A 3-D tensor of float and shape [batch_size, num_anchors, (num_classes - 1) * 4] Returns: Type Description Tuple - classification_loss : A scalar - localization_loss : A scalar View Source def compute_loss ( self , y_true : dict , weights : dict , classification_pred : tf . Tensor , localization_pred : tf . Tensor ) : \" \"\" Compute the loss of the FastRCNN Arguments: y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, 4] weights: A dict with: - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] classification_pred: A 3-D tensor of float and shape [batch_size, num_anchors, num_classes] localization_pred: A 3-D tensor of float and shape [batch_size, num_anchors, (num_classes - 1) * 4] Returns: Tuple: - `classification_loss`: A scalar - `localization_loss`: A scalar \"\" \" y_true_classification = tf . cast ( y_true [ BoxField . LABELS ] , tf . int32 ) accuracy , fg_accuracy , false_negative = compute_fast_rcnn_metrics ( y_true_classification , classification_pred ) self . add_metric ( accuracy , name = 'accuracy' , aggregation = 'mean' ) self . add_metric ( fg_accuracy , name = 'fg_accuracy' , aggregation = 'mean' ) self . add_metric ( false_negative , name = 'false_negative' , aggregation = 'mean' ) # y_true[BoxField.LABELS] is just 1 and 0 we are using it as mask to extract # the corresponding target anchors batch_size = tf . shape ( classification_pred ) [ 0 ] # We create a boolean mask to extract the desired localization prediction to compute # the loss one_hot_targets = tf . one_hot ( y_true_classification , self . _num_classes , dtype = tf . int8 ) one_hot_targets = tf . reshape ( one_hot_targets , [ - 1 ] ) # We need to insert a fake background classes at the position 0 localization_pred = tf . pad ( localization_pred , [[ 0 , 0 ] , [ 0 , 0 ] , [ 4 , 0 ]] ) localization_pred = tf . reshape ( localization_pred , [ - 1 , 4 ] ) extracted_localization_pred = tf . boolean_mask ( localization_pred , one_hot_targets > 0 ) extracted_localization_pred = tf . reshape ( extracted_localization_pred , ( batch_size , - 1 , 4 )) y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : extracted_localization_pred } return self . compute_losses ( y_true , y_pred , weights ) compute_losses def compute_losses ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], weights : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> dict Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Parameters: Name Description y_pred A dict of tensors of shape [N, nb_boxes, num_output]. y_true A dict of tensors of shape [N, nb_boxes, num_output]. weights A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: Type Description None dict : A dict of different losses View Source def compute_losses ( self , y_true : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , weights : Dict [ str, tf.Tensor ] ) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ) : losses = loss ( tf . cast ( y_true [ target ] , tf . float32 ), tf . cast ( y_pred [ target ] , tf . float32 ), sample_weight = tf . cast ( weights [ target ] , tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField.LABELS ] , axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f '{self.name}_classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f '{self.name}_localization_loss' , aggregation = 'mean' ) self . add_loss ( [ classification_loss, localization_loss ] ) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f '{self.name}_segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss } sample_boxes def sample_boxes ( self , anchors : tensorflow . python . framework . ops . Tensor , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], sampling_size : int = 512 , sampling_positive_ratio : float = 0.25 ) Perform the sampling of the target anchors. During the training a set of RoIs is detected by the RPN. However, you do not want to analyse all the set. You only want to analyse the anchors that you sampled with this method. Parameters: Name Description anchors A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] ground_truths A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 sampling_size Desired sampling size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. sampling_positive_ratio Desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: Type Description Tuple 1. y_true: A dict with : - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_anchors, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_anchors, box_code_dimension] 2. weights: A dict with: - BoxField.LABELS : A 2-D tensor of shape [batch_size, num_anchors], - BoxField.BOXES : A 2-D tensor of shape [batch_size, num_anchors] Raises: Type Description ValueError If the batch_size is None. ValueError If the batch_size between your ground_truths and the anchors does not match. View Source def sample_boxes ( self , anchors : tf . Tensor , ground_truths : Dict [ str, tf.Tensor ] , sampling_size : int = 512 , sampling_positive_ratio : float = 0.25 ) : \"\"\"Perform the sampling of the target anchors. During the training a set of RoIs is detected by the RPN. However, you do not want to analyse all the set. You only want to analyse the anchors that you sampled with this method. Arguments: anchors: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 sampling_size: Desired sampling size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. sampling_positive_ratio: Desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: Tuple: 1. y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, box_code_dimension] 2. weights: A dict with: - `BoxField.LABELS`: A 2-D tensor of shape [batch_size, num_anchors], - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] Raises: ValueError: If the batch_size is None. ValueError: If the batch_size between your ground_truths and the anchors does not match. \"\"\" ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField.BOXES ] , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } y_true , weights = self . target_assigner . assign ( { BoxField . BOXES : anchors } , ground_truths ) labels = y_true [ BoxField.LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField.LABELS ] , sampling_size , labels , positive_fraction = sampling_positive_ratio , dtype = self . _compute_dtype ) weights [ BoxField.LABELS ] = sample_idx * weights [ BoxField.LABELS ] weights [ BoxField.BOXES ] = sample_idx * weights [ BoxField.BOXES ] selected_boxes_idx = tf . where ( sample_idx == 1 ) batch_size = tf . shape ( sample_idx ) [ 0 ] # Extract the selected anchors corresponding anchors # tf . gather_nd collaps the batch_together so we reshape with the proper batch_size anchors = tf . reshape ( tf . gather_nd ( anchors , selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField.BOXES ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField.BOXES ] , selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField.LABELS ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField.LABELS ] , selected_boxes_idx ), ( batch_size , - 1 )) for key in y_true . keys () : weights [ key ] = tf . reshape ( tf . gather_nd ( weights [ key ] , selected_boxes_idx ), ( batch_size , - 1 )) weights [ key ] = tf . stop_gradient ( weights [ key ] ) y_true [ key ] = tf . stop_gradient ( y_true [ key ] ) return y_true , weights , anchors","title":"Fast Rcnn"},{"location":"reference/kerod/layers/detection/fast_rcnn/#module-kerodlayersdetectionfast_rcnn","text":"None None View Source import functools from typing import Dict import tensorflow as tf import tensorflow . keras . layers as KL from tensorflow . keras import initializers from tensorflow . keras . losses import SparseCategoricalCrossentropy from kerod . core . box_coder import encode_boxes_faster_rcnn from kerod . core . losses import L1Loss from kerod . core . matcher import Matcher from kerod . core . sampling_ops import batch_sample_balanced_positive_negative from kerod . core . similarity import IoUSimilarity from kerod . core . standard_fields import BoxField from kerod . core . target_assigner import TargetAssigner from kerod . layers . detection . abstract_detection_head import AbstractDetectionHead from kerod . layers . detection . pooling_ops import multilevel_roi_align from kerod . utils . documentation import remove_unwanted_doc __ pdoc__ = {} class FastRCNN ( AbstractDetectionHead ) : \"\"\"Build the Fast-RCNN on top of the FPN. The parameters used are from [Feature Pyramidal Networks for Object Detection](https://arxiv.org/abs/1612.03144). Arguments: num_classes: The number of classes that predict the classification head (N+1) where N is the number of classes of your dataset and 1 is the background. Call arguments: inputs: A Tuple 1. `pyramid`: A List of tensors the output of the pyramid 2. `anchors`: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Call returns: Tuple: `classification_pred`: A logit Tensor of shape [batch_size, num_boxes, num_classes] `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" def __ init__ ( self , num_classes , **kwargs ) : super (). __ init__ ( num_classes , SparseCategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE , from_logits = True ), L1Loss ( reduction = tf . keras . losses . Reduction . NONE ), # like in tensorpack kernel_initializer_classification_head = initializers . RandomNormal ( stddev = 0.01 ), kernel_initializer_box_prediction_head = initializers . RandomNormal ( stddev = 0.001 ), **kwargs ) matcher = Matcher ([ 0.5 ], [ 0 , 1 ]) # The same scale_factors is used in decoding as well encode = functools . partial ( encode_boxes_faster_rcnn , scale_factors= ( 10.0 , 10.0 , 5.0 , 5.0 )) self . target_assigner = TargetAssigner ( IoUSimilarity (), matcher , encode , dtype = self . _ compute_dtype ) def build ( self , input_shape ) : self . denses = [ KL . Dense ( 1024 , kernel_initializer = initializers . VarianceScaling (), kernel_regularizer = self . _ kernel_regularizer , activation='relu' ) for _ in range ( 2 ) ] super (). build ( input_shape ) def call ( self , inputs ) : \"\"\"Build the computational graph of the fast RCNN HEAD. It performs a raw prediction of the FastRCNN head you can post_process them using: ```python from kerod.layers.post_processing import post_process_fast_rcnn_boxes outputs = post_process_fast_rcnn_boxes(classification_pred, localization_pred, anchors, images_information, num_classes) ``` where `images_information` is provided as input of your model and `num_classes` includes the background. Arguments: inputs: A Tuple 1. `pyramid`: A List of tensors the output of the pyramid 2. `anchors`: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Returns: Tuple: `classification_pred`: A logit Tensor of shape [batch_size, num_boxes, num_classes] `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" # Remove P6 pyramid = inputs [ 0 ][:- 1 ] anchors = inputs [ 1 ] # We can compute the original image shape regarding # TODO compute it more automatically without knowing that the last layer is stride 32 image_shape = tf . cast ( tf . shape ( pyramid [ - 1 ])[ 1 : 3 ] * 32 , dtype = self . _ compute_dtype ) boxe_tensors = multilevel_roi_align ( pyramid , anchors , image_shape , crop_size = 7 ) l = KL . Flatten ()( boxe_tensors ) for dense in self . denses : l = dense ( l ) classification_pred , localization_pred = self . build_detection_head ( tf . reshape ( l , ( - 1 , 1 , 1 , 1024 ))) batch_size = tf . shape ( anchors )[ 0 ] classification_pred = tf . reshape ( classification_pred , ( batch_size , - 1 , self . _ num_classes )) localization_pred = tf . reshape ( localization_pred , ( batch_size , - 1 , ( self . _ num_classes - 1 ) * 4 )) return classification_pred , localization_pred def sample_boxes ( self , anchors : tf . Tensor , ground_truths: Dict [ str , tf . Tensor ], sampling_size: int = 512 , sampling_positive_ratio: float = 0.25 ) : \"\"\"Perform the sampling of the target anchors. During the training a set of RoIs is detected by the RPN. However, you do not want to analyse all the set. You only want to analyse the anchors that you sampled with this method. Arguments: anchors: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 sampling_size: Desired sampling size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. sampling_positive_ratio: Desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: Tuple: 1. y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, box_code_dimension] 2. weights: A dict with: - `BoxField.LABELS`: A 2-D tensor of shape [batch_size, num_anchors], - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] Raises: ValueError: If the batch_size is None. ValueError: If the batch_size between your ground_truths and the anchors does not match. \"\"\" ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField . LABELS ] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField . BOXES ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } y_true , weights = self . target_assigner . assign ({ BoxField . BOXES : anchors }, ground_truths ) labels = y_true [ BoxField . LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField . LABELS ], sampling_size , labels , positive_fraction = sampling_positive_ratio , dtype = self . _ compute_dtype ) weights [ BoxField . LABELS ] = sample_idx * weights [ BoxField . LABELS ] weights [ BoxField . BOXES ] = sample_idx * weights [ BoxField . BOXES ] selected_boxes_idx = tf . where ( sample_idx == 1 ) batch_size = tf . shape ( sample_idx )[ 0 ] # Extract the selected anchors corresponding anchors # tf . gather_nd collaps the batch_together so we reshape with the proper batch_size anchors = tf . reshape ( tf . gather_nd ( anchors , selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField . BOXES ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField . BOXES ], selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField . LABELS ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField . LABELS ], selected_boxes_idx ), ( batch_size , - 1 )) for key in y_true . keys () : weights [ key ] = tf . reshape ( tf . gather_nd ( weights [ key ], selected_boxes_idx ), ( batch_size , - 1 )) weights [ key ] = tf . stop_gradient ( weights [ key ]) y_true [ key ] = tf . stop_gradient ( y_true [ key ]) return y_true , weights , anchors def compute_loss ( self , y_true: dict , weights : dict , classification_pred: tf . Tensor , localization_pred: tf . Tensor ) : \"\"\"Compute the loss of the FastRCNN Arguments: y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, 4] weights: A dict with: - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] classification_pred: A 3-D tensor of float and shape [batch_size, num_anchors, num_classes] localization_pred: A 3-D tensor of float and shape [batch_size, num_anchors, (num_classes - 1) * 4] Returns: Tuple: - `classification_loss`: A scalar - `localization_loss`: A scalar \"\"\" y_true_classification = tf . cast ( y_true [ BoxField . LABELS ], tf . int32 ) accuracy , fg_accuracy , false_negative = compute_fast_rcnn_metrics ( y_true_classification , classification_pred ) self . add_metric ( accuracy , name='accuracy' , aggregation='mean' ) self . add_metric ( fg_accuracy , name='fg_accuracy' , aggregation='mean' ) self . add_metric ( false_negative , name='false_negative' , aggregation='mean' ) # y_true [ BoxField . LABELS ] is just 1 and 0 we are using it as mask to extract # the corresponding target anchors batch_size = tf . shape ( classification_pred )[ 0 ] # We create a boolean mask to extract the desired localization prediction to compute # the loss one_hot_targets = tf . one_hot ( y_true_classification , self . _ num_classes , dtype = tf . int8 ) one_hot_targets = tf . reshape ( one_hot_targets , [ - 1 ]) # We need to insert a fake background classes at the position 0 localization_pred = tf . pad ( localization_pred , [[ 0 , 0 ], [ 0 , 0 ], [ 4 , 0 ]]) localization_pred = tf . reshape ( localization_pred , [ - 1 , 4 ]) extracted_localization_pred = tf . boolean_mask ( localization_pred , one_hot_targets > 0 ) extracted_localization_pred = tf . reshape ( extracted_localization_pred , ( batch_size , - 1 , 4 )) y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : extracted_localization_pred } return self . compute_losses ( y_true , y_pred , weights ) def compute_fast_rcnn_metrics ( y_true: tf . Tensor , y_pred: tf . Tensor ) : \"\"\"Useful metrics that allows to track how behave the training of the fast rcnn head. `Warning`: This function should be used if the ground_truths have been added to the RoIs. It won't work if the there are no foreground ground_truths in the sample_boxes which isn't possible if they have been added. Arguments: y_true: A one-hot encoded vector with shape [batch_size, num_sample_anchors, num_classes] y_pred: A tensor with shape [batch_size, num_sample_anchors, num_classes], representing the classification logits. Returns: Tuple: 1. `accuracy`: A scalar tensor representing the accuracy with the background classes included 2. `fg_accuracy`: A scalar tensor representing the accuracy without the background classes included 3. `false_negative`: A scalar tensor representing the ratio of boxes predicted as background instead of their respective class among the foreground example to predict. \"\"\" # compute usefull metrics # Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis=- 1 , name='label_prediction' , output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # The accuracy allows to determine if the models perform well ( background included ) accuracy = tf . reduce_mean ( correct , name='accuracy' ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) num_fg = tf . shape ( fg_inds )[ 0 ] fg_label_pred = tf . argmax ( tf . gather_nd ( y_pred , fg_inds ), axis=- 1 ) num_zero = tf . reduce_sum ( tf . cast ( tf . equal ( fg_label_pred , 0 ), tf . int32 ), name='num_zero' ) # Number of example predicted as background instead of one of our classes false_negative = tf . cast ( tf . truediv ( num_zero , num_fg ), tf . float32 , name='false_negative' ) fg_accuracy = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name='fg_accuracy' ) return accuracy , fg_accuracy , false_negative remove_unwanted_doc ( FastRCNN , __ pdoc__ )","title":"Module kerod.layers.detection.fast_rcnn"},{"location":"reference/kerod/layers/detection/fast_rcnn/#functions","text":"","title":"Functions"},{"location":"reference/kerod/layers/detection/fast_rcnn/#compute_fast_rcnn_metrics","text":"def compute_fast_rcnn_metrics ( y_true : tensorflow . python . framework . ops . Tensor , y_pred : tensorflow . python . framework . ops . Tensor ) Useful metrics that allows to track how behave the training of the fast rcnn head. Warning : This function should be used if the ground_truths have been added to the RoIs. It won't work if the there are no foreground ground_truths in the sample_boxes which isn't possible if they have been added. Parameters: Name Description y_true A one-hot encoded vector with shape [batch_size, num_sample_anchors, num_classes] y_pred A tensor with shape [batch_size, num_sample_anchors, num_classes], representing the classification logits. Returns: Type Description Tuple 1. accuracy : A scalar tensor representing the accuracy with the background classes included 2. fg_accuracy : A scalar tensor representing the accuracy without the background classes included 3. false_negative : A scalar tensor representing the ratio of boxes predicted as background instead of their respective class among the foreground example to predict. View Source def compute_fast_rcnn_metrics ( y_true : tf . Tensor , y_pred : tf . Tensor ) : \" \"\" Useful metrics that allows to track how behave the training of the fast rcnn head. `Warning`: This function should be used if the ground_truths have been added to the RoIs. It won't work if the there are no foreground ground_truths in the sample_boxes which isn't possible if they have been added. Arguments: y_true: A one-hot encoded vector with shape [batch_size, num_sample_anchors, num_classes] y_pred: A tensor with shape [batch_size, num_sample_anchors, num_classes], representing the classification logits. Returns: Tuple: 1. `accuracy`: A scalar tensor representing the accuracy with the background classes included 2. `fg_accuracy`: A scalar tensor representing the accuracy without the background classes included 3. `false_negative`: A scalar tensor representing the ratio of boxes predicted as background instead of their respective class among the foreground example to predict. \"\" \" # compute usefull metrics #Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis =- 1 , name = 'label_prediction' , output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # The accuracy allows to determine if the models perform well (background included) accuracy = tf . reduce_mean ( correct , name = 'accuracy' ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) num_fg = tf . shape ( fg_inds ) [ 0 ] fg_label_pred = tf . argmax ( tf . gather_nd ( y_pred , fg_inds ), axis =- 1 ) num_zero = tf . reduce_sum ( tf . cast ( tf . equal ( fg_label_pred , 0 ), tf . int32 ), name = 'num_zero' ) # Number of example predicted as background instead of one of our classes false_negative = tf . cast ( tf . truediv ( num_zero , num_fg ), tf . float32 , name = 'false_negative' ) fg_accuracy = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name = 'fg_accuracy' ) return accuracy , fg_accuracy , false_negative","title":"compute_fast_rcnn_metrics"},{"location":"reference/kerod/layers/detection/fast_rcnn/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/detection/fast_rcnn/#fastrcnn","text":"class FastRCNN ( num_classes , ** kwargs ) are from Feature Pyramidal Networks for Object Detection .","title":"FastRCNN"},{"location":"reference/kerod/layers/detection/fast_rcnn/#arguments","text":"Name Description num_classes The number of classes that predict the classification head (N+1) where N is the number of classes of your dataset and 1 is the background.","title":"Arguments"},{"location":"reference/kerod/layers/detection/fast_rcnn/#call-arguments","text":"Name Description inputs A Tuple 1. pyramid : A List of tensors the output of the pyramid 2. anchors : A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)]","title":"Call arguments"},{"location":"reference/kerod/layers/detection/fast_rcnn/#call-returns","text":"Type Description Tuple classification_pred : A logit Tensor of shape [batch_size, num_boxes, num_classes] localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] anchors : A Tensor of shape [batch_size, num_boxes, 4]","title":"Call returns"},{"location":"reference/kerod/layers/detection/fast_rcnn/#ancestors-in-mro","text":"kerod.layers.detection.abstract_detection_head.AbstractDetectionHead keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/detection/fast_rcnn/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/detection/fast_rcnn/#build_detection_head","text":"def build_detection_head ( self , inputs ) Build a detection head composed of a classification and box_detection. Parameters: Name Description inputs A tensor of shape [batch_size, H, W, C] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_detection_head ( self , inputs ) : \"\"\" Build a detection head composed of a classification and box_detection. Arguments : inputs : A tensor of shape [ batch_size , H , W , C ] Returns : Tuple : classification_head : a tensor of shape [ batch_size , num_anchors , 2 ] localization_head : a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head","title":"build_detection_head"},{"location":"reference/kerod/layers/detection/fast_rcnn/#build_segmentation_head","text":"def build_segmentation_head ( self , inputs ) Build the detection head Parameters: Name Description inputs A tensor of float and shape [N, H, W, C] Returns: Type Description tf.Tensor A tensor and shape [N, H 2, W 2, num_classes - 1] View Source def build_segmentation_head ( self , inputs ) : \"\"\" Build the detection head Arguments : inputs : A tensor of float and shape [ N , H , W , C ] Returns : tf . Tensor : A tensor and shape [ N , H * 2 , W * 2 , num_classes - 1 ] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x","title":"build_segmentation_head"},{"location":"reference/kerod/layers/detection/fast_rcnn/#call","text":"def call ( self , inputs ) Build the computational graph of the fast RCNN HEAD. It performs a raw prediction of the FastRCNN head you can post_process them using: from kerod.layers.post_processing import post_process_fast_rcnn_boxes outputs = post_process_fast_rcnn_boxes ( classification_pred , localization_pred , anchors , images_information , num_classes ) where images_information is provided as input of your model and num_classes includes the background. Parameters: Name Description inputs A Tuple 1. pyramid : A List of tensors the output of the pyramid 2. anchors : A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Returns: Type Description Tuple classification_pred : A logit Tensor of shape [batch_size, num_boxes, num_classes] localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs ) : \"\"\"Build the computational graph of the fast RCNN HEAD. It performs a raw prediction of the FastRCNN head you can post_process them using: ```python from kerod.layers.post_processing import post_process_fast_rcnn_boxes outputs = post_process_fast_rcnn_boxes(classification_pred, localization_pred, anchors, images_information, num_classes) ``` where `images_information` is provided as input of your model and `num_classes` includes the background. Arguments: inputs: A Tuple 1. `pyramid`: A List of tensors the output of the pyramid 2. `anchors`: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Returns: Tuple: `classification_pred`: A logit Tensor of shape [batch_size, num_boxes, num_classes] `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" # Remove P6 pyramid = inputs [ 0 ][:- 1 ] anchors = inputs [ 1 ] # We can compute the original image shape regarding # TODO compute it more automatically without knowing that the last layer is stride 32 image_shape = tf . cast ( tf . shape ( pyramid [ - 1 ])[ 1 : 3 ] * 32 , dtype = self . _ compute_dtype ) boxe_tensors = multilevel_roi_align ( pyramid , anchors , image_shape , crop_size = 7 ) l = KL . Flatten ()( boxe_tensors ) for dense in self . denses : l = dense ( l ) classification_pred , localization_pred = self . build_detection_head ( tf . reshape ( l , ( - 1 , 1 , 1 , 1024 ))) batch_size = tf . shape ( anchors )[ 0 ] classification_pred = tf . reshape ( classification_pred , ( batch_size , - 1 , self . _ num_classes )) localization_pred = tf . reshape ( localization_pred , ( batch_size , - 1 , ( self . _ num_classes - 1 ) * 4 )) return classification_pred , localization_pred","title":"call"},{"location":"reference/kerod/layers/detection/fast_rcnn/#compute_loss","text":"def compute_loss ( self , y_true : dict , weights : dict , classification_pred : tensorflow . python . framework . ops . Tensor , localization_pred : tensorflow . python . framework . ops . Tensor ) Compute the loss of the FastRCNN Parameters: Name Description y_true A dict with : - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_anchors, num_classes] - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_anchors, 4] weights A dict with: - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_anchors, num_classes] - BoxField.BOXES : A 2-D tensor of shape [batch_size, num_anchors] classification_pred A 3-D tensor of float and shape [batch_size, num_anchors, num_classes] localization_pred A 3-D tensor of float and shape [batch_size, num_anchors, (num_classes - 1) * 4] Returns: Type Description Tuple - classification_loss : A scalar - localization_loss : A scalar View Source def compute_loss ( self , y_true : dict , weights : dict , classification_pred : tf . Tensor , localization_pred : tf . Tensor ) : \" \"\" Compute the loss of the FastRCNN Arguments: y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, 4] weights: A dict with: - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] classification_pred: A 3-D tensor of float and shape [batch_size, num_anchors, num_classes] localization_pred: A 3-D tensor of float and shape [batch_size, num_anchors, (num_classes - 1) * 4] Returns: Tuple: - `classification_loss`: A scalar - `localization_loss`: A scalar \"\" \" y_true_classification = tf . cast ( y_true [ BoxField . LABELS ] , tf . int32 ) accuracy , fg_accuracy , false_negative = compute_fast_rcnn_metrics ( y_true_classification , classification_pred ) self . add_metric ( accuracy , name = 'accuracy' , aggregation = 'mean' ) self . add_metric ( fg_accuracy , name = 'fg_accuracy' , aggregation = 'mean' ) self . add_metric ( false_negative , name = 'false_negative' , aggregation = 'mean' ) # y_true[BoxField.LABELS] is just 1 and 0 we are using it as mask to extract # the corresponding target anchors batch_size = tf . shape ( classification_pred ) [ 0 ] # We create a boolean mask to extract the desired localization prediction to compute # the loss one_hot_targets = tf . one_hot ( y_true_classification , self . _num_classes , dtype = tf . int8 ) one_hot_targets = tf . reshape ( one_hot_targets , [ - 1 ] ) # We need to insert a fake background classes at the position 0 localization_pred = tf . pad ( localization_pred , [[ 0 , 0 ] , [ 0 , 0 ] , [ 4 , 0 ]] ) localization_pred = tf . reshape ( localization_pred , [ - 1 , 4 ] ) extracted_localization_pred = tf . boolean_mask ( localization_pred , one_hot_targets > 0 ) extracted_localization_pred = tf . reshape ( extracted_localization_pred , ( batch_size , - 1 , 4 )) y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : extracted_localization_pred } return self . compute_losses ( y_true , y_pred , weights )","title":"compute_loss"},{"location":"reference/kerod/layers/detection/fast_rcnn/#compute_losses","text":"def compute_losses ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], weights : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> dict Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Parameters: Name Description y_pred A dict of tensors of shape [N, nb_boxes, num_output]. y_true A dict of tensors of shape [N, nb_boxes, num_output]. weights A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: Type Description None dict : A dict of different losses View Source def compute_losses ( self , y_true : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , weights : Dict [ str, tf.Tensor ] ) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ) : losses = loss ( tf . cast ( y_true [ target ] , tf . float32 ), tf . cast ( y_pred [ target ] , tf . float32 ), sample_weight = tf . cast ( weights [ target ] , tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField.LABELS ] , axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f '{self.name}_classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f '{self.name}_localization_loss' , aggregation = 'mean' ) self . add_loss ( [ classification_loss, localization_loss ] ) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f '{self.name}_segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss }","title":"compute_losses"},{"location":"reference/kerod/layers/detection/fast_rcnn/#sample_boxes","text":"def sample_boxes ( self , anchors : tensorflow . python . framework . ops . Tensor , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], sampling_size : int = 512 , sampling_positive_ratio : float = 0.25 ) Perform the sampling of the target anchors. During the training a set of RoIs is detected by the RPN. However, you do not want to analyse all the set. You only want to analyse the anchors that you sampled with this method. Parameters: Name Description anchors A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] ground_truths A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 sampling_size Desired sampling size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. sampling_positive_ratio Desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: Type Description Tuple 1. y_true: A dict with : - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_anchors, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_anchors, box_code_dimension] 2. weights: A dict with: - BoxField.LABELS : A 2-D tensor of shape [batch_size, num_anchors], - BoxField.BOXES : A 2-D tensor of shape [batch_size, num_anchors] Raises: Type Description ValueError If the batch_size is None. ValueError If the batch_size between your ground_truths and the anchors does not match. View Source def sample_boxes ( self , anchors : tf . Tensor , ground_truths : Dict [ str, tf.Tensor ] , sampling_size : int = 512 , sampling_positive_ratio : float = 0.25 ) : \"\"\"Perform the sampling of the target anchors. During the training a set of RoIs is detected by the RPN. However, you do not want to analyse all the set. You only want to analyse the anchors that you sampled with this method. Arguments: anchors: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 sampling_size: Desired sampling size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. sampling_positive_ratio: Desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: Tuple: 1. y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, box_code_dimension] 2. weights: A dict with: - `BoxField.LABELS`: A 2-D tensor of shape [batch_size, num_anchors], - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] Raises: ValueError: If the batch_size is None. ValueError: If the batch_size between your ground_truths and the anchors does not match. \"\"\" ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField.BOXES ] , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } y_true , weights = self . target_assigner . assign ( { BoxField . BOXES : anchors } , ground_truths ) labels = y_true [ BoxField.LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField.LABELS ] , sampling_size , labels , positive_fraction = sampling_positive_ratio , dtype = self . _compute_dtype ) weights [ BoxField.LABELS ] = sample_idx * weights [ BoxField.LABELS ] weights [ BoxField.BOXES ] = sample_idx * weights [ BoxField.BOXES ] selected_boxes_idx = tf . where ( sample_idx == 1 ) batch_size = tf . shape ( sample_idx ) [ 0 ] # Extract the selected anchors corresponding anchors # tf . gather_nd collaps the batch_together so we reshape with the proper batch_size anchors = tf . reshape ( tf . gather_nd ( anchors , selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField.BOXES ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField.BOXES ] , selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField.LABELS ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField.LABELS ] , selected_boxes_idx ), ( batch_size , - 1 )) for key in y_true . keys () : weights [ key ] = tf . reshape ( tf . gather_nd ( weights [ key ] , selected_boxes_idx ), ( batch_size , - 1 )) weights [ key ] = tf . stop_gradient ( weights [ key ] ) y_true [ key ] = tf . stop_gradient ( y_true [ key ] ) return y_true , weights , anchors","title":"sample_boxes"},{"location":"reference/kerod/layers/detection/pooling_ops/","text":"Module kerod.layers.detection.pooling_ops None None View Source import tensorflow as tf import tensorflow.keras.layers as KL from tensorflow.keras import backend as K from kerod.core.box_ops import ( compute_area , normalize_box_coordinates , transform_fpcoor_for_tf ) def _crop_and_resize ( tensor , boxes , box_indices , crop_size : int ): \"\"\"Taken from tensorpack (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Aligned version of tf.image.crop_and_resize, following our definition of floating point boxes. Arguments: tensor: A 4-D tensor of shape [batch, image_height, image_width, depth]. Both image_height and image_width need to be positive. boxes: A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices: A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. crop_size: An int representing the ouput size of the crop. Returns: A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. \"\"\" boxes = tf . stop_gradient ( boxes ) # TF's crop_and_resize produces zeros on border. The symetric padding # allows to have a better interpolation for the boxes on the border of the image. tensor = tf . pad ( tensor , [[ 0 , 0 ], [ 1 , 1 ], [ 1 , 1 ], [ 0 , 0 ]], mode = 'SYMMETRIC' ) boxes = boxes + 1 # Height, width extraction tensor_shape = tf . shape ( tensor )[ 1 : 3 ] # The boxes should be at the size of the input tensor boxes = transform_fpcoor_for_tf ( boxes , tensor_shape , [ crop_size , crop_size ]) ret = tf . image . crop_and_resize ( tensor , tf . cast ( boxes , tf . float32 ), # crop and resize needs float32 tf . cast ( box_indices , tf . int32 ), crop_size = [ crop_size , crop_size ]) return ret def roi_align ( inputs , boxes , box_indices , image_shape , crop_size : int ): \"\"\"RoI align like operation from the paper Mask-RCNN. Arguments: inputs: A 4-D tensor of shape [batch, height, tensor_width, depth]. Both image_height and image_width need to be positive. boxes: A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices: A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. image_shape: A tuple with the height and the width of the original image input image crop_size: An int representing the ouput size of the crop. Returns: A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. \"\"\" normalized_boxes = normalize_box_coordinates ( boxes , image_shape [ 0 ], image_shape [ 1 ]) # Normalized the boxes to the input tensor_shape # TODO rewrite this normalization unnomarlization isn't pretty at all tensor_shape = tf . shape ( inputs )[ 1 : 3 ] normalized_boxes *= tf . cast ( tf . tile ( tensor_shape [ None ], [ 1 , 2 ]), normalized_boxes . dtype ) ret = _crop_and_resize ( inputs , normalized_boxes , box_indices , crop_size * 2 ) return KL . AveragePooling2D ( padding = 'same' )( ret ) def multilevel_roi_align ( inputs , boxes , image_shape , crop_size : int = 7 ): \"\"\"Perform a batch multilevel roi_align on the inputs Arguments: - *inputs*: A list of tensors of shape [batch_size, width, height, channel] representing the pyramid. - *boxes*: A tensor and shape [batch_size, num_boxes, (y1, x1, y2, x2)] - *image_shape*: A tuple with the height and the width of the original image input image Returns: A tensor and shape [batch_size * num_boxes, 7, 7, channel] \"\"\" boxes_per_level , box_indices_per_level , pos_per_level = match_boxes_to_their_pyramid_level ( boxes , len ( inputs )) tensors_per_level = [] for tensor , target_boxes , box_indices in zip ( inputs , boxes_per_level , box_indices_per_level ): tensors_per_level . append ( roi_align ( tensor , target_boxes , box_indices , image_shape , crop_size )) tensors = tf . concat ( values = tensors_per_level , axis = 0 ) original_pos = tf . concat ( values = pos_per_level , axis = 0 ) # Reorder the tensor per batch indices_to_reorder_boxes = tf . math . invert_permutation ( original_pos ) tensors = tf . gather ( tensors , indices_to_reorder_boxes ) return tensors def match_boxes_to_their_pyramid_level ( boxes , num_level ): \"\"\"Match the boxes to the proper level based on their area Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, 4] - *num_level*: Number of level of the target pyramid Returns: - *boxes_per_level*: A list of 2-D tensor and shape [N, 4] - *box_indices_per_level*: A list of 1-D tensor with int32 values in [0, batch). The value of a box_indices_per_level[lvl][i] specifies the image that the i-th box refers to. - *original_pos_per_level* A list of 1-D tensor with int32 values in [0, num_boxes * batch_size) It will be useful to reorder our boxes after the roi_align operation. \"\"\" def _select_level ( tensors , levels ): return [ tf . squeeze ( tf . gather ( tensors , selected_level ), 1 ) for selected_level in levels ] batch_size = tf . shape ( boxes )[ 0 ] num_boxes = tf . shape ( boxes )[ 1 ] boxes = tf . reshape ( boxes , ( - 1 , 4 )) box_levels = assign_pyramid_level_to_boxes ( boxes , num_level ) box_indices = tf . tile ( tf . expand_dims ( tf . range ( 0 , batch_size ), 1 ), [ 1 , num_boxes ]) box_indices = tf . reshape ( box_indices , ( - 1 ,)) box_original_pos = tf . range ( batch_size * num_boxes ) levels = [ tf . where ( tf . equal ( box_levels , i )) for i in range ( num_level )] boxes_per_level = _select_level ( boxes , levels ) box_indices_per_level = _select_level ( box_indices , levels ) original_pos_per_level = _select_level ( box_original_pos , levels ) return boxes_per_level , box_indices_per_level , original_pos_per_level def assign_pyramid_level_to_boxes ( boxes , num_level , level_target = 2 ): \"\"\"Compute the pyramid level of an RoI Arguments: - *boxes*: A tensor of shape [nb_batches * nb_boxes, 4] - *num_level*: Assign all the boxes mapped to a superior level to the num_level. level_target: Will affect all the boxes of area 224^2 to the level_target of the pyramid. Returns: A 2-D tensor of type int32 and shape [nb_batches * nb_boxes] corresponding to the target level of the pyramid. \"\"\" denominator = tf . constant ( 224 , dtype = boxes . dtype ) area = compute_area ( boxes ) k = level_target + tf . math . log ( tf . sqrt ( area ) / denominator + K . epsilon ()) * tf . cast ( 1. / tf . math . log ( 2.0 ), dtype = boxes . dtype ) k = tf . cast ( k , tf . int32 ) k = tf . clip_by_value ( k , 0 , num_level - 1 ) k = tf . reshape ( k , [ - 1 ]) return k Functions assign_pyramid_level_to_boxes def assign_pyramid_level_to_boxes ( boxes , num_level , level_target = 2 ) Compute the pyramid level of an RoI Arguments: boxes : A tensor of shape [nb_batches * nb_boxes, 4] num_level : Assign all the boxes mapped to a superior level to the num_level. level_target: Will affect all the boxes of area 224^2 to the level_target of the pyramid. Returns: A 2-D tensor of type int32 and shape [nb_batches * nb_boxes] corresponding to the target level of the pyramid. View Source def assign_pyramid_level_to_boxes ( boxes , num_level , level_target = 2 ): \"\"\"Compute the pyramid level of an RoI Arguments: - *boxes*: A tensor of shape [nb_batches * nb_boxes, 4] - *num_level*: Assign all the boxes mapped to a superior level to the num_level. level_target: Will affect all the boxes of area 224^2 to the level_target of the pyramid. Returns: A 2-D tensor of type int32 and shape [nb_batches * nb_boxes] corresponding to the target level of the pyramid. \"\"\" denominator = tf . constant ( 224 , dtype = boxes . dtype ) area = compute_area ( boxes ) k = level_target + tf . math . log ( tf . sqrt ( area ) / denominator + K . epsilon ()) * tf . cast ( 1. / tf . math . log ( 2.0 ), dtype = boxes . dtype ) k = tf . cast ( k , tf . int32 ) k = tf . clip_by_value ( k , 0 , num_level - 1 ) k = tf . reshape ( k , [ - 1 ]) return k match_boxes_to_their_pyramid_level def match_boxes_to_their_pyramid_level ( boxes , num_level ) Match the boxes to the proper level based on their area Arguments: boxes : A tensor of shape [batch_size, num_boxes, 4] num_level : Number of level of the target pyramid Returns: boxes_per_level : A list of 2-D tensor and shape [N, 4] box_indices_per_level : A list of 1-D tensor with int32 values in [0, batch). The value of a box_indices_per_level[lvl][i] specifies the image that the i-th box refers to. original_pos_per_level A list of 1-D tensor with int32 values in [0, num_boxes * batch_size) It will be useful to reorder our boxes after the roi_align operation. View Source def match_boxes_to_their_pyramid_level ( boxes , num_level ) : \"\"\"Match the boxes to the proper level based on their area Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, 4] - *num_level*: Number of level of the target pyramid Returns: - *boxes_per_level*: A list of 2-D tensor and shape [N, 4] - *box_indices_per_level*: A list of 1-D tensor with int32 values in [0, batch). The value of a box_indices_per_level[lvl][i] specifies the image that the i-th box refers to. - *original_pos_per_level* A list of 1-D tensor with int32 values in [0, num_boxes * batch_size) It will be useful to reorder our boxes after the roi_align operation. \"\"\" def _select_level ( tensors , levels ) : return [ tf.squeeze(tf.gather(tensors, selected_level), 1) for selected_level in levels ] batch_size = tf . shape ( boxes ) [ 0 ] num_boxes = tf . shape ( boxes ) [ 1 ] boxes = tf . reshape ( boxes , ( - 1 , 4 )) box_levels = assign_pyramid_level_to_boxes ( boxes , num_level ) box_indices = tf . tile ( tf . expand_dims ( tf . range ( 0 , batch_size ), 1 ), [ 1, num_boxes ] ) box_indices = tf . reshape ( box_indices , ( - 1 ,)) box_original_pos = tf . range ( batch_size * num_boxes ) levels = [ tf.where(tf.equal(box_levels, i)) for i in range(num_level) ] boxes_per_level = _select_level ( boxes , levels ) box_indices_per_level = _select_level ( box_indices , levels ) original_pos_per_level = _select_level ( box_original_pos , levels ) return boxes_per_level , box_indices_per_level , original_pos_per_level multilevel_roi_align def multilevel_roi_align ( inputs , boxes , image_shape , crop_size : int = 7 ) Perform a batch multilevel roi_align on the inputs Arguments: inputs : A list of tensors of shape [batch_size, width, height, channel] representing the pyramid. boxes : A tensor and shape [batch_size, num_boxes, (y1, x1, y2, x2)] image_shape : A tuple with the height and the width of the original image input image Returns: A tensor and shape [batch_size * num_boxes, 7, 7, channel] View Source def multilevel_roi_align ( inputs , boxes , image_shape , crop_size : int = 7 ) : \"\"\" Perform a batch multilevel roi_align on the inputs Arguments : - * inputs * : A list of tensors of shape [ batch_size , width , height , channel ] representing the pyramid . - * boxes * : A tensor and shape [ batch_size , num_boxes , ( y1 , x1 , y2 , x2 ) ] - * image_shape * : A tuple with the height and the width of the original image input image Returns : A tensor and shape [ batch_size * num_boxes , 7 , 7 , channel ] \"\"\" boxes_per_level , box_indices_per_level , pos_per_level = match_boxes_to_their_pyramid_level ( boxes , len ( inputs )) tensors_per_level = [] for tensor , target_boxes , box_indices in zip ( inputs , boxes_per_level , box_indices_per_level ) : tensors_per_level . append ( roi_align ( tensor , target_boxes , box_indices , image_shape , crop_size )) tensors = tf . concat ( values = tensors_per_level , axis = 0 ) original_pos = tf . concat ( values = pos_per_level , axis = 0 ) # Reorder the tensor per batch indices_to_reorder_boxes = tf . math . invert_permutation ( original_pos ) tensors = tf . gather ( tensors , indices_to_reorder_boxes ) return tensors roi_align def roi_align ( inputs , boxes , box_indices , image_shape , crop_size : int ) RoI align like operation from the paper Mask-RCNN. Parameters: Name Description inputs A 4-D tensor of shape [batch, height, tensor_width, depth]. Both image_height and image_width need to be positive. boxes A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. image_shape A tuple with the height and the width of the original image input image crop_size An int representing the ouput size of the crop. Returns: Type Description None A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. View Source def roi_align ( inputs , boxes , box_indices , image_shape , crop_size : int ) : \"\"\"RoI align like operation from the paper Mask-RCNN. Arguments: inputs: A 4-D tensor of shape [batch, height, tensor_width, depth]. Both image_height and image_width need to be positive. boxes: A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices: A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. image_shape: A tuple with the height and the width of the original image input image crop_size: An int representing the ouput size of the crop. Returns: A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. \"\"\" normalized_boxes = normalize_box_coordinates ( boxes , image_shape [ 0 ] , image_shape [ 1 ] ) # Normalized the boxes to the input tensor_shape # TODO rewrite this normalization unnomarlization isn 't pretty at all tensor_shape = tf.shape(inputs)[1:3] normalized_boxes *= tf.cast(tf.tile(tensor_shape[None], [1, 2]), normalized_boxes.dtype) ret = _crop_and_resize(inputs, normalized_boxes, box_indices, crop_size * 2) return KL.AveragePooling2D(padding=' same ' )( ret )","title":"Pooling Ops"},{"location":"reference/kerod/layers/detection/pooling_ops/#module-kerodlayersdetectionpooling_ops","text":"None None View Source import tensorflow as tf import tensorflow.keras.layers as KL from tensorflow.keras import backend as K from kerod.core.box_ops import ( compute_area , normalize_box_coordinates , transform_fpcoor_for_tf ) def _crop_and_resize ( tensor , boxes , box_indices , crop_size : int ): \"\"\"Taken from tensorpack (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Aligned version of tf.image.crop_and_resize, following our definition of floating point boxes. Arguments: tensor: A 4-D tensor of shape [batch, image_height, image_width, depth]. Both image_height and image_width need to be positive. boxes: A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices: A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. crop_size: An int representing the ouput size of the crop. Returns: A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. \"\"\" boxes = tf . stop_gradient ( boxes ) # TF's crop_and_resize produces zeros on border. The symetric padding # allows to have a better interpolation for the boxes on the border of the image. tensor = tf . pad ( tensor , [[ 0 , 0 ], [ 1 , 1 ], [ 1 , 1 ], [ 0 , 0 ]], mode = 'SYMMETRIC' ) boxes = boxes + 1 # Height, width extraction tensor_shape = tf . shape ( tensor )[ 1 : 3 ] # The boxes should be at the size of the input tensor boxes = transform_fpcoor_for_tf ( boxes , tensor_shape , [ crop_size , crop_size ]) ret = tf . image . crop_and_resize ( tensor , tf . cast ( boxes , tf . float32 ), # crop and resize needs float32 tf . cast ( box_indices , tf . int32 ), crop_size = [ crop_size , crop_size ]) return ret def roi_align ( inputs , boxes , box_indices , image_shape , crop_size : int ): \"\"\"RoI align like operation from the paper Mask-RCNN. Arguments: inputs: A 4-D tensor of shape [batch, height, tensor_width, depth]. Both image_height and image_width need to be positive. boxes: A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices: A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. image_shape: A tuple with the height and the width of the original image input image crop_size: An int representing the ouput size of the crop. Returns: A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. \"\"\" normalized_boxes = normalize_box_coordinates ( boxes , image_shape [ 0 ], image_shape [ 1 ]) # Normalized the boxes to the input tensor_shape # TODO rewrite this normalization unnomarlization isn't pretty at all tensor_shape = tf . shape ( inputs )[ 1 : 3 ] normalized_boxes *= tf . cast ( tf . tile ( tensor_shape [ None ], [ 1 , 2 ]), normalized_boxes . dtype ) ret = _crop_and_resize ( inputs , normalized_boxes , box_indices , crop_size * 2 ) return KL . AveragePooling2D ( padding = 'same' )( ret ) def multilevel_roi_align ( inputs , boxes , image_shape , crop_size : int = 7 ): \"\"\"Perform a batch multilevel roi_align on the inputs Arguments: - *inputs*: A list of tensors of shape [batch_size, width, height, channel] representing the pyramid. - *boxes*: A tensor and shape [batch_size, num_boxes, (y1, x1, y2, x2)] - *image_shape*: A tuple with the height and the width of the original image input image Returns: A tensor and shape [batch_size * num_boxes, 7, 7, channel] \"\"\" boxes_per_level , box_indices_per_level , pos_per_level = match_boxes_to_their_pyramid_level ( boxes , len ( inputs )) tensors_per_level = [] for tensor , target_boxes , box_indices in zip ( inputs , boxes_per_level , box_indices_per_level ): tensors_per_level . append ( roi_align ( tensor , target_boxes , box_indices , image_shape , crop_size )) tensors = tf . concat ( values = tensors_per_level , axis = 0 ) original_pos = tf . concat ( values = pos_per_level , axis = 0 ) # Reorder the tensor per batch indices_to_reorder_boxes = tf . math . invert_permutation ( original_pos ) tensors = tf . gather ( tensors , indices_to_reorder_boxes ) return tensors def match_boxes_to_their_pyramid_level ( boxes , num_level ): \"\"\"Match the boxes to the proper level based on their area Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, 4] - *num_level*: Number of level of the target pyramid Returns: - *boxes_per_level*: A list of 2-D tensor and shape [N, 4] - *box_indices_per_level*: A list of 1-D tensor with int32 values in [0, batch). The value of a box_indices_per_level[lvl][i] specifies the image that the i-th box refers to. - *original_pos_per_level* A list of 1-D tensor with int32 values in [0, num_boxes * batch_size) It will be useful to reorder our boxes after the roi_align operation. \"\"\" def _select_level ( tensors , levels ): return [ tf . squeeze ( tf . gather ( tensors , selected_level ), 1 ) for selected_level in levels ] batch_size = tf . shape ( boxes )[ 0 ] num_boxes = tf . shape ( boxes )[ 1 ] boxes = tf . reshape ( boxes , ( - 1 , 4 )) box_levels = assign_pyramid_level_to_boxes ( boxes , num_level ) box_indices = tf . tile ( tf . expand_dims ( tf . range ( 0 , batch_size ), 1 ), [ 1 , num_boxes ]) box_indices = tf . reshape ( box_indices , ( - 1 ,)) box_original_pos = tf . range ( batch_size * num_boxes ) levels = [ tf . where ( tf . equal ( box_levels , i )) for i in range ( num_level )] boxes_per_level = _select_level ( boxes , levels ) box_indices_per_level = _select_level ( box_indices , levels ) original_pos_per_level = _select_level ( box_original_pos , levels ) return boxes_per_level , box_indices_per_level , original_pos_per_level def assign_pyramid_level_to_boxes ( boxes , num_level , level_target = 2 ): \"\"\"Compute the pyramid level of an RoI Arguments: - *boxes*: A tensor of shape [nb_batches * nb_boxes, 4] - *num_level*: Assign all the boxes mapped to a superior level to the num_level. level_target: Will affect all the boxes of area 224^2 to the level_target of the pyramid. Returns: A 2-D tensor of type int32 and shape [nb_batches * nb_boxes] corresponding to the target level of the pyramid. \"\"\" denominator = tf . constant ( 224 , dtype = boxes . dtype ) area = compute_area ( boxes ) k = level_target + tf . math . log ( tf . sqrt ( area ) / denominator + K . epsilon ()) * tf . cast ( 1. / tf . math . log ( 2.0 ), dtype = boxes . dtype ) k = tf . cast ( k , tf . int32 ) k = tf . clip_by_value ( k , 0 , num_level - 1 ) k = tf . reshape ( k , [ - 1 ]) return k","title":"Module kerod.layers.detection.pooling_ops"},{"location":"reference/kerod/layers/detection/pooling_ops/#functions","text":"","title":"Functions"},{"location":"reference/kerod/layers/detection/pooling_ops/#assign_pyramid_level_to_boxes","text":"def assign_pyramid_level_to_boxes ( boxes , num_level , level_target = 2 ) Compute the pyramid level of an RoI Arguments: boxes : A tensor of shape [nb_batches * nb_boxes, 4] num_level : Assign all the boxes mapped to a superior level to the num_level. level_target: Will affect all the boxes of area 224^2 to the level_target of the pyramid. Returns: A 2-D tensor of type int32 and shape [nb_batches * nb_boxes] corresponding to the target level of the pyramid. View Source def assign_pyramid_level_to_boxes ( boxes , num_level , level_target = 2 ): \"\"\"Compute the pyramid level of an RoI Arguments: - *boxes*: A tensor of shape [nb_batches * nb_boxes, 4] - *num_level*: Assign all the boxes mapped to a superior level to the num_level. level_target: Will affect all the boxes of area 224^2 to the level_target of the pyramid. Returns: A 2-D tensor of type int32 and shape [nb_batches * nb_boxes] corresponding to the target level of the pyramid. \"\"\" denominator = tf . constant ( 224 , dtype = boxes . dtype ) area = compute_area ( boxes ) k = level_target + tf . math . log ( tf . sqrt ( area ) / denominator + K . epsilon ()) * tf . cast ( 1. / tf . math . log ( 2.0 ), dtype = boxes . dtype ) k = tf . cast ( k , tf . int32 ) k = tf . clip_by_value ( k , 0 , num_level - 1 ) k = tf . reshape ( k , [ - 1 ]) return k","title":"assign_pyramid_level_to_boxes"},{"location":"reference/kerod/layers/detection/pooling_ops/#match_boxes_to_their_pyramid_level","text":"def match_boxes_to_their_pyramid_level ( boxes , num_level ) Match the boxes to the proper level based on their area Arguments: boxes : A tensor of shape [batch_size, num_boxes, 4] num_level : Number of level of the target pyramid Returns: boxes_per_level : A list of 2-D tensor and shape [N, 4] box_indices_per_level : A list of 1-D tensor with int32 values in [0, batch). The value of a box_indices_per_level[lvl][i] specifies the image that the i-th box refers to. original_pos_per_level A list of 1-D tensor with int32 values in [0, num_boxes * batch_size) It will be useful to reorder our boxes after the roi_align operation. View Source def match_boxes_to_their_pyramid_level ( boxes , num_level ) : \"\"\"Match the boxes to the proper level based on their area Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, 4] - *num_level*: Number of level of the target pyramid Returns: - *boxes_per_level*: A list of 2-D tensor and shape [N, 4] - *box_indices_per_level*: A list of 1-D tensor with int32 values in [0, batch). The value of a box_indices_per_level[lvl][i] specifies the image that the i-th box refers to. - *original_pos_per_level* A list of 1-D tensor with int32 values in [0, num_boxes * batch_size) It will be useful to reorder our boxes after the roi_align operation. \"\"\" def _select_level ( tensors , levels ) : return [ tf.squeeze(tf.gather(tensors, selected_level), 1) for selected_level in levels ] batch_size = tf . shape ( boxes ) [ 0 ] num_boxes = tf . shape ( boxes ) [ 1 ] boxes = tf . reshape ( boxes , ( - 1 , 4 )) box_levels = assign_pyramid_level_to_boxes ( boxes , num_level ) box_indices = tf . tile ( tf . expand_dims ( tf . range ( 0 , batch_size ), 1 ), [ 1, num_boxes ] ) box_indices = tf . reshape ( box_indices , ( - 1 ,)) box_original_pos = tf . range ( batch_size * num_boxes ) levels = [ tf.where(tf.equal(box_levels, i)) for i in range(num_level) ] boxes_per_level = _select_level ( boxes , levels ) box_indices_per_level = _select_level ( box_indices , levels ) original_pos_per_level = _select_level ( box_original_pos , levels ) return boxes_per_level , box_indices_per_level , original_pos_per_level","title":"match_boxes_to_their_pyramid_level"},{"location":"reference/kerod/layers/detection/pooling_ops/#multilevel_roi_align","text":"def multilevel_roi_align ( inputs , boxes , image_shape , crop_size : int = 7 ) Perform a batch multilevel roi_align on the inputs Arguments: inputs : A list of tensors of shape [batch_size, width, height, channel] representing the pyramid. boxes : A tensor and shape [batch_size, num_boxes, (y1, x1, y2, x2)] image_shape : A tuple with the height and the width of the original image input image Returns: A tensor and shape [batch_size * num_boxes, 7, 7, channel] View Source def multilevel_roi_align ( inputs , boxes , image_shape , crop_size : int = 7 ) : \"\"\" Perform a batch multilevel roi_align on the inputs Arguments : - * inputs * : A list of tensors of shape [ batch_size , width , height , channel ] representing the pyramid . - * boxes * : A tensor and shape [ batch_size , num_boxes , ( y1 , x1 , y2 , x2 ) ] - * image_shape * : A tuple with the height and the width of the original image input image Returns : A tensor and shape [ batch_size * num_boxes , 7 , 7 , channel ] \"\"\" boxes_per_level , box_indices_per_level , pos_per_level = match_boxes_to_their_pyramid_level ( boxes , len ( inputs )) tensors_per_level = [] for tensor , target_boxes , box_indices in zip ( inputs , boxes_per_level , box_indices_per_level ) : tensors_per_level . append ( roi_align ( tensor , target_boxes , box_indices , image_shape , crop_size )) tensors = tf . concat ( values = tensors_per_level , axis = 0 ) original_pos = tf . concat ( values = pos_per_level , axis = 0 ) # Reorder the tensor per batch indices_to_reorder_boxes = tf . math . invert_permutation ( original_pos ) tensors = tf . gather ( tensors , indices_to_reorder_boxes ) return tensors","title":"multilevel_roi_align"},{"location":"reference/kerod/layers/detection/pooling_ops/#roi_align","text":"def roi_align ( inputs , boxes , box_indices , image_shape , crop_size : int ) RoI align like operation from the paper Mask-RCNN. Parameters: Name Description inputs A 4-D tensor of shape [batch, height, tensor_width, depth]. Both image_height and image_width need to be positive. boxes A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. image_shape A tuple with the height and the width of the original image input image crop_size An int representing the ouput size of the crop. Returns: Type Description None A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. View Source def roi_align ( inputs , boxes , box_indices , image_shape , crop_size : int ) : \"\"\"RoI align like operation from the paper Mask-RCNN. Arguments: inputs: A 4-D tensor of shape [batch, height, tensor_width, depth]. Both image_height and image_width need to be positive. boxes: A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices: A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. image_shape: A tuple with the height and the width of the original image input image crop_size: An int representing the ouput size of the crop. Returns: A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. \"\"\" normalized_boxes = normalize_box_coordinates ( boxes , image_shape [ 0 ] , image_shape [ 1 ] ) # Normalized the boxes to the input tensor_shape # TODO rewrite this normalization unnomarlization isn 't pretty at all tensor_shape = tf.shape(inputs)[1:3] normalized_boxes *= tf.cast(tf.tile(tensor_shape[None], [1, 2]), normalized_boxes.dtype) ret = _crop_and_resize(inputs, normalized_boxes, box_indices, crop_size * 2) return KL.AveragePooling2D(padding=' same ' )( ret )","title":"roi_align"},{"location":"reference/kerod/layers/detection/rpn/","text":"Module kerod.layers.detection.rpn None None View Source from typing import List import tensorflow as tf import tensorflow.keras.layers as KL from tensorflow.keras import initializers from tensorflow.keras.losses import SparseCategoricalCrossentropy from kerod.core.box_coder import encode_boxes_faster_rcnn from kerod.core.losses import L1Loss from kerod.core.matcher import Matcher from kerod.core.sampling_ops import batch_sample_balanced_positive_negative from kerod.core.similarity import IoUSimilarity from kerod.core.standard_fields import BoxField from kerod.core.target_assigner import TargetAssigner from kerod.layers import Anchors from kerod.layers.detection.abstract_detection_head import \\ AbstractDetectionHead from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} SAMPLING_SIZE = 256 SAMPLING_POSITIVE_RATIO = 0.5 class RegionProposalNetwork ( AbstractDetectionHead ): \"\"\"It has been introduced in the [Faster R-CNN paper](https://arxiv.org/abs/1506.01497) and use the parameters from [Feature Pyramidal Networks for Object Detection](https://arxiv.org/abs/1612.03144). Arguments: anchor_ratios: The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) Call arguments: inputs: A List of tensors the output of the pyramid Call returns: Tuple: - `localization_pred`: A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - `classification_pred`: A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - `anchors`: A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] \"\"\" def __init__ ( self , anchor_ratios = ( 0.5 , 1 , 2 ), ** kwargs ): super () . __init__ ( 2 , SparseCategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE , from_logits = True ), L1Loss ( reduction = tf . keras . losses . Reduction . NONE ), multiples = len ( anchor_ratios ), kernel_initializer_classification_head = initializers . RandomNormal ( stddev = 0.01 ), kernel_initializer_box_prediction_head = initializers . RandomNormal ( stddev = 0.01 ), ** kwargs ) #Force each ground_truths to match to at least one anchor matcher = Matcher ([ 0.3 , 0.7 ], [ 0 , - 1 , 1 ], allow_low_quality_matches = True ) self . target_assigner = TargetAssigner ( IoUSimilarity (), matcher , encode_boxes_faster_rcnn , dtype = self . _compute_dtype ) anchor_strides = ( 4 , 8 , 16 , 32 , 64 ) anchor_zises = ( 32 , 64 , 128 , 256 , 512 ) self . _anchor_ratios = anchor_ratios # Precompute a deterministic grid of anchors for each layer of the pyramid. # We will extract a subpart of the anchors according to self . _anchors = [ Anchors ( stride , size , self . _anchor_ratios ) for stride , size in zip ( anchor_strides , anchor_zises ) ] def build ( self , input_shape ): self . rpn_conv2d = KL . Conv2D ( 512 , ( 3 , 3 ), padding = 'same' , kernel_initializer = self . _kernel_initializer_classification_head , kernel_regularizer = self . _kernel_regularizer ) super () . build ( input_shape ) def build_rpn_head ( self , inputs ): \"\"\"Predictions for the classification and the regression Arguments: inputs: A tensor of shape [batch_size, width, height, channel] Returns: Tuple: classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] \"\"\" batch_size = tf . shape ( inputs )[ 0 ] rpn_conv2d = self . rpn_conv2d ( inputs ) classification_head , localization_head = self . build_detection_head ( rpn_conv2d ) classification_head = tf . reshape ( classification_head , ( batch_size , - 1 , 2 )) localization_head = tf . reshape ( localization_head , ( batch_size , - 1 , 4 )) return classification_head , localization_head def call ( self , inputs : List [ tf . Tensor ]): \"\"\"Create the computation graph for the rpn inference Arguments: inputs: A List of tensors the output of the pyramid Returns: Tuple: - `localization_pred`: A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - `classification_pred`: A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - `anchors`: A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] \"\"\" anchors = [ anchors ( tensor ) for tensor , anchors in zip ( inputs , self . _anchors )] rpn_predictions = [ self . build_rpn_head ( tensor ) for tensor in inputs ] localization_pred = [ prediction [ 1 ] for prediction in rpn_predictions ] classification_pred = [ prediction [ 0 ] for prediction in rpn_predictions ] return localization_pred , classification_pred , anchors def compute_loss ( self , localization_pred , classification_pred , anchors , ground_truths ): \"\"\"Compute the loss Arguments: localization_pred: A list of tensors of shape [batch_size, num_anchors, 4]. classification_pred: A list of tensors of shape [batch_size, num_anchors, 2] anchors: A list of tensors of shape [num_anchors, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 Returns: Tuple: - `classification_loss`: A scalar in tf.float32 - `localization_loss`: A scalar in tf.float32 \"\"\" localization_pred = tf . concat ( localization_pred , 1 ) classification_pred = tf . concat ( classification_pred , 1 ) anchors = tf . concat ( anchors , 0 ) ground_truths = { # We add one because the background is not counted in ground_truths[BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField . BOXES ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } # anchors are deterministic duplicate them to create a batch anchors = tf . tile ( anchors [ None ], ( tf . shape ( ground_truths [ BoxField . BOXES ])[ 0 ], 1 , 1 )) y_true , weights = self . target_assigner . assign ({ BoxField . BOXES : anchors }, ground_truths ) y_true [ BoxField . LABELS ] = tf . minimum ( y_true [ BoxField . LABELS ], 1 ) ## Compute metrics recall = compute_rpn_metrics ( y_true [ BoxField . LABELS ], classification_pred , weights [ BoxField . LABELS ]) self . add_metric ( recall , name = 'rpn_recall' , aggregation = 'mean' ) # All the boxes which are not -1 can be sampled labels = y_true [ BoxField . LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField . LABELS ], SAMPLING_SIZE , labels , positive_fraction = SAMPLING_POSITIVE_RATIO , dtype = self . _compute_dtype ) weights [ BoxField . LABELS ] = sample_idx * weights [ BoxField . LABELS ] weights [ BoxField . BOXES ] = sample_idx * weights [ BoxField . BOXES ] y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : localization_pred } return self . compute_losses ( y_true , y_pred , weights ) def get_config ( self ): base_config = super () . get_config () base_config [ 'anchor_ratios' ] = self . _anchor_ratios return base_config def compute_rpn_metrics ( y_true : tf . Tensor , y_pred : tf . Tensor , weights : tf . Tensor ): \"\"\"Useful metrics that allows to track how behave the training of the rpn head. Arguments: y_true: A tensor vector with shape [batch_size, num_anchors] where 0 = background and 1 = foreground. y_pred: A tensor of shape [batch_size, num_anchors, 2], representing the classification logits. weights: A tensor of shape [batch_size, num_anchors] where weights should Returns: tf.Tensor: Recall, among all the boxes that we had to find how much did we found. \"\"\" # Force the cast to avoid type issue when the mixed precision is activated y_true , y_pred , weights = tf . cast ( y_true , tf . float32 ), tf . cast ( y_pred , tf . float32 ), tf . cast ( weights , tf . float32 ) # Sometimes the weights have decimal value we do not want that weights = tf . clip_by_value ( tf . math . ceil ( weights ), 0 , 1 ) masked_y_true = y_true * weights prediction = tf . cast ( tf . argmax ( y_pred , axis =- 1 , name = 'label_prediction' ), tf . float32 ) * weights # 0 or 1 correct = tf . cast ( tf . equal ( prediction , masked_y_true ), tf . float32 ) fg_inds = tf . where ( masked_y_true == 1 ) num_valid_anchor = tf . math . count_nonzero ( masked_y_true ) num_pos_foreground_prediction = tf . math . count_nonzero ( tf . gather_nd ( correct , fg_inds )) recall = tf . truediv ( num_pos_foreground_prediction , num_valid_anchor , name = 'recall' ) return recall remove_unwanted_doc ( RegionProposalNetwork , __pdoc__ ) Variables SAMPLING_POSITIVE_RATIO SAMPLING_SIZE Functions compute_rpn_metrics def compute_rpn_metrics ( y_true : tensorflow . python . framework . ops . Tensor , y_pred : tensorflow . python . framework . ops . Tensor , weights : tensorflow . python . framework . ops . Tensor ) Useful metrics that allows to track how behave the training of the rpn head. Parameters: Name Description y_true A tensor vector with shape [batch_size, num_anchors] where 0 = background and 1 = foreground. y_pred A tensor of shape [batch_size, num_anchors, 2], representing the classification logits. weights A tensor of shape [batch_size, num_anchors] where weights should Returns: Type Description tf.Tensor Recall, among all the boxes that we had to find how much did we found. View Source def compute_rpn_metrics ( y_true: tf . Tensor , y_pred: tf . Tensor , weights: tf . Tensor ) : \"\"\"Useful metrics that allows to track how behave the training of the rpn head. Arguments: y_true: A tensor vector with shape [ batch_size , num_anchors ] where 0 = background and 1 = foreground . y_pred: A tensor of shape [ batch_size , num_anchors , 2 ], representing the classification logits . weights: A tensor of shape [ batch_size , num_anchors ] where weights should Returns: tf . Tensor: Recall , among all the boxes that we had to find how much did we found . \"\"\" # Force the cast to avoid type issue when the mixed precision is activated y_true , y_pred , weights = tf . cast ( y_true , tf . float32 ), tf . cast ( y_pred , tf . float32 ), tf . cast ( weights , tf . float32 ) # Sometimes the weights have decimal value we do not want that weights = tf . clip_by_value ( tf . math . ceil ( weights ), 0 , 1 ) masked_y_true = y_true * weights prediction = tf . cast ( tf . argmax ( y_pred , axis =- 1 , name = ' label_prediction '), tf . float32 ) * weights # 0 or 1 correct = tf . cast ( tf . equal ( prediction , masked_y_true ), tf . float32 ) fg_inds = tf . where ( masked_y_true == 1 ) num_valid_anchor = tf . math . count_nonzero ( masked_y_true ) num_pos_foreground_prediction = tf . math . count_nonzero ( tf . gather_nd ( correct , fg_inds )) recall = tf . truediv ( num_pos_foreground_prediction , num_valid_anchor , name = ' recall ') return recall Classes RegionProposalNetwork class RegionProposalNetwork ( anchor_ratios = ( 0.5 , 1 , 2 ), ** kwargs ) use the parameters from Feature Pyramidal Networks for Object Detection . Arguments Name Description anchor_ratios The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) Call arguments Name Description inputs A List of tensors the output of the pyramid Call returns Type Description Tuple - localization_pred : A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - classification_pred : A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - anchors : A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] Ancestors (in MRO) kerod.layers.detection.abstract_detection_head.AbstractDetectionHead keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector Methods build_detection_head def build_detection_head ( self , inputs ) Build a detection head composed of a classification and box_detection. Parameters: Name Description inputs A tensor of shape [batch_size, H, W, C] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_detection_head ( self , inputs ) : \"\"\" Build a detection head composed of a classification and box_detection. Arguments : inputs : A tensor of shape [ batch_size , H , W , C ] Returns : Tuple : classification_head : a tensor of shape [ batch_size , num_anchors , 2 ] localization_head : a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head build_rpn_head def build_rpn_head ( self , inputs ) Predictions for the classification and the regression Parameters: Name Description inputs A tensor of shape [batch_size, width, height, channel] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_rpn_head ( self , inputs ) : \"\"\"Predictions for the classification and the regression Arguments: inputs: A tensor of shape [ batch_size , width , height , channel ] Returns: Tuple: classification_head: a tensor of shape [ batch_size , num_anchors , 2 ] localization_head: a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" batch_size = tf . shape ( inputs )[ 0 ] rpn_conv2d = self . rpn_conv2d ( inputs ) classification_head , localization_head = self . build_detection_head ( rpn_conv2d ) classification_head = tf . reshape ( classification_head , ( batch_size , - 1 , 2 )) localization_head = tf . reshape ( localization_head , ( batch_size , - 1 , 4 )) return classification_head , localization_head build_segmentation_head def build_segmentation_head ( self , inputs ) Build the detection head Parameters: Name Description inputs A tensor of float and shape [N, H, W, C] Returns: Type Description tf.Tensor A tensor and shape [N, H 2, W 2, num_classes - 1] View Source def build_segmentation_head ( self , inputs ) : \"\"\" Build the detection head Arguments : inputs : A tensor of float and shape [ N , H , W , C ] Returns : tf . Tensor : A tensor and shape [ N , H * 2 , W * 2 , num_classes - 1 ] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x call def call ( self , inputs : List [ tensorflow . python . framework . ops . Tensor ] ) Create the computation graph for the rpn inference Parameters: Name Description inputs A List of tensors the output of the pyramid Returns: Type Description Tuple - localization_pred : A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - classification_pred : A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - anchors : A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] View Source def call ( self , inputs : List [ tf . Tensor ] ) : \" \"\" Create the computation graph for the rpn inference Arguments: inputs: A List of tensors the output of the pyramid Returns: Tuple: - `localization_pred`: A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - `classification_pred`: A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - `anchors`: A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] \"\" \" anchors = [ anchors ( tensor ) for tensor , anchors in zip ( inputs , self . _anchors ) ] rpn_predictions = [ self . build_rpn_head ( tensor ) for tensor in inputs ] localization_pred = [ prediction [ 1 ] for prediction in rpn_predictions ] classification_pred = [ prediction [ 0 ] for prediction in rpn_predictions ] return localization_pred , classification_pred , anchors compute_loss def compute_loss ( self , localization_pred , classification_pred , anchors , ground_truths ) Compute the loss Parameters: Name Description localization_pred A list of tensors of shape [batch_size, num_anchors, 4]. classification_pred A list of tensors of shape [batch_size, num_anchors, 2] anchors A list of tensors of shape [num_anchors, (y_min, x_min, y_max, x_max)] ground_truths A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 Returns: Type Description Tuple - classification_loss : A scalar in tf.float32 - localization_loss : A scalar in tf.float32 View Source def compute_loss ( self , localization_pred , classification_pred , anchors , ground_truths ) : \" \"\" Compute the loss Arguments: localization_pred: A list of tensors of shape [batch_size, num_anchors, 4]. classification_pred: A list of tensors of shape [batch_size, num_anchors, 2] anchors: A list of tensors of shape [num_anchors, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 Returns: Tuple: - `classification_loss`: A scalar in tf.float32 - `localization_loss`: A scalar in tf.float32 \"\" \" localization_pred = tf . concat ( localization_pred , 1 ) classification_pred = tf . concat ( classification_pred , 1 ) anchors = tf . concat ( anchors , 0 ) ground_truths = { # We add one because the background is not counted in ground_truths[BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField . BOXES ] , BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } # anchors are deterministic duplicate them to create a batch anchors = tf . tile ( anchors [ None ] , ( tf . shape ( ground_truths [ BoxField . BOXES ] ) [ 0 ] , 1 , 1 )) y_true , weights = self . target_assigner . assign ( { BoxField . BOXES : anchors } , ground_truths ) y_true [ BoxField . LABELS ] = tf . minimum ( y_true [ BoxField . LABELS ] , 1 ) ## Compute metrics recall = compute_rpn_metrics ( y_true [ BoxField . LABELS ] , classification_pred , weights [ BoxField . LABELS ] ) self . add_metric ( recall , name = 'rpn_recall' , aggregation = 'mean' ) # All the boxes which are not -1 can be sampled labels = y_true [ BoxField . LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField . LABELS ] , SAMPLING_SIZE , labels , positive_fraction = SAMPLING_POSITIVE_RATIO , dtype = self . _compute_dtype ) weights [ BoxField . LABELS ] = sample_idx * weights [ BoxField . LABELS ] weights [ BoxField . BOXES ] = sample_idx * weights [ BoxField . BOXES ] y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : localization_pred } return self . compute_losses ( y_true , y_pred , weights ) compute_losses def compute_losses ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], weights : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> dict Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Parameters: Name Description y_pred A dict of tensors of shape [N, nb_boxes, num_output]. y_true A dict of tensors of shape [N, nb_boxes, num_output]. weights A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: Type Description None dict : A dict of different losses View Source def compute_losses ( self , y_true : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , weights : Dict [ str, tf.Tensor ] ) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ) : losses = loss ( tf . cast ( y_true [ target ] , tf . float32 ), tf . cast ( y_pred [ target ] , tf . float32 ), sample_weight = tf . cast ( weights [ target ] , tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField.LABELS ] , axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f '{self.name}_classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f '{self.name}_localization_loss' , aggregation = 'mean' ) self . add_loss ( [ classification_loss, localization_loss ] ) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f '{self.name}_segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss }","title":"Rpn"},{"location":"reference/kerod/layers/detection/rpn/#module-kerodlayersdetectionrpn","text":"None None View Source from typing import List import tensorflow as tf import tensorflow.keras.layers as KL from tensorflow.keras import initializers from tensorflow.keras.losses import SparseCategoricalCrossentropy from kerod.core.box_coder import encode_boxes_faster_rcnn from kerod.core.losses import L1Loss from kerod.core.matcher import Matcher from kerod.core.sampling_ops import batch_sample_balanced_positive_negative from kerod.core.similarity import IoUSimilarity from kerod.core.standard_fields import BoxField from kerod.core.target_assigner import TargetAssigner from kerod.layers import Anchors from kerod.layers.detection.abstract_detection_head import \\ AbstractDetectionHead from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} SAMPLING_SIZE = 256 SAMPLING_POSITIVE_RATIO = 0.5 class RegionProposalNetwork ( AbstractDetectionHead ): \"\"\"It has been introduced in the [Faster R-CNN paper](https://arxiv.org/abs/1506.01497) and use the parameters from [Feature Pyramidal Networks for Object Detection](https://arxiv.org/abs/1612.03144). Arguments: anchor_ratios: The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) Call arguments: inputs: A List of tensors the output of the pyramid Call returns: Tuple: - `localization_pred`: A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - `classification_pred`: A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - `anchors`: A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] \"\"\" def __init__ ( self , anchor_ratios = ( 0.5 , 1 , 2 ), ** kwargs ): super () . __init__ ( 2 , SparseCategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE , from_logits = True ), L1Loss ( reduction = tf . keras . losses . Reduction . NONE ), multiples = len ( anchor_ratios ), kernel_initializer_classification_head = initializers . RandomNormal ( stddev = 0.01 ), kernel_initializer_box_prediction_head = initializers . RandomNormal ( stddev = 0.01 ), ** kwargs ) #Force each ground_truths to match to at least one anchor matcher = Matcher ([ 0.3 , 0.7 ], [ 0 , - 1 , 1 ], allow_low_quality_matches = True ) self . target_assigner = TargetAssigner ( IoUSimilarity (), matcher , encode_boxes_faster_rcnn , dtype = self . _compute_dtype ) anchor_strides = ( 4 , 8 , 16 , 32 , 64 ) anchor_zises = ( 32 , 64 , 128 , 256 , 512 ) self . _anchor_ratios = anchor_ratios # Precompute a deterministic grid of anchors for each layer of the pyramid. # We will extract a subpart of the anchors according to self . _anchors = [ Anchors ( stride , size , self . _anchor_ratios ) for stride , size in zip ( anchor_strides , anchor_zises ) ] def build ( self , input_shape ): self . rpn_conv2d = KL . Conv2D ( 512 , ( 3 , 3 ), padding = 'same' , kernel_initializer = self . _kernel_initializer_classification_head , kernel_regularizer = self . _kernel_regularizer ) super () . build ( input_shape ) def build_rpn_head ( self , inputs ): \"\"\"Predictions for the classification and the regression Arguments: inputs: A tensor of shape [batch_size, width, height, channel] Returns: Tuple: classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] \"\"\" batch_size = tf . shape ( inputs )[ 0 ] rpn_conv2d = self . rpn_conv2d ( inputs ) classification_head , localization_head = self . build_detection_head ( rpn_conv2d ) classification_head = tf . reshape ( classification_head , ( batch_size , - 1 , 2 )) localization_head = tf . reshape ( localization_head , ( batch_size , - 1 , 4 )) return classification_head , localization_head def call ( self , inputs : List [ tf . Tensor ]): \"\"\"Create the computation graph for the rpn inference Arguments: inputs: A List of tensors the output of the pyramid Returns: Tuple: - `localization_pred`: A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - `classification_pred`: A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - `anchors`: A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] \"\"\" anchors = [ anchors ( tensor ) for tensor , anchors in zip ( inputs , self . _anchors )] rpn_predictions = [ self . build_rpn_head ( tensor ) for tensor in inputs ] localization_pred = [ prediction [ 1 ] for prediction in rpn_predictions ] classification_pred = [ prediction [ 0 ] for prediction in rpn_predictions ] return localization_pred , classification_pred , anchors def compute_loss ( self , localization_pred , classification_pred , anchors , ground_truths ): \"\"\"Compute the loss Arguments: localization_pred: A list of tensors of shape [batch_size, num_anchors, 4]. classification_pred: A list of tensors of shape [batch_size, num_anchors, 2] anchors: A list of tensors of shape [num_anchors, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 Returns: Tuple: - `classification_loss`: A scalar in tf.float32 - `localization_loss`: A scalar in tf.float32 \"\"\" localization_pred = tf . concat ( localization_pred , 1 ) classification_pred = tf . concat ( classification_pred , 1 ) anchors = tf . concat ( anchors , 0 ) ground_truths = { # We add one because the background is not counted in ground_truths[BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField . BOXES ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } # anchors are deterministic duplicate them to create a batch anchors = tf . tile ( anchors [ None ], ( tf . shape ( ground_truths [ BoxField . BOXES ])[ 0 ], 1 , 1 )) y_true , weights = self . target_assigner . assign ({ BoxField . BOXES : anchors }, ground_truths ) y_true [ BoxField . LABELS ] = tf . minimum ( y_true [ BoxField . LABELS ], 1 ) ## Compute metrics recall = compute_rpn_metrics ( y_true [ BoxField . LABELS ], classification_pred , weights [ BoxField . LABELS ]) self . add_metric ( recall , name = 'rpn_recall' , aggregation = 'mean' ) # All the boxes which are not -1 can be sampled labels = y_true [ BoxField . LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField . LABELS ], SAMPLING_SIZE , labels , positive_fraction = SAMPLING_POSITIVE_RATIO , dtype = self . _compute_dtype ) weights [ BoxField . LABELS ] = sample_idx * weights [ BoxField . LABELS ] weights [ BoxField . BOXES ] = sample_idx * weights [ BoxField . BOXES ] y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : localization_pred } return self . compute_losses ( y_true , y_pred , weights ) def get_config ( self ): base_config = super () . get_config () base_config [ 'anchor_ratios' ] = self . _anchor_ratios return base_config def compute_rpn_metrics ( y_true : tf . Tensor , y_pred : tf . Tensor , weights : tf . Tensor ): \"\"\"Useful metrics that allows to track how behave the training of the rpn head. Arguments: y_true: A tensor vector with shape [batch_size, num_anchors] where 0 = background and 1 = foreground. y_pred: A tensor of shape [batch_size, num_anchors, 2], representing the classification logits. weights: A tensor of shape [batch_size, num_anchors] where weights should Returns: tf.Tensor: Recall, among all the boxes that we had to find how much did we found. \"\"\" # Force the cast to avoid type issue when the mixed precision is activated y_true , y_pred , weights = tf . cast ( y_true , tf . float32 ), tf . cast ( y_pred , tf . float32 ), tf . cast ( weights , tf . float32 ) # Sometimes the weights have decimal value we do not want that weights = tf . clip_by_value ( tf . math . ceil ( weights ), 0 , 1 ) masked_y_true = y_true * weights prediction = tf . cast ( tf . argmax ( y_pred , axis =- 1 , name = 'label_prediction' ), tf . float32 ) * weights # 0 or 1 correct = tf . cast ( tf . equal ( prediction , masked_y_true ), tf . float32 ) fg_inds = tf . where ( masked_y_true == 1 ) num_valid_anchor = tf . math . count_nonzero ( masked_y_true ) num_pos_foreground_prediction = tf . math . count_nonzero ( tf . gather_nd ( correct , fg_inds )) recall = tf . truediv ( num_pos_foreground_prediction , num_valid_anchor , name = 'recall' ) return recall remove_unwanted_doc ( RegionProposalNetwork , __pdoc__ )","title":"Module kerod.layers.detection.rpn"},{"location":"reference/kerod/layers/detection/rpn/#variables","text":"SAMPLING_POSITIVE_RATIO SAMPLING_SIZE","title":"Variables"},{"location":"reference/kerod/layers/detection/rpn/#functions","text":"","title":"Functions"},{"location":"reference/kerod/layers/detection/rpn/#compute_rpn_metrics","text":"def compute_rpn_metrics ( y_true : tensorflow . python . framework . ops . Tensor , y_pred : tensorflow . python . framework . ops . Tensor , weights : tensorflow . python . framework . ops . Tensor ) Useful metrics that allows to track how behave the training of the rpn head. Parameters: Name Description y_true A tensor vector with shape [batch_size, num_anchors] where 0 = background and 1 = foreground. y_pred A tensor of shape [batch_size, num_anchors, 2], representing the classification logits. weights A tensor of shape [batch_size, num_anchors] where weights should Returns: Type Description tf.Tensor Recall, among all the boxes that we had to find how much did we found. View Source def compute_rpn_metrics ( y_true: tf . Tensor , y_pred: tf . Tensor , weights: tf . Tensor ) : \"\"\"Useful metrics that allows to track how behave the training of the rpn head. Arguments: y_true: A tensor vector with shape [ batch_size , num_anchors ] where 0 = background and 1 = foreground . y_pred: A tensor of shape [ batch_size , num_anchors , 2 ], representing the classification logits . weights: A tensor of shape [ batch_size , num_anchors ] where weights should Returns: tf . Tensor: Recall , among all the boxes that we had to find how much did we found . \"\"\" # Force the cast to avoid type issue when the mixed precision is activated y_true , y_pred , weights = tf . cast ( y_true , tf . float32 ), tf . cast ( y_pred , tf . float32 ), tf . cast ( weights , tf . float32 ) # Sometimes the weights have decimal value we do not want that weights = tf . clip_by_value ( tf . math . ceil ( weights ), 0 , 1 ) masked_y_true = y_true * weights prediction = tf . cast ( tf . argmax ( y_pred , axis =- 1 , name = ' label_prediction '), tf . float32 ) * weights # 0 or 1 correct = tf . cast ( tf . equal ( prediction , masked_y_true ), tf . float32 ) fg_inds = tf . where ( masked_y_true == 1 ) num_valid_anchor = tf . math . count_nonzero ( masked_y_true ) num_pos_foreground_prediction = tf . math . count_nonzero ( tf . gather_nd ( correct , fg_inds )) recall = tf . truediv ( num_pos_foreground_prediction , num_valid_anchor , name = ' recall ') return recall","title":"compute_rpn_metrics"},{"location":"reference/kerod/layers/detection/rpn/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/detection/rpn/#regionproposalnetwork","text":"class RegionProposalNetwork ( anchor_ratios = ( 0.5 , 1 , 2 ), ** kwargs ) use the parameters from Feature Pyramidal Networks for Object Detection .","title":"RegionProposalNetwork"},{"location":"reference/kerod/layers/detection/rpn/#arguments","text":"Name Description anchor_ratios The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2)","title":"Arguments"},{"location":"reference/kerod/layers/detection/rpn/#call-arguments","text":"Name Description inputs A List of tensors the output of the pyramid","title":"Call arguments"},{"location":"reference/kerod/layers/detection/rpn/#call-returns","text":"Type Description Tuple - localization_pred : A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - classification_pred : A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - anchors : A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)]","title":"Call returns"},{"location":"reference/kerod/layers/detection/rpn/#ancestors-in-mro","text":"kerod.layers.detection.abstract_detection_head.AbstractDetectionHead keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/detection/rpn/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/detection/rpn/#build_detection_head","text":"def build_detection_head ( self , inputs ) Build a detection head composed of a classification and box_detection. Parameters: Name Description inputs A tensor of shape [batch_size, H, W, C] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_detection_head ( self , inputs ) : \"\"\" Build a detection head composed of a classification and box_detection. Arguments : inputs : A tensor of shape [ batch_size , H , W , C ] Returns : Tuple : classification_head : a tensor of shape [ batch_size , num_anchors , 2 ] localization_head : a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head","title":"build_detection_head"},{"location":"reference/kerod/layers/detection/rpn/#build_rpn_head","text":"def build_rpn_head ( self , inputs ) Predictions for the classification and the regression Parameters: Name Description inputs A tensor of shape [batch_size, width, height, channel] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_rpn_head ( self , inputs ) : \"\"\"Predictions for the classification and the regression Arguments: inputs: A tensor of shape [ batch_size , width , height , channel ] Returns: Tuple: classification_head: a tensor of shape [ batch_size , num_anchors , 2 ] localization_head: a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" batch_size = tf . shape ( inputs )[ 0 ] rpn_conv2d = self . rpn_conv2d ( inputs ) classification_head , localization_head = self . build_detection_head ( rpn_conv2d ) classification_head = tf . reshape ( classification_head , ( batch_size , - 1 , 2 )) localization_head = tf . reshape ( localization_head , ( batch_size , - 1 , 4 )) return classification_head , localization_head","title":"build_rpn_head"},{"location":"reference/kerod/layers/detection/rpn/#build_segmentation_head","text":"def build_segmentation_head ( self , inputs ) Build the detection head Parameters: Name Description inputs A tensor of float and shape [N, H, W, C] Returns: Type Description tf.Tensor A tensor and shape [N, H 2, W 2, num_classes - 1] View Source def build_segmentation_head ( self , inputs ) : \"\"\" Build the detection head Arguments : inputs : A tensor of float and shape [ N , H , W , C ] Returns : tf . Tensor : A tensor and shape [ N , H * 2 , W * 2 , num_classes - 1 ] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x","title":"build_segmentation_head"},{"location":"reference/kerod/layers/detection/rpn/#call","text":"def call ( self , inputs : List [ tensorflow . python . framework . ops . Tensor ] ) Create the computation graph for the rpn inference Parameters: Name Description inputs A List of tensors the output of the pyramid Returns: Type Description Tuple - localization_pred : A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - classification_pred : A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - anchors : A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] View Source def call ( self , inputs : List [ tf . Tensor ] ) : \" \"\" Create the computation graph for the rpn inference Arguments: inputs: A List of tensors the output of the pyramid Returns: Tuple: - `localization_pred`: A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - `classification_pred`: A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - `anchors`: A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] \"\" \" anchors = [ anchors ( tensor ) for tensor , anchors in zip ( inputs , self . _anchors ) ] rpn_predictions = [ self . build_rpn_head ( tensor ) for tensor in inputs ] localization_pred = [ prediction [ 1 ] for prediction in rpn_predictions ] classification_pred = [ prediction [ 0 ] for prediction in rpn_predictions ] return localization_pred , classification_pred , anchors","title":"call"},{"location":"reference/kerod/layers/detection/rpn/#compute_loss","text":"def compute_loss ( self , localization_pred , classification_pred , anchors , ground_truths ) Compute the loss Parameters: Name Description localization_pred A list of tensors of shape [batch_size, num_anchors, 4]. classification_pred A list of tensors of shape [batch_size, num_anchors, 2] anchors A list of tensors of shape [num_anchors, (y_min, x_min, y_max, x_max)] ground_truths A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 Returns: Type Description Tuple - classification_loss : A scalar in tf.float32 - localization_loss : A scalar in tf.float32 View Source def compute_loss ( self , localization_pred , classification_pred , anchors , ground_truths ) : \" \"\" Compute the loss Arguments: localization_pred: A list of tensors of shape [batch_size, num_anchors, 4]. classification_pred: A list of tensors of shape [batch_size, num_anchors, 2] anchors: A list of tensors of shape [num_anchors, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 Returns: Tuple: - `classification_loss`: A scalar in tf.float32 - `localization_loss`: A scalar in tf.float32 \"\" \" localization_pred = tf . concat ( localization_pred , 1 ) classification_pred = tf . concat ( classification_pred , 1 ) anchors = tf . concat ( anchors , 0 ) ground_truths = { # We add one because the background is not counted in ground_truths[BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField . BOXES ] , BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } # anchors are deterministic duplicate them to create a batch anchors = tf . tile ( anchors [ None ] , ( tf . shape ( ground_truths [ BoxField . BOXES ] ) [ 0 ] , 1 , 1 )) y_true , weights = self . target_assigner . assign ( { BoxField . BOXES : anchors } , ground_truths ) y_true [ BoxField . LABELS ] = tf . minimum ( y_true [ BoxField . LABELS ] , 1 ) ## Compute metrics recall = compute_rpn_metrics ( y_true [ BoxField . LABELS ] , classification_pred , weights [ BoxField . LABELS ] ) self . add_metric ( recall , name = 'rpn_recall' , aggregation = 'mean' ) # All the boxes which are not -1 can be sampled labels = y_true [ BoxField . LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField . LABELS ] , SAMPLING_SIZE , labels , positive_fraction = SAMPLING_POSITIVE_RATIO , dtype = self . _compute_dtype ) weights [ BoxField . LABELS ] = sample_idx * weights [ BoxField . LABELS ] weights [ BoxField . BOXES ] = sample_idx * weights [ BoxField . BOXES ] y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : localization_pred } return self . compute_losses ( y_true , y_pred , weights )","title":"compute_loss"},{"location":"reference/kerod/layers/detection/rpn/#compute_losses","text":"def compute_losses ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], weights : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> dict Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Parameters: Name Description y_pred A dict of tensors of shape [N, nb_boxes, num_output]. y_true A dict of tensors of shape [N, nb_boxes, num_output]. weights A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: Type Description None dict : A dict of different losses View Source def compute_losses ( self , y_true : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , weights : Dict [ str, tf.Tensor ] ) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ) : losses = loss ( tf . cast ( y_true [ target ] , tf . float32 ), tf . cast ( y_pred [ target ] , tf . float32 ), sample_weight = tf . cast ( weights [ target ] , tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField.LABELS ] , axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f '{self.name}_classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f '{self.name}_localization_loss' , aggregation = 'mean' ) self . add_loss ( [ classification_loss, localization_loss ] ) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f '{self.name}_segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss }","title":"compute_losses"},{"location":"reference/kerod/layers/post_processing/","text":"Module kerod.layers.post_processing None None View Source from kerod.layers.post_processing.non_maximum_suppression import ( post_process_fast_rcnn_boxes , post_process_rpn ) Sub-modules kerod.layers.post_processing.non_maximum_suppression kerod.layers.post_processing.post_processing_detr","title":"Index"},{"location":"reference/kerod/layers/post_processing/#module-kerodlayerspost_processing","text":"None None View Source from kerod.layers.post_processing.non_maximum_suppression import ( post_process_fast_rcnn_boxes , post_process_rpn )","title":"Module kerod.layers.post_processing"},{"location":"reference/kerod/layers/post_processing/#sub-modules","text":"kerod.layers.post_processing.non_maximum_suppression kerod.layers.post_processing.post_processing_detr","title":"Sub-modules"},{"location":"reference/kerod/layers/post_processing/non_maximum_suppression/","text":"Module kerod.layers.post_processing.non_maximum_suppression Methods using non maximum_suppression to handle overlaps between boxes. None View Source \"\"\"Methods using non maximum_suppression to handle overlaps between boxes. \"\"\" from typing import List import tensorflow as tf from kerod.core.box_coder import decode_boxes_faster_rcnn from kerod.core.box_ops import clip_boxes from kerod.core.standard_fields import BoxField from kerod.utils.ops import get_full_indices def post_process_rpn ( cls_pred_per_lvl : List [ tf . Tensor ], loc_pred_per_lvl : List [ tf . Tensor ], anchors_per_lvl : List [ tf . Tensor ], image_information , pre_nms_topk_per_lvl , post_nms_topk = None , iou_threshold : float = 0.7 ): \"\"\"Sample RPN proposals by the following steps: 1. Pick top k1 by scores 2. NMS them 3. Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output. Arguments: - *cls_pred_per_lvl*: A list of Tensor of shape [batch_size, num_boxes, 2]. One item per level of the pyramid. - *loc_pred_per_lvl*: A list of Tensor of shape [batch_size, num_boxes, 4 * (num_anchors)]. One item per level of the pyramid. - *anchors_per_lvl*: A list of Tensor of shape [batch_size, num_boxes * num_anchors, 4] One item per level of the pyramid. - *image_informations*: A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. - *pre_nms_topk_per_lvl*: Will extract at each level this amount of boxes - post_nms_topk: Number of boxes selected after the nms Returns: - *nmsed_boxes*: A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. - *nmsed_scores*: A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. \"\"\" topk_boxes_per_lvl = [] topk_scores_per_lvl = [] for cls_pred , loc_pred , anchors in zip ( cls_pred_per_lvl , loc_pred_per_lvl , anchors_per_lvl ): batch_size = tf . shape ( cls_pred )[ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors ) boxes = clip_boxes ( boxes , image_information ) # Remove the background classes scores = cls_pred [:, :, 1 ] topk = tf . minimum ( pre_nms_topk_per_lvl , tf . size ( scores [ 0 ])) topk_scores , topk_indices = tf . math . top_k ( scores , k = topk , sorted = False ) topk_indices = get_full_indices ( topk_indices ) topk_boxes_per_lvl . append ( tf . cast ( tf . gather_nd ( boxes , topk_indices ), tf . float32 )) topk_scores_per_lvl . append ( tf . cast ( topk_scores , tf . float32 )) topk_boxes = tf . concat ( topk_boxes_per_lvl , 1 ) topk_scores = tf . concat ( topk_scores_per_lvl , 1 ) if post_nms_topk is None : post_nms_topk = pre_nms_topk_per_lvl nmsed_boxes , _ , _ , _ = tf . image . combined_non_max_suppression ( topk_boxes [:, :, None ], topk_scores [ ... , None ], post_nms_topk , post_nms_topk , iou_threshold = iou_threshold , score_threshold = 0 , clip_boxes = False ) return tf . stop_gradient ( nmsed_boxes ) def post_process_fast_rcnn_boxes ( cls_pred : tf . Tensor , loc_pred : tf . Tensor , anchors : tf . Tensor , image_information : tf . Tensor , num_classes , max_output_size_per_class : int = 100 , max_total_size : int = 100 , iou_threshold : float = 0.5 , score_threshold : float = 0.05 ): \"\"\"This is the classical post_processing for the Faster RCNN paper. This operation performs non_max_suppression on the inputs per batch, across all classes. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Also note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is the final boxes, scores and classes tensor returned after performing non_max_suppression. Arguments: - *cls_pred*: A Tensor of shape [batch_size, num_boxes, num_classes] - *loc_pred*: A Tensor of shape [batch_size, num_boxes, 4 * num_classes] - *anchors*: A Tensor of shape [batch_size, num_boxes, 4] - *image_informations*: A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. - *num_classes*: The number of classes (background is not included). - *max_output_size_per_class*: A scalar integer `Tensor` representing the maximum number of boxes to be selected by non max suppression per class - *max_total_size*: A scalar representing maximum number of boxes retained over all classes. - *iou_threshold*: A float representing the threshold for deciding whether boxes overlap too much with respect to IOU. - *score_threshold*: A float representing the threshold for deciding when to remove boxes based on score. `O.05` is used like in Detectron or Tensorpack. Returns: - *nmsed_boxes*: A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. - *nmsed_scores*: A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. - *nmsed_classes*: A Tensor of shape [batch_size, max_detections] containing the class for boxes. - *valid_detections*: A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings. \"\"\" batch_size = tf . shape ( cls_pred )[ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) anchors = tf . reshape ( tf . tile ( anchors , [ 1 , 1 , num_classes ]), ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors , scale_factors = ( 10.0 , 10.0 , 5.0 , 5.0 )) boxes = clip_boxes ( boxes , image_information ) boxes = tf . reshape ( boxes , ( batch_size , - 1 , num_classes , 4 )) nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections = tf . image . combined_non_max_suppression ( tf . cast ( boxes , tf . float32 ), tf . cast ( cls_pred , tf . float32 ), max_output_size_per_class , max_total_size , iou_threshold = iou_threshold , score_threshold = score_threshold , clip_boxes = False ) normalizer_boxes = tf . tile ( image_information [:, None ], [ 1 , 1 , 2 ]) nmsed_boxes = tf . math . divide ( nmsed_boxes , normalizer_boxes , name = BoxField . BOXES ) nmsed_scores = tf . identity ( nmsed_scores , name = BoxField . SCORES ) nmsed_labels = tf . identity ( nmsed_labels , name = BoxField . LABELS ) valid_detections = tf . identity ( valid_detections , name = BoxField . NUM_BOXES ) return nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections Functions post_process_fast_rcnn_boxes def post_process_fast_rcnn_boxes ( cls_pred : tensorflow . python . framework . ops . Tensor , loc_pred : tensorflow . python . framework . ops . Tensor , anchors : tensorflow . python . framework . ops . Tensor , image_information : tensorflow . python . framework . ops . Tensor , num_classes , max_output_size_per_class : int = 100 , max_total_size : int = 100 , iou_threshold : float = 0.5 , score_threshold : float = 0.05 ) This is the classical post_processing for the Faster RCNN paper. This operation performs non_max_suppression on the inputs per batch, across all classes. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Also note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is the final boxes, scores and classes tensor returned after performing non_max_suppression. Arguments: cls_pred : A Tensor of shape [batch_size, num_boxes, num_classes] loc_pred : A Tensor of shape [batch_size, num_boxes, 4 * num_classes] anchors : A Tensor of shape [batch_size, num_boxes, 4] image_informations : A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. num_classes : The number of classes (background is not included). max_output_size_per_class : A scalar integer Tensor representing the maximum number of boxes to be selected by non max suppression per class max_total_size : A scalar representing maximum number of boxes retained over all classes. iou_threshold : A float representing the threshold for deciding whether boxes overlap too much with respect to IOU. score_threshold : A float representing the threshold for deciding when to remove boxes based on score. O.05 is used like in Detectron or Tensorpack. Returns: nmsed_boxes : A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. nmsed_scores : A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. nmsed_classes : A Tensor of shape [batch_size, max_detections] containing the class for boxes. valid_detections : A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings. View Source def post_process_fast_rcnn_boxes ( cls_pred : tf . Tensor , loc_pred : tf . Tensor , anchors : tf . Tensor , image_information : tf . Tensor , num_classes , max_output_size_per_class : int = 100 , max_total_size : int = 100 , iou_threshold : float = 0.5 , score_threshold : float = 0.05 ) : \"\"\"This is the classical post_processing for the Faster RCNN paper. This operation performs non_max_suppression on the inputs per batch, across all classes. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Also note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is the final boxes, scores and classes tensor returned after performing non_max_suppression. Arguments: - *cls_pred*: A Tensor of shape [batch_size, num_boxes, num_classes] - *loc_pred*: A Tensor of shape [batch_size, num_boxes, 4 * num_classes] - *anchors*: A Tensor of shape [batch_size, num_boxes, 4] - *image_informations*: A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. - *num_classes*: The number of classes (background is not included). - *max_output_size_per_class*: A scalar integer `Tensor` representing the maximum number of boxes to be selected by non max suppression per class - *max_total_size*: A scalar representing maximum number of boxes retained over all classes. - *iou_threshold*: A float representing the threshold for deciding whether boxes overlap too much with respect to IOU. - *score_threshold*: A float representing the threshold for deciding when to remove boxes based on score. `O.05` is used like in Detectron or Tensorpack. Returns: - *nmsed_boxes*: A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. - *nmsed_scores*: A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. - *nmsed_classes*: A Tensor of shape [batch_size, max_detections] containing the class for boxes. - *valid_detections*: A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings. \"\"\" batch_size = tf . shape ( cls_pred ) [ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) anchors = tf . reshape ( tf . tile ( anchors , [ 1, 1, num_classes ] ), ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors , scale_factors = ( 10.0 , 10.0 , 5.0 , 5.0 )) boxes = clip_boxes ( boxes , image_information ) boxes = tf . reshape ( boxes , ( batch_size , - 1 , num_classes , 4 )) nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections = tf . image . combined_non_max_suppression ( tf . cast ( boxes , tf . float32 ), tf . cast ( cls_pred , tf . float32 ), max_output_size_per_class , max_total_size , iou_threshold = iou_threshold , score_threshold = score_threshold , clip_boxes = False ) normalizer_boxes = tf . tile ( image_information [ :, None ] , [ 1, 1, 2 ] ) nmsed_boxes = tf . math . divide ( nmsed_boxes , normalizer_boxes , name = BoxField . BOXES ) nmsed_scores = tf . identity ( nmsed_scores , name = BoxField . SCORES ) nmsed_labels = tf . identity ( nmsed_labels , name = BoxField . LABELS ) valid_detections = tf . identity ( valid_detections , name = BoxField . NUM_BOXES ) return nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections post_process_rpn def post_process_rpn ( cls_pred_per_lvl : List [ tensorflow . python . framework . ops . Tensor ], loc_pred_per_lvl : List [ tensorflow . python . framework . ops . Tensor ], anchors_per_lvl : List [ tensorflow . python . framework . ops . Tensor ], image_information , pre_nms_topk_per_lvl , post_nms_topk = None , iou_threshold : float = 0.7 ) Sample RPN proposals by the following steps: Pick top k1 by scores NMS them Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output. Arguments: cls_pred_per_lvl : A list of Tensor of shape [batch_size, num_boxes, 2]. One item per level of the pyramid. loc_pred_per_lvl : A list of Tensor of shape [batch_size, num_boxes, 4 * (num_anchors)]. One item per level of the pyramid. anchors_per_lvl : A list of Tensor of shape [batch_size, num_boxes * num_anchors, 4] One item per level of the pyramid. image_informations : A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. pre_nms_topk_per_lvl : Will extract at each level this amount of boxes post_nms_topk: Number of boxes selected after the nms Returns: nmsed_boxes : A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. nmsed_scores : A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. View Source def post_process_rpn ( cls_pred_per_lvl : List [ tf . Tensor ], loc_pred_per_lvl : List [ tf . Tensor ], anchors_per_lvl : List [ tf . Tensor ], image_information , pre_nms_topk_per_lvl , post_nms_topk = None , iou_threshold : float = 0 . 7 ) : \"\"\" Sample RPN proposals by the following steps: 1 . Pick top k1 by scores 2 . NMS them 3 . Pick top k2 by scores . Default k2 == k1 , i . e . does not filter the NMS output . Arguments : - * cls_pred_per_lvl * : A list of Tensor of shape [ batch_size , num_boxes , 2 ]. One item per level of the pyramid . - * loc_pred_per_lvl * : A list of Tensor of shape [ batch_size , num_boxes , 4 * ( num_anchors ) ]. One item per level of the pyramid . - * anchors_per_lvl * : A list of Tensor of shape [ batch_size , num_boxes * num_anchors , 4 ] One item per level of the pyramid . - * image_informations * : A Tensor of shape [ batch_size , ( height , width ) ] The height and the width are without the padding . - * pre_nms_topk_per_lvl * : Will extract at each level this amount of boxes - post_nms_topk : Number of boxes selected after the nms Returns : - * nmsed_boxes * : A Tensor of shape [ batch_size , max_detections , 4 ] containing the non - max suppressed boxes . - * nmsed_scores * : A Tensor of shape [ batch_size , max_detections ] containing the scores for the boxes . \"\"\" topk_boxes_per_lvl = [] topk_scores_per_lvl = [] for cls_pred , loc_pred , anchors in zip ( cls_pred_per_lvl , loc_pred_per_lvl , anchors_per_lvl ) : batch_size = tf . shape ( cls_pred ) [ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors ) boxes = clip_boxes ( boxes , image_information ) # Remove the background classes scores = cls_pred [:, :, 1 ] topk = tf . minimum ( pre_nms_topk_per_lvl , tf . size ( scores [ 0 ] )) topk_scores , topk_indices = tf . math . top_k ( scores , k = topk , sorted = False ) topk_indices = get_full_indices ( topk_indices ) topk_boxes_per_lvl . append ( tf . cast ( tf . gather_nd ( boxes , topk_indices ) , tf . float32 )) topk_scores_per_lvl . append ( tf . cast ( topk_scores , tf . float32 )) topk_boxes = tf . concat ( topk_boxes_per_lvl , 1 ) topk_scores = tf . concat ( topk_scores_per_lvl , 1 ) if post_nms_topk is None : post_nms_topk = pre_nms_topk_per_lvl nmsed_boxes , _ , _ , _ = tf . image . combined_non_max_suppression ( topk_boxes [:, :, None ], topk_scores [..., None ], post_nms_topk , post_nms_topk , iou_threshold = iou_threshold , score_threshold = 0 , clip_boxes = False ) return tf . stop_gradient ( nmsed_boxes )","title":"Non Maximum Suppression"},{"location":"reference/kerod/layers/post_processing/non_maximum_suppression/#module-kerodlayerspost_processingnon_maximum_suppression","text":"Methods using non maximum_suppression to handle overlaps between boxes. None View Source \"\"\"Methods using non maximum_suppression to handle overlaps between boxes. \"\"\" from typing import List import tensorflow as tf from kerod.core.box_coder import decode_boxes_faster_rcnn from kerod.core.box_ops import clip_boxes from kerod.core.standard_fields import BoxField from kerod.utils.ops import get_full_indices def post_process_rpn ( cls_pred_per_lvl : List [ tf . Tensor ], loc_pred_per_lvl : List [ tf . Tensor ], anchors_per_lvl : List [ tf . Tensor ], image_information , pre_nms_topk_per_lvl , post_nms_topk = None , iou_threshold : float = 0.7 ): \"\"\"Sample RPN proposals by the following steps: 1. Pick top k1 by scores 2. NMS them 3. Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output. Arguments: - *cls_pred_per_lvl*: A list of Tensor of shape [batch_size, num_boxes, 2]. One item per level of the pyramid. - *loc_pred_per_lvl*: A list of Tensor of shape [batch_size, num_boxes, 4 * (num_anchors)]. One item per level of the pyramid. - *anchors_per_lvl*: A list of Tensor of shape [batch_size, num_boxes * num_anchors, 4] One item per level of the pyramid. - *image_informations*: A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. - *pre_nms_topk_per_lvl*: Will extract at each level this amount of boxes - post_nms_topk: Number of boxes selected after the nms Returns: - *nmsed_boxes*: A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. - *nmsed_scores*: A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. \"\"\" topk_boxes_per_lvl = [] topk_scores_per_lvl = [] for cls_pred , loc_pred , anchors in zip ( cls_pred_per_lvl , loc_pred_per_lvl , anchors_per_lvl ): batch_size = tf . shape ( cls_pred )[ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors ) boxes = clip_boxes ( boxes , image_information ) # Remove the background classes scores = cls_pred [:, :, 1 ] topk = tf . minimum ( pre_nms_topk_per_lvl , tf . size ( scores [ 0 ])) topk_scores , topk_indices = tf . math . top_k ( scores , k = topk , sorted = False ) topk_indices = get_full_indices ( topk_indices ) topk_boxes_per_lvl . append ( tf . cast ( tf . gather_nd ( boxes , topk_indices ), tf . float32 )) topk_scores_per_lvl . append ( tf . cast ( topk_scores , tf . float32 )) topk_boxes = tf . concat ( topk_boxes_per_lvl , 1 ) topk_scores = tf . concat ( topk_scores_per_lvl , 1 ) if post_nms_topk is None : post_nms_topk = pre_nms_topk_per_lvl nmsed_boxes , _ , _ , _ = tf . image . combined_non_max_suppression ( topk_boxes [:, :, None ], topk_scores [ ... , None ], post_nms_topk , post_nms_topk , iou_threshold = iou_threshold , score_threshold = 0 , clip_boxes = False ) return tf . stop_gradient ( nmsed_boxes ) def post_process_fast_rcnn_boxes ( cls_pred : tf . Tensor , loc_pred : tf . Tensor , anchors : tf . Tensor , image_information : tf . Tensor , num_classes , max_output_size_per_class : int = 100 , max_total_size : int = 100 , iou_threshold : float = 0.5 , score_threshold : float = 0.05 ): \"\"\"This is the classical post_processing for the Faster RCNN paper. This operation performs non_max_suppression on the inputs per batch, across all classes. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Also note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is the final boxes, scores and classes tensor returned after performing non_max_suppression. Arguments: - *cls_pred*: A Tensor of shape [batch_size, num_boxes, num_classes] - *loc_pred*: A Tensor of shape [batch_size, num_boxes, 4 * num_classes] - *anchors*: A Tensor of shape [batch_size, num_boxes, 4] - *image_informations*: A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. - *num_classes*: The number of classes (background is not included). - *max_output_size_per_class*: A scalar integer `Tensor` representing the maximum number of boxes to be selected by non max suppression per class - *max_total_size*: A scalar representing maximum number of boxes retained over all classes. - *iou_threshold*: A float representing the threshold for deciding whether boxes overlap too much with respect to IOU. - *score_threshold*: A float representing the threshold for deciding when to remove boxes based on score. `O.05` is used like in Detectron or Tensorpack. Returns: - *nmsed_boxes*: A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. - *nmsed_scores*: A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. - *nmsed_classes*: A Tensor of shape [batch_size, max_detections] containing the class for boxes. - *valid_detections*: A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings. \"\"\" batch_size = tf . shape ( cls_pred )[ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) anchors = tf . reshape ( tf . tile ( anchors , [ 1 , 1 , num_classes ]), ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors , scale_factors = ( 10.0 , 10.0 , 5.0 , 5.0 )) boxes = clip_boxes ( boxes , image_information ) boxes = tf . reshape ( boxes , ( batch_size , - 1 , num_classes , 4 )) nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections = tf . image . combined_non_max_suppression ( tf . cast ( boxes , tf . float32 ), tf . cast ( cls_pred , tf . float32 ), max_output_size_per_class , max_total_size , iou_threshold = iou_threshold , score_threshold = score_threshold , clip_boxes = False ) normalizer_boxes = tf . tile ( image_information [:, None ], [ 1 , 1 , 2 ]) nmsed_boxes = tf . math . divide ( nmsed_boxes , normalizer_boxes , name = BoxField . BOXES ) nmsed_scores = tf . identity ( nmsed_scores , name = BoxField . SCORES ) nmsed_labels = tf . identity ( nmsed_labels , name = BoxField . LABELS ) valid_detections = tf . identity ( valid_detections , name = BoxField . NUM_BOXES ) return nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections","title":"Module kerod.layers.post_processing.non_maximum_suppression"},{"location":"reference/kerod/layers/post_processing/non_maximum_suppression/#functions","text":"","title":"Functions"},{"location":"reference/kerod/layers/post_processing/non_maximum_suppression/#post_process_fast_rcnn_boxes","text":"def post_process_fast_rcnn_boxes ( cls_pred : tensorflow . python . framework . ops . Tensor , loc_pred : tensorflow . python . framework . ops . Tensor , anchors : tensorflow . python . framework . ops . Tensor , image_information : tensorflow . python . framework . ops . Tensor , num_classes , max_output_size_per_class : int = 100 , max_total_size : int = 100 , iou_threshold : float = 0.5 , score_threshold : float = 0.05 ) This is the classical post_processing for the Faster RCNN paper. This operation performs non_max_suppression on the inputs per batch, across all classes. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Also note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is the final boxes, scores and classes tensor returned after performing non_max_suppression. Arguments: cls_pred : A Tensor of shape [batch_size, num_boxes, num_classes] loc_pred : A Tensor of shape [batch_size, num_boxes, 4 * num_classes] anchors : A Tensor of shape [batch_size, num_boxes, 4] image_informations : A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. num_classes : The number of classes (background is not included). max_output_size_per_class : A scalar integer Tensor representing the maximum number of boxes to be selected by non max suppression per class max_total_size : A scalar representing maximum number of boxes retained over all classes. iou_threshold : A float representing the threshold for deciding whether boxes overlap too much with respect to IOU. score_threshold : A float representing the threshold for deciding when to remove boxes based on score. O.05 is used like in Detectron or Tensorpack. Returns: nmsed_boxes : A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. nmsed_scores : A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. nmsed_classes : A Tensor of shape [batch_size, max_detections] containing the class for boxes. valid_detections : A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings. View Source def post_process_fast_rcnn_boxes ( cls_pred : tf . Tensor , loc_pred : tf . Tensor , anchors : tf . Tensor , image_information : tf . Tensor , num_classes , max_output_size_per_class : int = 100 , max_total_size : int = 100 , iou_threshold : float = 0.5 , score_threshold : float = 0.05 ) : \"\"\"This is the classical post_processing for the Faster RCNN paper. This operation performs non_max_suppression on the inputs per batch, across all classes. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Also note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is the final boxes, scores and classes tensor returned after performing non_max_suppression. Arguments: - *cls_pred*: A Tensor of shape [batch_size, num_boxes, num_classes] - *loc_pred*: A Tensor of shape [batch_size, num_boxes, 4 * num_classes] - *anchors*: A Tensor of shape [batch_size, num_boxes, 4] - *image_informations*: A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. - *num_classes*: The number of classes (background is not included). - *max_output_size_per_class*: A scalar integer `Tensor` representing the maximum number of boxes to be selected by non max suppression per class - *max_total_size*: A scalar representing maximum number of boxes retained over all classes. - *iou_threshold*: A float representing the threshold for deciding whether boxes overlap too much with respect to IOU. - *score_threshold*: A float representing the threshold for deciding when to remove boxes based on score. `O.05` is used like in Detectron or Tensorpack. Returns: - *nmsed_boxes*: A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. - *nmsed_scores*: A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. - *nmsed_classes*: A Tensor of shape [batch_size, max_detections] containing the class for boxes. - *valid_detections*: A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings. \"\"\" batch_size = tf . shape ( cls_pred ) [ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) anchors = tf . reshape ( tf . tile ( anchors , [ 1, 1, num_classes ] ), ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors , scale_factors = ( 10.0 , 10.0 , 5.0 , 5.0 )) boxes = clip_boxes ( boxes , image_information ) boxes = tf . reshape ( boxes , ( batch_size , - 1 , num_classes , 4 )) nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections = tf . image . combined_non_max_suppression ( tf . cast ( boxes , tf . float32 ), tf . cast ( cls_pred , tf . float32 ), max_output_size_per_class , max_total_size , iou_threshold = iou_threshold , score_threshold = score_threshold , clip_boxes = False ) normalizer_boxes = tf . tile ( image_information [ :, None ] , [ 1, 1, 2 ] ) nmsed_boxes = tf . math . divide ( nmsed_boxes , normalizer_boxes , name = BoxField . BOXES ) nmsed_scores = tf . identity ( nmsed_scores , name = BoxField . SCORES ) nmsed_labels = tf . identity ( nmsed_labels , name = BoxField . LABELS ) valid_detections = tf . identity ( valid_detections , name = BoxField . NUM_BOXES ) return nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections","title":"post_process_fast_rcnn_boxes"},{"location":"reference/kerod/layers/post_processing/non_maximum_suppression/#post_process_rpn","text":"def post_process_rpn ( cls_pred_per_lvl : List [ tensorflow . python . framework . ops . Tensor ], loc_pred_per_lvl : List [ tensorflow . python . framework . ops . Tensor ], anchors_per_lvl : List [ tensorflow . python . framework . ops . Tensor ], image_information , pre_nms_topk_per_lvl , post_nms_topk = None , iou_threshold : float = 0.7 ) Sample RPN proposals by the following steps: Pick top k1 by scores NMS them Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output. Arguments: cls_pred_per_lvl : A list of Tensor of shape [batch_size, num_boxes, 2]. One item per level of the pyramid. loc_pred_per_lvl : A list of Tensor of shape [batch_size, num_boxes, 4 * (num_anchors)]. One item per level of the pyramid. anchors_per_lvl : A list of Tensor of shape [batch_size, num_boxes * num_anchors, 4] One item per level of the pyramid. image_informations : A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. pre_nms_topk_per_lvl : Will extract at each level this amount of boxes post_nms_topk: Number of boxes selected after the nms Returns: nmsed_boxes : A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. nmsed_scores : A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. View Source def post_process_rpn ( cls_pred_per_lvl : List [ tf . Tensor ], loc_pred_per_lvl : List [ tf . Tensor ], anchors_per_lvl : List [ tf . Tensor ], image_information , pre_nms_topk_per_lvl , post_nms_topk = None , iou_threshold : float = 0 . 7 ) : \"\"\" Sample RPN proposals by the following steps: 1 . Pick top k1 by scores 2 . NMS them 3 . Pick top k2 by scores . Default k2 == k1 , i . e . does not filter the NMS output . Arguments : - * cls_pred_per_lvl * : A list of Tensor of shape [ batch_size , num_boxes , 2 ]. One item per level of the pyramid . - * loc_pred_per_lvl * : A list of Tensor of shape [ batch_size , num_boxes , 4 * ( num_anchors ) ]. One item per level of the pyramid . - * anchors_per_lvl * : A list of Tensor of shape [ batch_size , num_boxes * num_anchors , 4 ] One item per level of the pyramid . - * image_informations * : A Tensor of shape [ batch_size , ( height , width ) ] The height and the width are without the padding . - * pre_nms_topk_per_lvl * : Will extract at each level this amount of boxes - post_nms_topk : Number of boxes selected after the nms Returns : - * nmsed_boxes * : A Tensor of shape [ batch_size , max_detections , 4 ] containing the non - max suppressed boxes . - * nmsed_scores * : A Tensor of shape [ batch_size , max_detections ] containing the scores for the boxes . \"\"\" topk_boxes_per_lvl = [] topk_scores_per_lvl = [] for cls_pred , loc_pred , anchors in zip ( cls_pred_per_lvl , loc_pred_per_lvl , anchors_per_lvl ) : batch_size = tf . shape ( cls_pred ) [ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors ) boxes = clip_boxes ( boxes , image_information ) # Remove the background classes scores = cls_pred [:, :, 1 ] topk = tf . minimum ( pre_nms_topk_per_lvl , tf . size ( scores [ 0 ] )) topk_scores , topk_indices = tf . math . top_k ( scores , k = topk , sorted = False ) topk_indices = get_full_indices ( topk_indices ) topk_boxes_per_lvl . append ( tf . cast ( tf . gather_nd ( boxes , topk_indices ) , tf . float32 )) topk_scores_per_lvl . append ( tf . cast ( topk_scores , tf . float32 )) topk_boxes = tf . concat ( topk_boxes_per_lvl , 1 ) topk_scores = tf . concat ( topk_scores_per_lvl , 1 ) if post_nms_topk is None : post_nms_topk = pre_nms_topk_per_lvl nmsed_boxes , _ , _ , _ = tf . image . combined_non_max_suppression ( topk_boxes [:, :, None ], topk_scores [..., None ], post_nms_topk , post_nms_topk , iou_threshold = iou_threshold , score_threshold = 0 , clip_boxes = False ) return tf . stop_gradient ( nmsed_boxes )","title":"post_process_rpn"},{"location":"reference/kerod/layers/post_processing/post_processing_detr/","text":"Module kerod.layers.post_processing.post_processing_detr None None View Source import tensorflow as tf from kerod.core.standard_fields import BoxField from kerod.utils.ops import get_full_indices from kerod.core.box_ops import convert_to_xyxy_coordinates def post_processing ( boxes : tf . Tensor , logits : tf . Tensor , image_information : tf . Tensor , image_padded_information : tf . Tensor , sorted = True ): \"\"\"PostProcessing described in the paper Object Detection with transformers \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) Arguments: - *logits*: A Tensor of shape [batch_size, num_queries, num_classes + 1] representing the class probability. - *localization_pred*: A Tensor of shape [batch_size, num_queries, (y_cent, x_cent, h, w)] - *image_information*: A 2-D tensor of float32 and shape [2, (height, width)]. It contains the shape of the image without any padding. - *image_padded_information*: A 2-D tensor of float32 and shape [(height_pad, width_pad)]. It contains the shape of the image without any padding. This padding is added during the dataset step when we batch the images together (padded_batch). - *sorted*: Return all the elements sorted by scores in descending order. Returns: - *boxes*: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - *scores*: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - *classes*: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\"\" probabilities = tf . nn . softmax ( logits , axis =- 1 ) # Remove the background at pos 0 scores = tf . reduce_max ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . SCORES ) labels = tf . argmax ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . LABELS ) # Prediction between 0 and 1 are performed with padding # Boxes (y1,x1,y2,x2) * Padded_image_(h,w,h,w) /unpadded_image_(h,w,h,w) # where padded_image and unpadded_image are in the image space image_padded_information = tf . cast ( image_padded_information , boxes . dtype ) image_information = tf . cast ( image_information , boxes . dtype ) # [batch_size, (y1_coeff, x1_coeff, y2_coeff, x2_coeff)] coeffs = tf . tile ( image_padded_information , [ 2 ]) / tf . tile ( image_information , [ 1 , 2 ]) boxes = convert_to_xyxy_coordinates ( boxes ) boxes_without_padding = boxes * coeffs [:, None ] boxes_without_padding = tf . clip_by_value ( boxes_without_padding , 0 , 1 , name = BoxField . BOXES ) if not sorted : return boxes_without_padding , scores , labels sorted_scores , indices = tf . math . top_k ( scores , k = tf . shape ( scores )[ - 1 ], sorted = True ) indices = get_full_indices ( indices ) sorted_labels = tf . gather_nd ( labels , indices ) sorted_boxes_without_padding = tf . gather_nd ( boxes_without_padding , indices ) return sorted_boxes_without_padding , sorted_scores , sorted_labels Functions post_processing def post_processing ( boxes : tensorflow . python . framework . ops . Tensor , logits : tensorflow . python . framework . ops . Tensor , image_information : tensorflow . python . framework . ops . Tensor , image_padded_information : tensorflow . python . framework . ops . Tensor , sorted = True ) PostProcessing described in the paper Object Detection with transformers \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) Arguments: logits : A Tensor of shape [batch_size, num_queries, num_classes + 1] representing the class probability. localization_pred : A Tensor of shape [batch_size, num_queries, (y_cent, x_cent, h, w)] image_information : A 2-D tensor of float32 and shape [2, (height, width)]. It contains the shape of the image without any padding. image_padded_information : A 2-D tensor of float32 and shape [(height_pad, width_pad)]. It contains the shape of the image without any padding. This padding is added during the dataset step when we batch the images together (padded_batch). sorted : Return all the elements sorted by scores in descending order. Returns: boxes : A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. scores : A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. classes : A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def post_processing ( boxes : tf . Tensor , logits : tf . Tensor , image_information : tf . Tensor , image_padded_information : tf . Tensor , sorted = True ) : \"\"\" PostProcessing described in the paper Object Detection with transformers \" To optimize for AP, we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4 . Example : background + 3 classes [ 0 . 54 , 0 . 40 , 0 . 03 , 0 . 03 ] => score = 0 . 40 , label = 0 ( 1 - 1 ) Arguments : - * logits * : A Tensor of shape [ batch_size , num_queries , num_classes + 1 ] representing the class probability . - * localization_pred * : A Tensor of shape [ batch_size , num_queries , ( y_cent , x_cent , h , w ) ] - * image_information * : A 2 - D tensor of float32 and shape [ 2 , ( height , width ) ]. It contains the shape of the image without any padding . - * image_padded_information * : A 2 - D tensor of float32 and shape [ ( height_pad , width_pad ) ]. It contains the shape of the image without any padding . This padding is added during the dataset step when we batch the images together ( padded_batch ) . - * sorted * : Return all the elements sorted by scores in descending order . Returns : - * boxes * : A Tensor of shape [ batch_size , self . num_queries , ( y1 , x1 , y2 , x2 ) ] containing the boxes with the coordinates between 0 and 1 . - * scores * : A Tensor of shape [ batch_size , self . num_queries ] containing the score of the boxes . - * classes * : A Tensor of shape [ batch_size , self . num_queries ] containing the class of the boxes [ 0 , num_classes ) . \"\"\" probabilities = tf . nn . softmax ( logits , axis =- 1 ) # Remove the background at pos 0 scores = tf . reduce_max ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . SCORES ) labels = tf . argmax ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . LABELS ) # Prediction between 0 and 1 are performed with padding # Boxes ( y1 , x1 , y2 , x2 ) * Padded_image_ ( h , w , h , w ) / unpadded_image_ ( h , w , h , w ) # where padded_image and unpadded_image are in the image space image_padded_information = tf . cast ( image_padded_information , boxes . dtype ) image_information = tf . cast ( image_information , boxes . dtype ) # [ batch_size , ( y1_coeff , x1_coeff , y2_coeff , x2_coeff ) ] coeffs = tf . tile ( image_padded_information , [ 2 ] ) / tf . tile ( image_information , [ 1 , 2 ] ) boxes = convert_to_xyxy_coordinates ( boxes ) boxes_without_padding = boxes * coeffs [:, None ] boxes_without_padding = tf . clip_by_value ( boxes_without_padding , 0 , 1 , name = BoxField . BOXES ) if not sorted : return boxes_without_padding , scores , labels sorted_scores , indices = tf . math . top_k ( scores , k = tf . shape ( scores ) [ - 1 ], sorted = True ) indices = get_full_indices ( indices ) sorted_labels = tf . gather_nd ( labels , indices ) sorted_boxes_without_padding = tf . gather_nd ( boxes_without_padding , indices ) return sorted_boxes_without_padding , sorted_scores , sorted_labels","title":"Post Processing Detr"},{"location":"reference/kerod/layers/post_processing/post_processing_detr/#module-kerodlayerspost_processingpost_processing_detr","text":"None None View Source import tensorflow as tf from kerod.core.standard_fields import BoxField from kerod.utils.ops import get_full_indices from kerod.core.box_ops import convert_to_xyxy_coordinates def post_processing ( boxes : tf . Tensor , logits : tf . Tensor , image_information : tf . Tensor , image_padded_information : tf . Tensor , sorted = True ): \"\"\"PostProcessing described in the paper Object Detection with transformers \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) Arguments: - *logits*: A Tensor of shape [batch_size, num_queries, num_classes + 1] representing the class probability. - *localization_pred*: A Tensor of shape [batch_size, num_queries, (y_cent, x_cent, h, w)] - *image_information*: A 2-D tensor of float32 and shape [2, (height, width)]. It contains the shape of the image without any padding. - *image_padded_information*: A 2-D tensor of float32 and shape [(height_pad, width_pad)]. It contains the shape of the image without any padding. This padding is added during the dataset step when we batch the images together (padded_batch). - *sorted*: Return all the elements sorted by scores in descending order. Returns: - *boxes*: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - *scores*: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - *classes*: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\"\" probabilities = tf . nn . softmax ( logits , axis =- 1 ) # Remove the background at pos 0 scores = tf . reduce_max ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . SCORES ) labels = tf . argmax ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . LABELS ) # Prediction between 0 and 1 are performed with padding # Boxes (y1,x1,y2,x2) * Padded_image_(h,w,h,w) /unpadded_image_(h,w,h,w) # where padded_image and unpadded_image are in the image space image_padded_information = tf . cast ( image_padded_information , boxes . dtype ) image_information = tf . cast ( image_information , boxes . dtype ) # [batch_size, (y1_coeff, x1_coeff, y2_coeff, x2_coeff)] coeffs = tf . tile ( image_padded_information , [ 2 ]) / tf . tile ( image_information , [ 1 , 2 ]) boxes = convert_to_xyxy_coordinates ( boxes ) boxes_without_padding = boxes * coeffs [:, None ] boxes_without_padding = tf . clip_by_value ( boxes_without_padding , 0 , 1 , name = BoxField . BOXES ) if not sorted : return boxes_without_padding , scores , labels sorted_scores , indices = tf . math . top_k ( scores , k = tf . shape ( scores )[ - 1 ], sorted = True ) indices = get_full_indices ( indices ) sorted_labels = tf . gather_nd ( labels , indices ) sorted_boxes_without_padding = tf . gather_nd ( boxes_without_padding , indices ) return sorted_boxes_without_padding , sorted_scores , sorted_labels","title":"Module kerod.layers.post_processing.post_processing_detr"},{"location":"reference/kerod/layers/post_processing/post_processing_detr/#functions","text":"","title":"Functions"},{"location":"reference/kerod/layers/post_processing/post_processing_detr/#post_processing","text":"def post_processing ( boxes : tensorflow . python . framework . ops . Tensor , logits : tensorflow . python . framework . ops . Tensor , image_information : tensorflow . python . framework . ops . Tensor , image_padded_information : tensorflow . python . framework . ops . Tensor , sorted = True ) PostProcessing described in the paper Object Detection with transformers \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) Arguments: logits : A Tensor of shape [batch_size, num_queries, num_classes + 1] representing the class probability. localization_pred : A Tensor of shape [batch_size, num_queries, (y_cent, x_cent, h, w)] image_information : A 2-D tensor of float32 and shape [2, (height, width)]. It contains the shape of the image without any padding. image_padded_information : A 2-D tensor of float32 and shape [(height_pad, width_pad)]. It contains the shape of the image without any padding. This padding is added during the dataset step when we batch the images together (padded_batch). sorted : Return all the elements sorted by scores in descending order. Returns: boxes : A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. scores : A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. classes : A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def post_processing ( boxes : tf . Tensor , logits : tf . Tensor , image_information : tf . Tensor , image_padded_information : tf . Tensor , sorted = True ) : \"\"\" PostProcessing described in the paper Object Detection with transformers \" To optimize for AP, we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4 . Example : background + 3 classes [ 0 . 54 , 0 . 40 , 0 . 03 , 0 . 03 ] => score = 0 . 40 , label = 0 ( 1 - 1 ) Arguments : - * logits * : A Tensor of shape [ batch_size , num_queries , num_classes + 1 ] representing the class probability . - * localization_pred * : A Tensor of shape [ batch_size , num_queries , ( y_cent , x_cent , h , w ) ] - * image_information * : A 2 - D tensor of float32 and shape [ 2 , ( height , width ) ]. It contains the shape of the image without any padding . - * image_padded_information * : A 2 - D tensor of float32 and shape [ ( height_pad , width_pad ) ]. It contains the shape of the image without any padding . This padding is added during the dataset step when we batch the images together ( padded_batch ) . - * sorted * : Return all the elements sorted by scores in descending order . Returns : - * boxes * : A Tensor of shape [ batch_size , self . num_queries , ( y1 , x1 , y2 , x2 ) ] containing the boxes with the coordinates between 0 and 1 . - * scores * : A Tensor of shape [ batch_size , self . num_queries ] containing the score of the boxes . - * classes * : A Tensor of shape [ batch_size , self . num_queries ] containing the class of the boxes [ 0 , num_classes ) . \"\"\" probabilities = tf . nn . softmax ( logits , axis =- 1 ) # Remove the background at pos 0 scores = tf . reduce_max ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . SCORES ) labels = tf . argmax ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . LABELS ) # Prediction between 0 and 1 are performed with padding # Boxes ( y1 , x1 , y2 , x2 ) * Padded_image_ ( h , w , h , w ) / unpadded_image_ ( h , w , h , w ) # where padded_image and unpadded_image are in the image space image_padded_information = tf . cast ( image_padded_information , boxes . dtype ) image_information = tf . cast ( image_information , boxes . dtype ) # [ batch_size , ( y1_coeff , x1_coeff , y2_coeff , x2_coeff ) ] coeffs = tf . tile ( image_padded_information , [ 2 ] ) / tf . tile ( image_information , [ 1 , 2 ] ) boxes = convert_to_xyxy_coordinates ( boxes ) boxes_without_padding = boxes * coeffs [:, None ] boxes_without_padding = tf . clip_by_value ( boxes_without_padding , 0 , 1 , name = BoxField . BOXES ) if not sorted : return boxes_without_padding , scores , labels sorted_scores , indices = tf . math . top_k ( scores , k = tf . shape ( scores ) [ - 1 ], sorted = True ) indices = get_full_indices ( indices ) sorted_labels = tf . gather_nd ( labels , indices ) sorted_boxes_without_padding = tf . gather_nd ( boxes_without_padding , indices ) return sorted_boxes_without_padding , sorted_scores , sorted_labels","title":"post_processing"},{"location":"reference/kerod/layers/smca/","text":"Module kerod.layers.smca None None Sub-modules kerod.layers.smca.reference_points kerod.layers.smca.weight_map","title":"Index"},{"location":"reference/kerod/layers/smca/#module-kerodlayerssmca","text":"None None","title":"Module kerod.layers.smca"},{"location":"reference/kerod/layers/smca/#sub-modules","text":"kerod.layers.smca.reference_points kerod.layers.smca.weight_map","title":"Sub-modules"},{"location":"reference/kerod/layers/smca/reference_points/","text":"Module kerod.layers.smca.reference_points None None View Source import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class SMCAReferencePoints ( tf . keras . layers . Layer ): \"\"\"Multi head reference points from the paper [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). Based on the object queries will create a set of reference points which will allow to create a [spatial dynamical weight maps](./weight_map.py) in order to modulate the co-attention inside the transformer Arguments: hidden_dim: Positive integer, dimensionality of the hidden space. num_heads: Positive integer, each head starts from a head-shared center and then predicts a head-specific center offset and head specific scales. Call arguments: object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. Call returns: Tuple: - `reference_points`: A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - `embedding_reference_points`: A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied. \"\"\" def __init__ ( self , hidden_dim : int , num_heads : int , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_heads = num_heads self . hidden_dim = hidden_dim self . xy_embed = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( 2 ) # (y_cent, x_cent) ]) # Each head will have its proper focus weight and width self . yx_offset_hw_embed = tf . keras . layers . Dense ( 4 * num_heads ) def call ( self , object_queries ): \"\"\" Args: object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. Call returns: Tuple: - `reference_points`: A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - `embedding_reference_points`: A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied. \"\"\" yx_pre_sigmoid = self . xy_embed ( object_queries ) # [bs, num_queries, 2] yx = tf . nn . sigmoid ( yx_pre_sigmoid ) # Where y and x are offsets predictions for 'yx' (above) # h and w are the scales predictions yx_offset_hw = self . yx_offset_hw_embed ( object_queries ) # [bs, num_queries, head * 4] batch_size = tf . shape ( object_queries )[ 0 ] num_queries = tf . shape ( object_queries )[ 1 ] # Add offset coordinates to yx and concatenate with scale predictions # yx => [bs, num_queries, head, 2] yx = tf . tile ( yx [:, :, None ], ( 1 , 1 , self . num_heads , 1 )) # yx_offset_hw => [bs, num_queries, head, 4] yx_offset_hw = tf . reshape ( yx_offset_hw , ( batch_size , - 1 , self . num_heads , 4 )) yxhw = tf . concat ([ yx , tf . zeros (( batch_size , num_queries , self . num_heads , 2 ))], axis =- 1 ) return yxhw + yx_offset_hw , yx_pre_sigmoid def get_config ( self ): config = super () . get_config () config [ 'num_heads' ] = self . num_heads config [ 'hidden_dim' ] = self . hidden_dim return config remove_unwanted_doc ( SMCAReferencePoints , __pdoc__ ) Classes SMCAReferencePoints class SMCAReferencePoints ( hidden_dim : int , num_heads : int , ** kwargs ) Based on the object queries will create a set of reference points which will allow to create a spatial dynamical weight maps in order to modulate the co-attention inside the transformer Arguments Name Description hidden_dim Positive integer, dimensionality of the hidden space. num_heads Positive integer, each head starts from a head-shared center and then predicts a head-specific center offset and head specific scales. Call arguments Name Description object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. Call returns Type Description Tuple - reference_points : A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - embedding_reference_points : A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied. Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , object_queries ) Parameters: Name Description object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. View Source def call ( self , object_queries ) : \" \"\" Args: object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. Call returns: Tuple: - `reference_points`: A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - `embedding_reference_points`: A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied. \"\" \" yx_pre_sigmoid = self . xy_embed ( object_queries ) # [bs, num_queries, 2] yx = tf . nn . sigmoid ( yx_pre_sigmoid ) # Where y and x are offsets predictions for 'yx' (above) # h and w are the scales predictions yx_offset_hw = self . yx_offset_hw_embed ( object_queries ) # [bs, num_queries, head * 4] batch_size = tf . shape ( object_queries ) [ 0 ] num_queries = tf . shape ( object_queries ) [ 1 ] # Add offset coordinates to yx and concatenate with scale predictions # yx => [bs, num_queries, head, 2] yx = tf . tile ( yx [ : , : , None ] , ( 1 , 1 , self . num_heads , 1 )) # yx_offset_hw => [bs, num_queries, head, 4] yx_offset_hw = tf . reshape ( yx_offset_hw , ( batch_size , - 1 , self . num_heads , 4 )) yxhw = tf . concat ( [ yx , tf . zeros (( batch_size , num_queries , self . num_heads , 2 )) ] , axis =- 1 ) return yxhw + yx_offset_hw , yx_pre_sigmoid","title":"Reference Points"},{"location":"reference/kerod/layers/smca/reference_points/#module-kerodlayerssmcareference_points","text":"None None View Source import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class SMCAReferencePoints ( tf . keras . layers . Layer ): \"\"\"Multi head reference points from the paper [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). Based on the object queries will create a set of reference points which will allow to create a [spatial dynamical weight maps](./weight_map.py) in order to modulate the co-attention inside the transformer Arguments: hidden_dim: Positive integer, dimensionality of the hidden space. num_heads: Positive integer, each head starts from a head-shared center and then predicts a head-specific center offset and head specific scales. Call arguments: object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. Call returns: Tuple: - `reference_points`: A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - `embedding_reference_points`: A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied. \"\"\" def __init__ ( self , hidden_dim : int , num_heads : int , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_heads = num_heads self . hidden_dim = hidden_dim self . xy_embed = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( 2 ) # (y_cent, x_cent) ]) # Each head will have its proper focus weight and width self . yx_offset_hw_embed = tf . keras . layers . Dense ( 4 * num_heads ) def call ( self , object_queries ): \"\"\" Args: object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. Call returns: Tuple: - `reference_points`: A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - `embedding_reference_points`: A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied. \"\"\" yx_pre_sigmoid = self . xy_embed ( object_queries ) # [bs, num_queries, 2] yx = tf . nn . sigmoid ( yx_pre_sigmoid ) # Where y and x are offsets predictions for 'yx' (above) # h and w are the scales predictions yx_offset_hw = self . yx_offset_hw_embed ( object_queries ) # [bs, num_queries, head * 4] batch_size = tf . shape ( object_queries )[ 0 ] num_queries = tf . shape ( object_queries )[ 1 ] # Add offset coordinates to yx and concatenate with scale predictions # yx => [bs, num_queries, head, 2] yx = tf . tile ( yx [:, :, None ], ( 1 , 1 , self . num_heads , 1 )) # yx_offset_hw => [bs, num_queries, head, 4] yx_offset_hw = tf . reshape ( yx_offset_hw , ( batch_size , - 1 , self . num_heads , 4 )) yxhw = tf . concat ([ yx , tf . zeros (( batch_size , num_queries , self . num_heads , 2 ))], axis =- 1 ) return yxhw + yx_offset_hw , yx_pre_sigmoid def get_config ( self ): config = super () . get_config () config [ 'num_heads' ] = self . num_heads config [ 'hidden_dim' ] = self . hidden_dim return config remove_unwanted_doc ( SMCAReferencePoints , __pdoc__ )","title":"Module kerod.layers.smca.reference_points"},{"location":"reference/kerod/layers/smca/reference_points/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/smca/reference_points/#smcareferencepoints","text":"class SMCAReferencePoints ( hidden_dim : int , num_heads : int , ** kwargs ) Based on the object queries will create a set of reference points which will allow to create a spatial dynamical weight maps in order to modulate the co-attention inside the transformer","title":"SMCAReferencePoints"},{"location":"reference/kerod/layers/smca/reference_points/#arguments","text":"Name Description hidden_dim Positive integer, dimensionality of the hidden space. num_heads Positive integer, each head starts from a head-shared center and then predicts a head-specific center offset and head specific scales.","title":"Arguments"},{"location":"reference/kerod/layers/smca/reference_points/#call-arguments","text":"Name Description object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder.","title":"Call arguments"},{"location":"reference/kerod/layers/smca/reference_points/#call-returns","text":"Type Description Tuple - reference_points : A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - embedding_reference_points : A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied.","title":"Call returns"},{"location":"reference/kerod/layers/smca/reference_points/#ancestors-in-mro","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/smca/reference_points/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/smca/reference_points/#call","text":"def call ( self , object_queries ) Parameters: Name Description object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. View Source def call ( self , object_queries ) : \" \"\" Args: object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. Call returns: Tuple: - `reference_points`: A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - `embedding_reference_points`: A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied. \"\" \" yx_pre_sigmoid = self . xy_embed ( object_queries ) # [bs, num_queries, 2] yx = tf . nn . sigmoid ( yx_pre_sigmoid ) # Where y and x are offsets predictions for 'yx' (above) # h and w are the scales predictions yx_offset_hw = self . yx_offset_hw_embed ( object_queries ) # [bs, num_queries, head * 4] batch_size = tf . shape ( object_queries ) [ 0 ] num_queries = tf . shape ( object_queries ) [ 1 ] # Add offset coordinates to yx and concatenate with scale predictions # yx => [bs, num_queries, head, 2] yx = tf . tile ( yx [ : , : , None ] , ( 1 , 1 , self . num_heads , 1 )) # yx_offset_hw => [bs, num_queries, head, 4] yx_offset_hw = tf . reshape ( yx_offset_hw , ( batch_size , - 1 , self . num_heads , 4 )) yxhw = tf . concat ( [ yx , tf . zeros (( batch_size , num_queries , self . num_heads , 2 )) ] , axis =- 1 ) return yxhw + yx_offset_hw , yx_pre_sigmoid","title":"call"},{"location":"reference/kerod/layers/smca/weight_map/","text":"Module kerod.layers.smca.weight_map None None View Source import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class DynamicalWeightMaps ( tf . keras . layers . Layer ): \"\"\"Dynamical spatial weight maps described in [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). Each object query first dynamically predicts the center and scale of its responsible object, which are then used to generate a 2-D Gaussian like spatial weight map. Example: This is an example of spatial weight maps from a 20 x 20 tensor and where we generated 25 random reference points. ![Spatial weight map](https://raw.githubusercontent.com/EmGarr/kerod/master/ressources/spatial_weight_map.png) This spatial_weight_map is then flatten to obtain a [25, 400] matrix representing the the modulated co-attention. ![Spatial weight map flatten](https://raw.githubusercontent.com/EmGarr/kerod/master/ressources/flatten_weight_map.png) Args: beta: Modulate the bandwidth of the Gaussian-like distribution. Call arguments: height: The targeted height of the feature map width: The targeted width of the feature map ref_points: A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Call returns: tf.Tensor: The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width]. \"\"\" def __init__ ( self , beta = 8. , ** kwargs ): super () . __init__ ( ** kwargs ) self . _beta = beta def call ( self , height : int , width : int , ref_points : tf . Tensor ): \"\"\" Args: height: The targeted height of the feature map width: The targeted width of the feature map ref_points: A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Returns: tf.Tensor: The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width]. \"\"\" x = tf . cast ( tf . linspace ( 0 , 1 , width ), self . dtype ) y = tf . cast ( tf . linspace ( 0 , 1 , height ), self . dtype ) x , y = tf . meshgrid ( x , y ) # x and y have shape [height, width] ref_points = tf . transpose ( ref_points , ( 0 , 2 , 1 , 3 )) y_cent , x_cent , h , w = tf . split ( ref_points , 4 , axis =- 1 ) # [batch_size, heads, N, 1] => [batch_size,heads, N, 1, 1] y_cent , x_cent = y_cent [ ... , tf . newaxis ], x_cent [ ... , tf . newaxis ] h , w = h [ ... , tf . newaxis ], w [ ... , tf . newaxis ] # [height, width] => [1, 1, 1, height, width] y , x = y [ tf . newaxis , tf . newaxis , tf . newaxis ], x [ tf . newaxis , tf . newaxis , tf . newaxis ] # [batch_size, heads, N, height, width] beta = tf . cast ( tf . convert_to_tensor ( self . _beta ), self . dtype ) x_term = - ( x - x_cent ) ** 2 / ( beta * w ** 2 ) y_term = - ( y - y_cent ) ** 2 / ( beta * h ** 2 ) weight_map = tf . math . exp ( x_term + y_term ) batch_size , num_heads = tf . shape ( weight_map )[ 0 ], tf . shape ( weight_map )[ 1 ] return tf . reshape ( weight_map , ( batch_size , num_heads , - 1 , height * width )) def get_config ( self ): config = super () . get_config () config [ 'beta' ] = self . _beta return config remove_unwanted_doc ( DynamicalWeightMaps , __pdoc__ ) Classes DynamicalWeightMaps class DynamicalWeightMaps ( beta = 8.0 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . Each object query first dynamically predicts the center and scale of its responsible object, which are then used to generate a 2-D Gaussian like spatial weight map. Example: This is an example of spatial weight maps from a 20 x 20 tensor and where we generated 25 random reference points. This spatial_weight_map is then flatten to obtain a [25, 400] matrix representing the the modulated co-attention. Args: beta: Modulate the bandwidth of the Gaussian-like distribution. Call arguments: height: The targeted height of the feature map width: The targeted width of the feature map ref_points: A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Call returns: tf.Tensor: The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width]. Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , height : int , width : int , ref_points : tensorflow . python . framework . ops . Tensor ) Parameters: Name Description height The targeted height of the feature map width The targeted width of the feature map ref_points A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Returns: Type Description tf.Tensor The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width]. View Source def call ( self , height : int , width : int , ref_points : tf . Tensor ) : \"\"\" Args : height : The targeted height of the feature map width : The targeted width of the feature map ref_points : A tensor of shape [ batch_size , N , heads , ( y , x , h , w ) ] in [ 0 , 1 ] Returns : tf . Tensor : The weight_map per reference points a 4 D tensor of float 32 and shape [ batch_size , heads , N , height * width ]. \"\"\" x = tf . cast ( tf . linspace ( 0 , 1 , width ) , self . dtype ) y = tf . cast ( tf . linspace ( 0 , 1 , height ) , self . dtype ) x , y = tf . meshgrid ( x , y ) # x and y have shape [ height , width ] ref_points = tf . transpose ( ref_points , ( 0 , 2 , 1 , 3 )) y_cent , x_cent , h , w = tf . split ( ref_points , 4 , axis =- 1 ) # [ batch_size , heads , N , 1 ] => [ batch_size , heads , N , 1 , 1 ] y_cent , x_cent = y_cent [..., tf . newaxis ], x_cent [..., tf . newaxis ] h , w = h [..., tf . newaxis ], w [..., tf . newaxis ] # [ height , width ] => [ 1 , 1 , 1 , height , width ] y , x = y [ tf . newaxis , tf . newaxis , tf . newaxis ], x [ tf . newaxis , tf . newaxis , tf . newaxis ] # [ batch_size , heads , N , height , width ] beta = tf . cast ( tf . convert_to_tensor ( self . _beta ) , self . dtype ) x_term = - ( x - x_cent ) ** 2 / ( beta * w ** 2 ) y_term = - ( y - y_cent ) ** 2 / ( beta * h ** 2 ) weight_map = tf . math . exp ( x_term + y_term ) batch_size , num_heads = tf . shape ( weight_map ) [ 0 ], tf . shape ( weight_map ) [ 1 ] return tf . reshape ( weight_map , ( batch_size , num_heads , - 1 , height * width ))","title":"Weight Map"},{"location":"reference/kerod/layers/smca/weight_map/#module-kerodlayerssmcaweight_map","text":"None None View Source import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class DynamicalWeightMaps ( tf . keras . layers . Layer ): \"\"\"Dynamical spatial weight maps described in [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). Each object query first dynamically predicts the center and scale of its responsible object, which are then used to generate a 2-D Gaussian like spatial weight map. Example: This is an example of spatial weight maps from a 20 x 20 tensor and where we generated 25 random reference points. ![Spatial weight map](https://raw.githubusercontent.com/EmGarr/kerod/master/ressources/spatial_weight_map.png) This spatial_weight_map is then flatten to obtain a [25, 400] matrix representing the the modulated co-attention. ![Spatial weight map flatten](https://raw.githubusercontent.com/EmGarr/kerod/master/ressources/flatten_weight_map.png) Args: beta: Modulate the bandwidth of the Gaussian-like distribution. Call arguments: height: The targeted height of the feature map width: The targeted width of the feature map ref_points: A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Call returns: tf.Tensor: The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width]. \"\"\" def __init__ ( self , beta = 8. , ** kwargs ): super () . __init__ ( ** kwargs ) self . _beta = beta def call ( self , height : int , width : int , ref_points : tf . Tensor ): \"\"\" Args: height: The targeted height of the feature map width: The targeted width of the feature map ref_points: A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Returns: tf.Tensor: The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width]. \"\"\" x = tf . cast ( tf . linspace ( 0 , 1 , width ), self . dtype ) y = tf . cast ( tf . linspace ( 0 , 1 , height ), self . dtype ) x , y = tf . meshgrid ( x , y ) # x and y have shape [height, width] ref_points = tf . transpose ( ref_points , ( 0 , 2 , 1 , 3 )) y_cent , x_cent , h , w = tf . split ( ref_points , 4 , axis =- 1 ) # [batch_size, heads, N, 1] => [batch_size,heads, N, 1, 1] y_cent , x_cent = y_cent [ ... , tf . newaxis ], x_cent [ ... , tf . newaxis ] h , w = h [ ... , tf . newaxis ], w [ ... , tf . newaxis ] # [height, width] => [1, 1, 1, height, width] y , x = y [ tf . newaxis , tf . newaxis , tf . newaxis ], x [ tf . newaxis , tf . newaxis , tf . newaxis ] # [batch_size, heads, N, height, width] beta = tf . cast ( tf . convert_to_tensor ( self . _beta ), self . dtype ) x_term = - ( x - x_cent ) ** 2 / ( beta * w ** 2 ) y_term = - ( y - y_cent ) ** 2 / ( beta * h ** 2 ) weight_map = tf . math . exp ( x_term + y_term ) batch_size , num_heads = tf . shape ( weight_map )[ 0 ], tf . shape ( weight_map )[ 1 ] return tf . reshape ( weight_map , ( batch_size , num_heads , - 1 , height * width )) def get_config ( self ): config = super () . get_config () config [ 'beta' ] = self . _beta return config remove_unwanted_doc ( DynamicalWeightMaps , __pdoc__ )","title":"Module kerod.layers.smca.weight_map"},{"location":"reference/kerod/layers/smca/weight_map/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/smca/weight_map/#dynamicalweightmaps","text":"class DynamicalWeightMaps ( beta = 8.0 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . Each object query first dynamically predicts the center and scale of its responsible object, which are then used to generate a 2-D Gaussian like spatial weight map. Example: This is an example of spatial weight maps from a 20 x 20 tensor and where we generated 25 random reference points. This spatial_weight_map is then flatten to obtain a [25, 400] matrix representing the the modulated co-attention. Args: beta: Modulate the bandwidth of the Gaussian-like distribution. Call arguments: height: The targeted height of the feature map width: The targeted width of the feature map ref_points: A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Call returns: tf.Tensor: The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width].","title":"DynamicalWeightMaps"},{"location":"reference/kerod/layers/smca/weight_map/#ancestors-in-mro","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/smca/weight_map/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/smca/weight_map/#call","text":"def call ( self , height : int , width : int , ref_points : tensorflow . python . framework . ops . Tensor ) Parameters: Name Description height The targeted height of the feature map width The targeted width of the feature map ref_points A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Returns: Type Description tf.Tensor The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width]. View Source def call ( self , height : int , width : int , ref_points : tf . Tensor ) : \"\"\" Args : height : The targeted height of the feature map width : The targeted width of the feature map ref_points : A tensor of shape [ batch_size , N , heads , ( y , x , h , w ) ] in [ 0 , 1 ] Returns : tf . Tensor : The weight_map per reference points a 4 D tensor of float 32 and shape [ batch_size , heads , N , height * width ]. \"\"\" x = tf . cast ( tf . linspace ( 0 , 1 , width ) , self . dtype ) y = tf . cast ( tf . linspace ( 0 , 1 , height ) , self . dtype ) x , y = tf . meshgrid ( x , y ) # x and y have shape [ height , width ] ref_points = tf . transpose ( ref_points , ( 0 , 2 , 1 , 3 )) y_cent , x_cent , h , w = tf . split ( ref_points , 4 , axis =- 1 ) # [ batch_size , heads , N , 1 ] => [ batch_size , heads , N , 1 , 1 ] y_cent , x_cent = y_cent [..., tf . newaxis ], x_cent [..., tf . newaxis ] h , w = h [..., tf . newaxis ], w [..., tf . newaxis ] # [ height , width ] => [ 1 , 1 , 1 , height , width ] y , x = y [ tf . newaxis , tf . newaxis , tf . newaxis ], x [ tf . newaxis , tf . newaxis , tf . newaxis ] # [ batch_size , heads , N , height , width ] beta = tf . cast ( tf . convert_to_tensor ( self . _beta ) , self . dtype ) x_term = - ( x - x_cent ) ** 2 / ( beta * w ** 2 ) y_term = - ( y - y_cent ) ** 2 / ( beta * h ** 2 ) weight_map = tf . math . exp ( x_term + y_term ) batch_size , num_heads = tf . shape ( weight_map ) [ 0 ], tf . shape ( weight_map ) [ 1 ] return tf . reshape ( weight_map , ( batch_size , num_heads , - 1 , height * width ))","title":"call"},{"location":"reference/kerod/model/","text":"Module kerod.model None None View Source from kerod.model.factory import KerodModel Sub-modules kerod.model.backbone kerod.model.detr kerod.model.factory kerod.model.faster_rcnn kerod.model.smca_detr","title":"Index"},{"location":"reference/kerod/model/#module-kerodmodel","text":"None None View Source from kerod.model.factory import KerodModel","title":"Module kerod.model"},{"location":"reference/kerod/model/#sub-modules","text":"kerod.model.backbone kerod.model.detr kerod.model.factory kerod.model.faster_rcnn kerod.model.smca_detr","title":"Sub-modules"},{"location":"reference/kerod/model/detr/","text":"Module kerod.model.detr None None View Source from typing import Dict import tensorflow as tf from tensorflow.keras.losses import SparseCategoricalCrossentropy from tensorflow.python.keras.engine import data_adapter from tensorflow_addons.losses.giou_loss import GIoULoss from kerod.core.box_ops import ( convert_to_center_coordinates , convert_to_xyxy_coordinates ) from kerod.core.losses import L1Loss from kerod.core.matcher import hungarian_matching from kerod.core.similarity import DetrSimilarity from kerod.core.standard_fields import BoxField , DatasetField from kerod.core.target_assigner import TargetAssigner from kerod.layers import PositionEmbeddingSine , Transformer from kerod.layers.post_processing.post_processing_detr import \\ post_processing as detr_postprocessing from kerod.model.backbone.resnet import ResNet50 , ResNet50PytorchStyle from kerod.utils import item_assignment from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class DeTr ( tf . keras . Model ): \"\"\"Build a DeTr model according to the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) You can use it as follow: ```python model = DeTrResnet50Pytorch(80) base_lr = 0.1 optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr) model.compile(optimizer=optimizer, loss=None) model.fit(ds_train, validation_data=ds_test, epochs=11,) ``` Arguments: num_classes: The number of classes of your dataset (**do not include the background class** it is handle for you) backbone: A vision model like ResNet50. num_queries: number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries. Call arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Call returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" def __init__ ( self , num_classes : int , backbone , num_queries = 100 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_classes = num_classes self . num_queries = num_queries self . hidden_dim = 256 self . backbone = backbone self . input_proj = tf . keras . layers . Conv2D ( self . hidden_dim , 1 ) self . pos_embed = PositionEmbeddingSine ( output_dim = self . hidden_dim ) self . transformer_num_layers = 6 self . transformer = Transformer ( num_layers = self . transformer_num_layers , d_model = self . hidden_dim , num_heads = 8 , dim_feedforward = 2048 ) self . bbox_embed = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( 4 , activation = 'sigmoid' , dtype = tf . float32 ) # (x1, y1, x2, y2) ]) self . class_embed = tf . keras . layers . Dense ( num_classes + 1 , dtype = tf . float32 ) # Will create a learnable embedding matrix for all our queries # It is a matrix of [num_queries, self.hidden_dim] # The embedding layers self . query_embed = tf . keras . layers . Embedding ( num_queries , self . hidden_dim , embeddings_initializer = tf . keras . initializers . RandomNormal ( mean = 0. , stddev = 1. )) self . all_the_queries = tf . range ( num_queries ) # Loss computation self . weight_class , self . weight_l1 , self . weight_giou = 1 , 5 , 2 similarity_func = DetrSimilarity ( self . weight_class , self . weight_l1 , self . weight_giou ) self . target_assigner = TargetAssigner ( similarity_func , hungarian_matching , lambda gt , pred : gt , negative_class_weight = 1.0 ) # Relative classification weight applied to the no-object category # It down-weight the log-probability term of a no-object # by a factor 10 to account for class imbalance self . non_object_weight = tf . constant ( 0.1 , dtype = self . compute_dtype ) # Losses self . giou = GIoULoss ( reduction = tf . keras . losses . Reduction . NONE ) self . l1 = L1Loss ( reduction = tf . keras . losses . Reduction . NONE ) self . scc = SparseCategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE , from_logits = True ) # Metrics self . giou_metric = tf . keras . metrics . Mean ( name = \"giou_last_layer\" ) self . l1_metric = tf . keras . metrics . Mean ( name = \"l1_last_layer\" ) self . scc_metric = tf . keras . metrics . Mean ( name = \"scc_last_layer\" ) self . loss_metric = tf . keras . metrics . Mean ( name = \"loss\" ) self . precision_metric = tf . keras . metrics . SparseCategoricalAccuracy () # Object recall = foreground self . recall_metric = tf . keras . metrics . Mean ( name = \"object_recall\" ) @property def metrics ( self ): return [ self . loss_metric , self . giou_metric , self . l1_metric , self . scc_metric , self . precision_metric , self . recall_metric ] def call ( self , inputs , training = None ): \"\"\"Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images )[ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images )[ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ... , None ], tf . float32 ), tf . shape ( x )[ 1 : 3 ], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ], ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } def compute_loss ( self , ground_truths : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ], input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField . BOXES ] / tf . tile ( input_shape [ None ], [ 1 , 2 ]) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField . BOXES ], self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField . SCORES ], self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [{ BoxField . BOXES : boxes , BoxField . SCORES : logits } for boxes , logits in zip ( boxes_per_lvl , logits_per_lvl )] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField . NUM_BOXES ]), tf . float32 ) loss = 0 # Compute the Giou, L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ): # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss def _compute_loss ( self , y_pred : Dict [ str , tf . Tensor ], ground_truths : Dict [ str , tf . Tensor ], num_boxes : int , compute_metrics = False , ): y_true , weights = self . target_assigner . assign ( y_pred , ground_truths ) # Reduce the class imbalanced by applying to the weights # self.non_object_weight for the non object (pos 0) weights [ BoxField . LABELS ] = item_assignment ( weights [ BoxField . LABELS ], y_true [ BoxField . LABELS ] == 0 , self . non_object_weight ) # Caveats GIoU is buggy and if the batch_size is 1 and the sample_weight # is provided will raise an error giou = self . giou ( convert_to_xyxy_coordinates ( y_true [ BoxField . BOXES ]), convert_to_xyxy_coordinates ( y_pred [ BoxField . BOXES ]), sample_weight = weights [ BoxField . BOXES ]) l1 = self . l1 ( y_true [ BoxField . BOXES ], y_pred [ BoxField . BOXES ], sample_weight = weights [ BoxField . BOXES ]) # SparseCategoricalCrossentropy scc = self . scc ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) giou = self . weight_giou * tf . reduce_sum ( giou ) / num_boxes l1 = self . weight_l1 * tf . reduce_sum ( l1 ) / num_boxes scc = self . weight_class * tf . reduce_sum ( scc ) / tf . reduce_sum ( weights [ BoxField . LABELS ]) if compute_metrics : self . giou_metric . update_state ( giou ) self . l1_metric . update_state ( l1 ) self . scc_metric . update_state ( scc ) self . precision_metric . update_state ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) recall = compute_detr_metrics ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ]) self . recall_metric . update_state ( recall ) return giou + l1 + scc def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def test_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def predict_step ( self , data ): \"\"\"Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: boxes: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. scores: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. classes: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\"\" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ], y_pred [ BoxField . SCORES ], x [ DatasetField . IMAGES_INFO ], tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], ) return boxes_without_padding , scores , labels class DeTrResnet50 ( DeTr ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50 ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) class DeTrResnet50Pytorch ( DeTr ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50PytorchStyle ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) def compute_detr_metrics ( y_true : tf . Tensor , y_pred : tf . Tensor ): \"\"\"Useful metrics that allows to track how behave the training. Arguments: y_true: A one-hot encoded vector with shape [batch_size, num_object_queries, num_classes] y_pred: A tensor with shape [batch_size, num_object_queries, num_classes], representing the classification logits. Returns: tf.Tensor: Recall Among all the boxes that we had to find how much did we found. \"\"\" #Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis =- 1 , name = 'label_prediction' , output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) recall = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name = 'recall' ) return recall remove_unwanted_doc ( DeTr , __pdoc__ ) remove_unwanted_doc ( DeTrResnet50 , __pdoc__ ) remove_unwanted_doc ( DeTrResnet50Pytorch , __pdoc__ ) Functions compute_detr_metrics def compute_detr_metrics ( y_true : tensorflow . python . framework . ops . Tensor , y_pred : tensorflow . python . framework . ops . Tensor ) Useful metrics that allows to track how behave the training. Parameters: Name Description y_true A one-hot encoded vector with shape [batch_size, num_object_queries, num_classes] y_pred A tensor with shape [batch_size, num_object_queries, num_classes], representing the classification logits. Returns: Type Description tf.Tensor Recall Among all the boxes that we had to find how much did we found. View Source def compute_detr_metrics ( y_true: tf . Tensor , y_pred: tf . Tensor ) : \"\"\"Useful metrics that allows to track how behave the training. Arguments: y_true: A one - hot encoded vector with shape [ batch_size , num_object_queries , num_classes ] y_pred: A tensor with shape [ batch_size , num_object_queries , num_classes ], representing the classification logits . Returns: tf . Tensor: Recall Among all the boxes that we had to find how much did we found . \"\"\" # Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis =- 1 , name = ' label_prediction ', output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) recall = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name = ' recall ') return recall Classes DeTr class DeTr ( num_classes : int , backbone , num_queries = 100 , ** kwargs ) End-to-End Object Detection with Transformers You can use it as follow: model = DeTrResnet50Pytorch ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries. Call arguments Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Call returns Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. Ancestors (in MRO) keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector Descendants kerod.model.detr.DeTrResnet50 kerod.model.detr.DeTrResnet50Pytorch Methods call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } compile def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile compute_loss def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> int Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss compute_metrics def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics evaluate def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) evaluate_generator def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) fit def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history fit_generator def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch ) get_layer def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' ) load_weights def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status make_predict_function def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function make_test_function def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function make_train_function def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function predict def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs ) predict_generator def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) predict_on_batch def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs ) predict_step def predict_step ( self , data ) Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Type Description boxes A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. scores: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. classes: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def predict_step ( self , data ) : \"\"\" Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed . It means that if background was predicted the second maximum score would be outputed . Example : background + 3 classes [ 0 . 54 , 0 . 40 , 0 . 03 , 0 . 03 ] => score = 0 . 40 , label = 0 ( 1 - 1 ) \" To optimize for AP, we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4 . Experiments of Object Detection with Transformers Returns : boxes : A Tensor of shape [ batch_size , self . num_queries , ( y1 , x1 , y2 , x2 ) ] containing the boxes with the coordinates between 0 and 1 . scores : A Tensor of shape [ batch_size , self . num_queries ] containing the score of the boxes . classes : A Tensor of shape [ batch_size , self . num_queries ] containing the class of the boxes [ 0 , num_classes ) . \"\"\" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ], y_pred [ BoxField . SCORES ], x [ DatasetField . IMAGES_INFO ], tf . shape ( x [ DatasetField . IMAGES ] ) [ 1 : 3 ], ) return boxes_without_padding , scores , labels reset_metrics def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state () reset_states def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states () save def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. Example: from keras.models import load_model model . save ( 'my_model.h5' ) # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model ( 'my_model.h5' ) View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) : # pylint: disable=line-too-long \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" # pylint: enable=line-too-long save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces ) save_spec def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False ) save_weights def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ]) summary def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable ) test_on_batch def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) test_step def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name: m . result () for m in self . metrics } to_json def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs ) to_yaml def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' ) train_on_batch def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) train_step def train_step ( self , data ) View Source def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } DeTrResnet50 class DeTrResnet50 ( num_classes , num_queries = 100 , ** kwargs ) End-to-End Object Detection with Transformers You can use it as follow: model = DeTrResnet50Pytorch ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries. Call arguments Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Call returns Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. Ancestors (in MRO) kerod.model.detr.DeTr keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector Methods call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } compile def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile compute_loss def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> int Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss compute_metrics def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics evaluate def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) evaluate_generator def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) fit def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history fit_generator def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch ) get_layer def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' ) load_weights def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status make_predict_function def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function make_test_function def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function make_train_function def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function predict def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs ) predict_generator def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) predict_on_batch def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs ) predict_step def predict_step ( self , data ) Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Type Description boxes A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. scores: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. classes: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def predict_step ( self , data ) : \"\"\" Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed . It means that if background was predicted the second maximum score would be outputed . Example : background + 3 classes [ 0 . 54 , 0 . 40 , 0 . 03 , 0 . 03 ] => score = 0 . 40 , label = 0 ( 1 - 1 ) \" To optimize for AP, we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4 . Experiments of Object Detection with Transformers Returns : boxes : A Tensor of shape [ batch_size , self . num_queries , ( y1 , x1 , y2 , x2 ) ] containing the boxes with the coordinates between 0 and 1 . scores : A Tensor of shape [ batch_size , self . num_queries ] containing the score of the boxes . classes : A Tensor of shape [ batch_size , self . num_queries ] containing the class of the boxes [ 0 , num_classes ) . \"\"\" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ], y_pred [ BoxField . SCORES ], x [ DatasetField . IMAGES_INFO ], tf . shape ( x [ DatasetField . IMAGES ] ) [ 1 : 3 ], ) return boxes_without_padding , scores , labels reset_metrics def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state () reset_states def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states () save def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. Example: from keras.models import load_model model . save ( 'my_model.h5' ) # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model ( 'my_model.h5' ) View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) : # pylint: disable=line-too-long \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" # pylint: enable=line-too-long save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces ) save_spec def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False ) save_weights def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ]) summary def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable ) test_on_batch def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) test_step def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name: m . result () for m in self . metrics } to_json def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs ) to_yaml def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' ) train_on_batch def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) train_step def train_step ( self , data ) View Source def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } DeTrResnet50Pytorch class DeTrResnet50Pytorch ( num_classes , num_queries = 100 , ** kwargs ) End-to-End Object Detection with Transformers You can use it as follow: model = DeTrResnet50Pytorch ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries. Call arguments Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Call returns Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. Ancestors (in MRO) kerod.model.detr.DeTr keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector Methods call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } compile def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile compute_loss def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> int Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss compute_metrics def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics evaluate def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) evaluate_generator def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) fit def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history fit_generator def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch ) get_layer def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' ) load_weights def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status make_predict_function def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function make_test_function def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function make_train_function def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function predict def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs ) predict_generator def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) predict_on_batch def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs ) predict_step def predict_step ( self , data ) Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Type Description boxes A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. scores: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. classes: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def predict_step ( self , data ) : \"\"\" Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed . It means that if background was predicted the second maximum score would be outputed . Example : background + 3 classes [ 0 . 54 , 0 . 40 , 0 . 03 , 0 . 03 ] => score = 0 . 40 , label = 0 ( 1 - 1 ) \" To optimize for AP, we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4 . Experiments of Object Detection with Transformers Returns : boxes : A Tensor of shape [ batch_size , self . num_queries , ( y1 , x1 , y2 , x2 ) ] containing the boxes with the coordinates between 0 and 1 . scores : A Tensor of shape [ batch_size , self . num_queries ] containing the score of the boxes . classes : A Tensor of shape [ batch_size , self . num_queries ] containing the class of the boxes [ 0 , num_classes ) . \"\"\" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ], y_pred [ BoxField . SCORES ], x [ DatasetField . IMAGES_INFO ], tf . shape ( x [ DatasetField . IMAGES ] ) [ 1 : 3 ], ) return boxes_without_padding , scores , labels reset_metrics def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state () reset_states def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states () save def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. Example: from keras.models import load_model model . save ( 'my_model.h5' ) # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model ( 'my_model.h5' ) View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) : # pylint: disable=line-too-long \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" # pylint: enable=line-too-long save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces ) save_spec def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False ) save_weights def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ]) summary def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable ) test_on_batch def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) test_step def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name: m . result () for m in self . metrics } to_json def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs ) to_yaml def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' ) train_on_batch def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) train_step def train_step ( self , data ) View Source def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics }","title":"Detr"},{"location":"reference/kerod/model/detr/#module-kerodmodeldetr","text":"None None View Source from typing import Dict import tensorflow as tf from tensorflow.keras.losses import SparseCategoricalCrossentropy from tensorflow.python.keras.engine import data_adapter from tensorflow_addons.losses.giou_loss import GIoULoss from kerod.core.box_ops import ( convert_to_center_coordinates , convert_to_xyxy_coordinates ) from kerod.core.losses import L1Loss from kerod.core.matcher import hungarian_matching from kerod.core.similarity import DetrSimilarity from kerod.core.standard_fields import BoxField , DatasetField from kerod.core.target_assigner import TargetAssigner from kerod.layers import PositionEmbeddingSine , Transformer from kerod.layers.post_processing.post_processing_detr import \\ post_processing as detr_postprocessing from kerod.model.backbone.resnet import ResNet50 , ResNet50PytorchStyle from kerod.utils import item_assignment from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class DeTr ( tf . keras . Model ): \"\"\"Build a DeTr model according to the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) You can use it as follow: ```python model = DeTrResnet50Pytorch(80) base_lr = 0.1 optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr) model.compile(optimizer=optimizer, loss=None) model.fit(ds_train, validation_data=ds_test, epochs=11,) ``` Arguments: num_classes: The number of classes of your dataset (**do not include the background class** it is handle for you) backbone: A vision model like ResNet50. num_queries: number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries. Call arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Call returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" def __init__ ( self , num_classes : int , backbone , num_queries = 100 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_classes = num_classes self . num_queries = num_queries self . hidden_dim = 256 self . backbone = backbone self . input_proj = tf . keras . layers . Conv2D ( self . hidden_dim , 1 ) self . pos_embed = PositionEmbeddingSine ( output_dim = self . hidden_dim ) self . transformer_num_layers = 6 self . transformer = Transformer ( num_layers = self . transformer_num_layers , d_model = self . hidden_dim , num_heads = 8 , dim_feedforward = 2048 ) self . bbox_embed = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( 4 , activation = 'sigmoid' , dtype = tf . float32 ) # (x1, y1, x2, y2) ]) self . class_embed = tf . keras . layers . Dense ( num_classes + 1 , dtype = tf . float32 ) # Will create a learnable embedding matrix for all our queries # It is a matrix of [num_queries, self.hidden_dim] # The embedding layers self . query_embed = tf . keras . layers . Embedding ( num_queries , self . hidden_dim , embeddings_initializer = tf . keras . initializers . RandomNormal ( mean = 0. , stddev = 1. )) self . all_the_queries = tf . range ( num_queries ) # Loss computation self . weight_class , self . weight_l1 , self . weight_giou = 1 , 5 , 2 similarity_func = DetrSimilarity ( self . weight_class , self . weight_l1 , self . weight_giou ) self . target_assigner = TargetAssigner ( similarity_func , hungarian_matching , lambda gt , pred : gt , negative_class_weight = 1.0 ) # Relative classification weight applied to the no-object category # It down-weight the log-probability term of a no-object # by a factor 10 to account for class imbalance self . non_object_weight = tf . constant ( 0.1 , dtype = self . compute_dtype ) # Losses self . giou = GIoULoss ( reduction = tf . keras . losses . Reduction . NONE ) self . l1 = L1Loss ( reduction = tf . keras . losses . Reduction . NONE ) self . scc = SparseCategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE , from_logits = True ) # Metrics self . giou_metric = tf . keras . metrics . Mean ( name = \"giou_last_layer\" ) self . l1_metric = tf . keras . metrics . Mean ( name = \"l1_last_layer\" ) self . scc_metric = tf . keras . metrics . Mean ( name = \"scc_last_layer\" ) self . loss_metric = tf . keras . metrics . Mean ( name = \"loss\" ) self . precision_metric = tf . keras . metrics . SparseCategoricalAccuracy () # Object recall = foreground self . recall_metric = tf . keras . metrics . Mean ( name = \"object_recall\" ) @property def metrics ( self ): return [ self . loss_metric , self . giou_metric , self . l1_metric , self . scc_metric , self . precision_metric , self . recall_metric ] def call ( self , inputs , training = None ): \"\"\"Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images )[ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images )[ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ... , None ], tf . float32 ), tf . shape ( x )[ 1 : 3 ], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ], ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } def compute_loss ( self , ground_truths : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ], input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField . BOXES ] / tf . tile ( input_shape [ None ], [ 1 , 2 ]) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField . BOXES ], self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField . SCORES ], self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [{ BoxField . BOXES : boxes , BoxField . SCORES : logits } for boxes , logits in zip ( boxes_per_lvl , logits_per_lvl )] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField . NUM_BOXES ]), tf . float32 ) loss = 0 # Compute the Giou, L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ): # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss def _compute_loss ( self , y_pred : Dict [ str , tf . Tensor ], ground_truths : Dict [ str , tf . Tensor ], num_boxes : int , compute_metrics = False , ): y_true , weights = self . target_assigner . assign ( y_pred , ground_truths ) # Reduce the class imbalanced by applying to the weights # self.non_object_weight for the non object (pos 0) weights [ BoxField . LABELS ] = item_assignment ( weights [ BoxField . LABELS ], y_true [ BoxField . LABELS ] == 0 , self . non_object_weight ) # Caveats GIoU is buggy and if the batch_size is 1 and the sample_weight # is provided will raise an error giou = self . giou ( convert_to_xyxy_coordinates ( y_true [ BoxField . BOXES ]), convert_to_xyxy_coordinates ( y_pred [ BoxField . BOXES ]), sample_weight = weights [ BoxField . BOXES ]) l1 = self . l1 ( y_true [ BoxField . BOXES ], y_pred [ BoxField . BOXES ], sample_weight = weights [ BoxField . BOXES ]) # SparseCategoricalCrossentropy scc = self . scc ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) giou = self . weight_giou * tf . reduce_sum ( giou ) / num_boxes l1 = self . weight_l1 * tf . reduce_sum ( l1 ) / num_boxes scc = self . weight_class * tf . reduce_sum ( scc ) / tf . reduce_sum ( weights [ BoxField . LABELS ]) if compute_metrics : self . giou_metric . update_state ( giou ) self . l1_metric . update_state ( l1 ) self . scc_metric . update_state ( scc ) self . precision_metric . update_state ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) recall = compute_detr_metrics ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ]) self . recall_metric . update_state ( recall ) return giou + l1 + scc def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def test_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def predict_step ( self , data ): \"\"\"Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: boxes: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. scores: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. classes: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\"\" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ], y_pred [ BoxField . SCORES ], x [ DatasetField . IMAGES_INFO ], tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], ) return boxes_without_padding , scores , labels class DeTrResnet50 ( DeTr ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50 ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) class DeTrResnet50Pytorch ( DeTr ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50PytorchStyle ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) def compute_detr_metrics ( y_true : tf . Tensor , y_pred : tf . Tensor ): \"\"\"Useful metrics that allows to track how behave the training. Arguments: y_true: A one-hot encoded vector with shape [batch_size, num_object_queries, num_classes] y_pred: A tensor with shape [batch_size, num_object_queries, num_classes], representing the classification logits. Returns: tf.Tensor: Recall Among all the boxes that we had to find how much did we found. \"\"\" #Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis =- 1 , name = 'label_prediction' , output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) recall = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name = 'recall' ) return recall remove_unwanted_doc ( DeTr , __pdoc__ ) remove_unwanted_doc ( DeTrResnet50 , __pdoc__ ) remove_unwanted_doc ( DeTrResnet50Pytorch , __pdoc__ )","title":"Module kerod.model.detr"},{"location":"reference/kerod/model/detr/#functions","text":"","title":"Functions"},{"location":"reference/kerod/model/detr/#compute_detr_metrics","text":"def compute_detr_metrics ( y_true : tensorflow . python . framework . ops . Tensor , y_pred : tensorflow . python . framework . ops . Tensor ) Useful metrics that allows to track how behave the training. Parameters: Name Description y_true A one-hot encoded vector with shape [batch_size, num_object_queries, num_classes] y_pred A tensor with shape [batch_size, num_object_queries, num_classes], representing the classification logits. Returns: Type Description tf.Tensor Recall Among all the boxes that we had to find how much did we found. View Source def compute_detr_metrics ( y_true: tf . Tensor , y_pred: tf . Tensor ) : \"\"\"Useful metrics that allows to track how behave the training. Arguments: y_true: A one - hot encoded vector with shape [ batch_size , num_object_queries , num_classes ] y_pred: A tensor with shape [ batch_size , num_object_queries , num_classes ], representing the classification logits . Returns: tf . Tensor: Recall Among all the boxes that we had to find how much did we found . \"\"\" # Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis =- 1 , name = ' label_prediction ', output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) recall = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name = ' recall ') return recall","title":"compute_detr_metrics"},{"location":"reference/kerod/model/detr/#classes","text":"","title":"Classes"},{"location":"reference/kerod/model/detr/#detr","text":"class DeTr ( num_classes : int , backbone , num_queries = 100 , ** kwargs ) End-to-End Object Detection with Transformers You can use it as follow: model = DeTrResnet50Pytorch ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"DeTr"},{"location":"reference/kerod/model/detr/#arguments","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries.","title":"Arguments"},{"location":"reference/kerod/model/detr/#call-arguments","text":"Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode","title":"Call arguments"},{"location":"reference/kerod/model/detr/#call-returns","text":"Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise.","title":"Call returns"},{"location":"reference/kerod/model/detr/#ancestors-in-mro","text":"keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/detr/#descendants","text":"kerod.model.detr.DeTrResnet50 kerod.model.detr.DeTrResnet50Pytorch","title":"Descendants"},{"location":"reference/kerod/model/detr/#methods","text":"","title":"Methods"},{"location":"reference/kerod/model/detr/#call","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , }","title":"call"},{"location":"reference/kerod/model/detr/#compile","text":"def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile","title":"compile"},{"location":"reference/kerod/model/detr/#compute_loss","text":"def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> int Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss","title":"compute_loss"},{"location":"reference/kerod/model/detr/#compute_metrics","text":"def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics","title":"compute_metrics"},{"location":"reference/kerod/model/detr/#evaluate","text":"def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"evaluate"},{"location":"reference/kerod/model/detr/#evaluate_generator","text":"def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"evaluate_generator"},{"location":"reference/kerod/model/detr/#fit","text":"def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history","title":"fit"},{"location":"reference/kerod/model/detr/#fit_generator","text":"def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch )","title":"fit_generator"},{"location":"reference/kerod/model/detr/#get_layer","text":"def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' )","title":"get_layer"},{"location":"reference/kerod/model/detr/#load_weights","text":"def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status","title":"load_weights"},{"location":"reference/kerod/model/detr/#make_predict_function","text":"def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function","title":"make_predict_function"},{"location":"reference/kerod/model/detr/#make_test_function","text":"def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function","title":"make_test_function"},{"location":"reference/kerod/model/detr/#make_train_function","text":"def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function","title":"make_train_function"},{"location":"reference/kerod/model/detr/#predict","text":"def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs )","title":"predict"},{"location":"reference/kerod/model/detr/#predict_generator","text":"def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"predict_generator"},{"location":"reference/kerod/model/detr/#predict_on_batch","text":"def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs )","title":"predict_on_batch"},{"location":"reference/kerod/model/detr/#predict_step","text":"def predict_step ( self , data ) Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Type Description boxes A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. scores: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. classes: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def predict_step ( self , data ) : \"\"\" Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed . It means that if background was predicted the second maximum score would be outputed . Example : background + 3 classes [ 0 . 54 , 0 . 40 , 0 . 03 , 0 . 03 ] => score = 0 . 40 , label = 0 ( 1 - 1 ) \" To optimize for AP, we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4 . Experiments of Object Detection with Transformers Returns : boxes : A Tensor of shape [ batch_size , self . num_queries , ( y1 , x1 , y2 , x2 ) ] containing the boxes with the coordinates between 0 and 1 . scores : A Tensor of shape [ batch_size , self . num_queries ] containing the score of the boxes . classes : A Tensor of shape [ batch_size , self . num_queries ] containing the class of the boxes [ 0 , num_classes ) . \"\"\" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ], y_pred [ BoxField . SCORES ], x [ DatasetField . IMAGES_INFO ], tf . shape ( x [ DatasetField . IMAGES ] ) [ 1 : 3 ], ) return boxes_without_padding , scores , labels","title":"predict_step"},{"location":"reference/kerod/model/detr/#reset_metrics","text":"def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state ()","title":"reset_metrics"},{"location":"reference/kerod/model/detr/#reset_states","text":"def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states ()","title":"reset_states"},{"location":"reference/kerod/model/detr/#save","text":"def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. Example: from keras.models import load_model model . save ( 'my_model.h5' ) # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model ( 'my_model.h5' ) View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) : # pylint: disable=line-too-long \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" # pylint: enable=line-too-long save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces )","title":"save"},{"location":"reference/kerod/model/detr/#save_spec","text":"def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False )","title":"save_spec"},{"location":"reference/kerod/model/detr/#save_weights","text":"def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ])","title":"save_weights"},{"location":"reference/kerod/model/detr/#summary","text":"def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable )","title":"summary"},{"location":"reference/kerod/model/detr/#test_on_batch","text":"def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"test_on_batch"},{"location":"reference/kerod/model/detr/#test_step","text":"def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name: m . result () for m in self . metrics }","title":"test_step"},{"location":"reference/kerod/model/detr/#to_json","text":"def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs )","title":"to_json"},{"location":"reference/kerod/model/detr/#to_yaml","text":"def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' )","title":"to_yaml"},{"location":"reference/kerod/model/detr/#train_on_batch","text":"def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"train_on_batch"},{"location":"reference/kerod/model/detr/#train_step","text":"def train_step ( self , data ) View Source def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics }","title":"train_step"},{"location":"reference/kerod/model/detr/#detrresnet50","text":"class DeTrResnet50 ( num_classes , num_queries = 100 , ** kwargs ) End-to-End Object Detection with Transformers You can use it as follow: model = DeTrResnet50Pytorch ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"DeTrResnet50"},{"location":"reference/kerod/model/detr/#arguments_1","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries.","title":"Arguments"},{"location":"reference/kerod/model/detr/#call-arguments_1","text":"Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode","title":"Call arguments"},{"location":"reference/kerod/model/detr/#call-returns_1","text":"Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise.","title":"Call returns"},{"location":"reference/kerod/model/detr/#ancestors-in-mro_1","text":"kerod.model.detr.DeTr keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/detr/#methods_1","text":"","title":"Methods"},{"location":"reference/kerod/model/detr/#call_1","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , }","title":"call"},{"location":"reference/kerod/model/detr/#compile_1","text":"def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile","title":"compile"},{"location":"reference/kerod/model/detr/#compute_loss_1","text":"def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> int Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss","title":"compute_loss"},{"location":"reference/kerod/model/detr/#compute_metrics_1","text":"def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics","title":"compute_metrics"},{"location":"reference/kerod/model/detr/#evaluate_1","text":"def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"evaluate"},{"location":"reference/kerod/model/detr/#evaluate_generator_1","text":"def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"evaluate_generator"},{"location":"reference/kerod/model/detr/#fit_1","text":"def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history","title":"fit"},{"location":"reference/kerod/model/detr/#fit_generator_1","text":"def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch )","title":"fit_generator"},{"location":"reference/kerod/model/detr/#get_layer_1","text":"def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' )","title":"get_layer"},{"location":"reference/kerod/model/detr/#load_weights_1","text":"def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status","title":"load_weights"},{"location":"reference/kerod/model/detr/#make_predict_function_1","text":"def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function","title":"make_predict_function"},{"location":"reference/kerod/model/detr/#make_test_function_1","text":"def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function","title":"make_test_function"},{"location":"reference/kerod/model/detr/#make_train_function_1","text":"def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function","title":"make_train_function"},{"location":"reference/kerod/model/detr/#predict_1","text":"def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs )","title":"predict"},{"location":"reference/kerod/model/detr/#predict_generator_1","text":"def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"predict_generator"},{"location":"reference/kerod/model/detr/#predict_on_batch_1","text":"def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs )","title":"predict_on_batch"},{"location":"reference/kerod/model/detr/#predict_step_1","text":"def predict_step ( self , data ) Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Type Description boxes A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. scores: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. classes: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def predict_step ( self , data ) : \"\"\" Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed . It means that if background was predicted the second maximum score would be outputed . Example : background + 3 classes [ 0 . 54 , 0 . 40 , 0 . 03 , 0 . 03 ] => score = 0 . 40 , label = 0 ( 1 - 1 ) \" To optimize for AP, we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4 . Experiments of Object Detection with Transformers Returns : boxes : A Tensor of shape [ batch_size , self . num_queries , ( y1 , x1 , y2 , x2 ) ] containing the boxes with the coordinates between 0 and 1 . scores : A Tensor of shape [ batch_size , self . num_queries ] containing the score of the boxes . classes : A Tensor of shape [ batch_size , self . num_queries ] containing the class of the boxes [ 0 , num_classes ) . \"\"\" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ], y_pred [ BoxField . SCORES ], x [ DatasetField . IMAGES_INFO ], tf . shape ( x [ DatasetField . IMAGES ] ) [ 1 : 3 ], ) return boxes_without_padding , scores , labels","title":"predict_step"},{"location":"reference/kerod/model/detr/#reset_metrics_1","text":"def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state ()","title":"reset_metrics"},{"location":"reference/kerod/model/detr/#reset_states_1","text":"def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states ()","title":"reset_states"},{"location":"reference/kerod/model/detr/#save_1","text":"def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. Example: from keras.models import load_model model . save ( 'my_model.h5' ) # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model ( 'my_model.h5' ) View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) : # pylint: disable=line-too-long \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" # pylint: enable=line-too-long save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces )","title":"save"},{"location":"reference/kerod/model/detr/#save_spec_1","text":"def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False )","title":"save_spec"},{"location":"reference/kerod/model/detr/#save_weights_1","text":"def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ])","title":"save_weights"},{"location":"reference/kerod/model/detr/#summary_1","text":"def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable )","title":"summary"},{"location":"reference/kerod/model/detr/#test_on_batch_1","text":"def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"test_on_batch"},{"location":"reference/kerod/model/detr/#test_step_1","text":"def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name: m . result () for m in self . metrics }","title":"test_step"},{"location":"reference/kerod/model/detr/#to_json_1","text":"def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs )","title":"to_json"},{"location":"reference/kerod/model/detr/#to_yaml_1","text":"def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' )","title":"to_yaml"},{"location":"reference/kerod/model/detr/#train_on_batch_1","text":"def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"train_on_batch"},{"location":"reference/kerod/model/detr/#train_step_1","text":"def train_step ( self , data ) View Source def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics }","title":"train_step"},{"location":"reference/kerod/model/detr/#detrresnet50pytorch","text":"class DeTrResnet50Pytorch ( num_classes , num_queries = 100 , ** kwargs ) End-to-End Object Detection with Transformers You can use it as follow: model = DeTrResnet50Pytorch ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"DeTrResnet50Pytorch"},{"location":"reference/kerod/model/detr/#arguments_2","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries.","title":"Arguments"},{"location":"reference/kerod/model/detr/#call-arguments_2","text":"Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode","title":"Call arguments"},{"location":"reference/kerod/model/detr/#call-returns_2","text":"Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise.","title":"Call returns"},{"location":"reference/kerod/model/detr/#ancestors-in-mro_2","text":"kerod.model.detr.DeTr keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/detr/#methods_2","text":"","title":"Methods"},{"location":"reference/kerod/model/detr/#call_2","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , }","title":"call"},{"location":"reference/kerod/model/detr/#compile_2","text":"def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile","title":"compile"},{"location":"reference/kerod/model/detr/#compute_loss_2","text":"def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> int Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss","title":"compute_loss"},{"location":"reference/kerod/model/detr/#compute_metrics_2","text":"def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics","title":"compute_metrics"},{"location":"reference/kerod/model/detr/#evaluate_2","text":"def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"evaluate"},{"location":"reference/kerod/model/detr/#evaluate_generator_2","text":"def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"evaluate_generator"},{"location":"reference/kerod/model/detr/#fit_2","text":"def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history","title":"fit"},{"location":"reference/kerod/model/detr/#fit_generator_2","text":"def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch )","title":"fit_generator"},{"location":"reference/kerod/model/detr/#get_layer_2","text":"def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' )","title":"get_layer"},{"location":"reference/kerod/model/detr/#load_weights_2","text":"def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status","title":"load_weights"},{"location":"reference/kerod/model/detr/#make_predict_function_2","text":"def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function","title":"make_predict_function"},{"location":"reference/kerod/model/detr/#make_test_function_2","text":"def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function","title":"make_test_function"},{"location":"reference/kerod/model/detr/#make_train_function_2","text":"def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function","title":"make_train_function"},{"location":"reference/kerod/model/detr/#predict_2","text":"def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs )","title":"predict"},{"location":"reference/kerod/model/detr/#predict_generator_2","text":"def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"predict_generator"},{"location":"reference/kerod/model/detr/#predict_on_batch_2","text":"def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs )","title":"predict_on_batch"},{"location":"reference/kerod/model/detr/#predict_step_2","text":"def predict_step ( self , data ) Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Type Description boxes A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. scores: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. classes: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def predict_step ( self , data ) : \"\"\" Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed . It means that if background was predicted the second maximum score would be outputed . Example : background + 3 classes [ 0 . 54 , 0 . 40 , 0 . 03 , 0 . 03 ] => score = 0 . 40 , label = 0 ( 1 - 1 ) \" To optimize for AP, we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4 . Experiments of Object Detection with Transformers Returns : boxes : A Tensor of shape [ batch_size , self . num_queries , ( y1 , x1 , y2 , x2 ) ] containing the boxes with the coordinates between 0 and 1 . scores : A Tensor of shape [ batch_size , self . num_queries ] containing the score of the boxes . classes : A Tensor of shape [ batch_size , self . num_queries ] containing the class of the boxes [ 0 , num_classes ) . \"\"\" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ], y_pred [ BoxField . SCORES ], x [ DatasetField . IMAGES_INFO ], tf . shape ( x [ DatasetField . IMAGES ] ) [ 1 : 3 ], ) return boxes_without_padding , scores , labels","title":"predict_step"},{"location":"reference/kerod/model/detr/#reset_metrics_2","text":"def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state ()","title":"reset_metrics"},{"location":"reference/kerod/model/detr/#reset_states_2","text":"def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states ()","title":"reset_states"},{"location":"reference/kerod/model/detr/#save_2","text":"def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. Example: from keras.models import load_model model . save ( 'my_model.h5' ) # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model ( 'my_model.h5' ) View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) : # pylint: disable=line-too-long \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" # pylint: enable=line-too-long save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces )","title":"save"},{"location":"reference/kerod/model/detr/#save_spec_2","text":"def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False )","title":"save_spec"},{"location":"reference/kerod/model/detr/#save_weights_2","text":"def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ])","title":"save_weights"},{"location":"reference/kerod/model/detr/#summary_2","text":"def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable )","title":"summary"},{"location":"reference/kerod/model/detr/#test_on_batch_2","text":"def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"test_on_batch"},{"location":"reference/kerod/model/detr/#test_step_2","text":"def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name: m . result () for m in self . metrics }","title":"test_step"},{"location":"reference/kerod/model/detr/#to_json_2","text":"def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs )","title":"to_json"},{"location":"reference/kerod/model/detr/#to_yaml_2","text":"def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' )","title":"to_yaml"},{"location":"reference/kerod/model/detr/#train_on_batch_2","text":"def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"train_on_batch"},{"location":"reference/kerod/model/detr/#train_step_2","text":"def train_step ( self , data ) View Source def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics }","title":"train_step"},{"location":"reference/kerod/model/factory/","text":"Module kerod.model.factory None None View Source from enum import Enum import tensorflow as tf from kerod.model.detr import DeTrResnet50 , DeTrResnet50Pytorch from kerod.model.faster_rcnn import ( FasterRcnnFPNResnet50Caffe , FasterRcnnFPNResnet50Pytorch ) from kerod.model.smca_detr import SMCAR50Pytorch from kerod.utils.training import ( freeze_batch_normalization , freeze_layers_before ) class KerodModel ( str , Enum ): faster_rcnn_resnet50_pytorch = 'resnet50_pytorch' faster_rcnn_resnet50_caffe = 'resnet50_caffe' detr_resnet50 = 'detr_resnet50_pytorch' detr_resnet50_caffe = 'detr_resnet50' smca_r50 = 'smca_resnet50' def build_model ( num_classes : int , name : str = KerodModel . faster_rcnn_resnet50_pytorch . value ) -> tf . keras . Model : \"\"\"Build a localization model with all the `tf.keras.layers.BatchNormalization` frozen and all the layers before second residual block. Args: num_classes: Number of classes of your model. Do not include the background class. name: Target model that you wish to use: 'resnet50_pytorch', 'resnet50_caffe', 'detrresnet50_pytorch', 'smca_resnet50'. Returns: A `keras.Model` instance. Raises: NotImplementedError: If the provided isn't supported \"\"\" if name == KerodModel . faster_rcnn_resnet50_pytorch : model = FasterRcnnFPNResnet50Pytorch ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'resnet50/group0/block2/last_relu' ) return model elif name == KerodModel . faster_rcnn_resnet50_caffe : model = FasterRcnnFPNResnet50Caffe ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'conv2_block3_out' ) return model elif name == KerodModel . detr_resnet50 : model = DeTrResnet50Pytorch ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'resnet50/group0/block2/last_relu' ) return model elif name == KerodModel . detr_resnet50_caffe : model = DeTrResnet50 ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'conv2_block3_out' ) return model elif name == KerodModel . smca_r50 : model = SMCAR50Pytorch ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'resnet50/group0/block2/last_relu' ) return model raise NotImplementedError ( f 'Name: { name } is not implemented.' ) Functions build_model def build_model ( num_classes : int , name : str = 'resnet50_pytorch' ) -> keras . engine . training . Model Build a localization model with all the tf.keras.layers.BatchNormalization frozen and all the layers before second residual block. Parameters: Name Description num_classes Number of classes of your model. Do not include the background class. name Target model that you wish to use: 'resnet50_pytorch', 'resnet50_caffe', 'detrresnet50_pytorch', 'smca_resnet50'. Returns: Type Description None A keras.Model instance. Raises: Type Description NotImplementedError If the provided isn't supported View Source def build_model ( num_classes : int , name : str = KerodModel . faster_rcnn_resnet50_pytorch . value ) -> tf . keras . Model : \"\"\"Build a localization model with all the `tf.keras.layers.BatchNormalization` frozen and all the layers before second residual block. Args: num_classes: Number of classes of your model. Do not include the background class. name: Target model that you wish to use: ' resnet50_pytorch ', ' resnet50_caffe ', ' detrresnet50_pytorch ', ' smca_resnet50 '. Returns: A `keras.Model` instance. Raises: NotImplementedError: If the provided isn' t supported \"\"\" if name == KerodModel.faster_rcnn_resnet50_pytorch: model = FasterRcnnFPNResnet50Pytorch(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' resnet50 / group0 / block2 / last_relu ') return model elif name == KerodModel.faster_rcnn_resnet50_caffe: model = FasterRcnnFPNResnet50Caffe(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' conv2_block3_out ') return model elif name == KerodModel.detr_resnet50: model = DeTrResnet50Pytorch(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' resnet50 / group0 / block2 / last_relu ') return model elif name == KerodModel.detr_resnet50_caffe: model = DeTrResnet50(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' conv2_block3_out ') return model elif name == KerodModel.smca_r50: model = SMCAR50Pytorch(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' resnet50 / group0 / block2 / last_relu ') return model raise NotImplementedError(f' Name : { name } is not implemented . ') Classes KerodModel class KerodModel ( / , * args , ** kwargs ) Ancestors (in MRO) builtins.str enum.Enum Class variables detr_resnet50 detr_resnet50_caffe faster_rcnn_resnet50_caffe faster_rcnn_resnet50_pytorch name smca_r50 value","title":"Factory"},{"location":"reference/kerod/model/factory/#module-kerodmodelfactory","text":"None None View Source from enum import Enum import tensorflow as tf from kerod.model.detr import DeTrResnet50 , DeTrResnet50Pytorch from kerod.model.faster_rcnn import ( FasterRcnnFPNResnet50Caffe , FasterRcnnFPNResnet50Pytorch ) from kerod.model.smca_detr import SMCAR50Pytorch from kerod.utils.training import ( freeze_batch_normalization , freeze_layers_before ) class KerodModel ( str , Enum ): faster_rcnn_resnet50_pytorch = 'resnet50_pytorch' faster_rcnn_resnet50_caffe = 'resnet50_caffe' detr_resnet50 = 'detr_resnet50_pytorch' detr_resnet50_caffe = 'detr_resnet50' smca_r50 = 'smca_resnet50' def build_model ( num_classes : int , name : str = KerodModel . faster_rcnn_resnet50_pytorch . value ) -> tf . keras . Model : \"\"\"Build a localization model with all the `tf.keras.layers.BatchNormalization` frozen and all the layers before second residual block. Args: num_classes: Number of classes of your model. Do not include the background class. name: Target model that you wish to use: 'resnet50_pytorch', 'resnet50_caffe', 'detrresnet50_pytorch', 'smca_resnet50'. Returns: A `keras.Model` instance. Raises: NotImplementedError: If the provided isn't supported \"\"\" if name == KerodModel . faster_rcnn_resnet50_pytorch : model = FasterRcnnFPNResnet50Pytorch ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'resnet50/group0/block2/last_relu' ) return model elif name == KerodModel . faster_rcnn_resnet50_caffe : model = FasterRcnnFPNResnet50Caffe ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'conv2_block3_out' ) return model elif name == KerodModel . detr_resnet50 : model = DeTrResnet50Pytorch ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'resnet50/group0/block2/last_relu' ) return model elif name == KerodModel . detr_resnet50_caffe : model = DeTrResnet50 ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'conv2_block3_out' ) return model elif name == KerodModel . smca_r50 : model = SMCAR50Pytorch ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'resnet50/group0/block2/last_relu' ) return model raise NotImplementedError ( f 'Name: { name } is not implemented.' )","title":"Module kerod.model.factory"},{"location":"reference/kerod/model/factory/#functions","text":"","title":"Functions"},{"location":"reference/kerod/model/factory/#build_model","text":"def build_model ( num_classes : int , name : str = 'resnet50_pytorch' ) -> keras . engine . training . Model Build a localization model with all the tf.keras.layers.BatchNormalization frozen and all the layers before second residual block. Parameters: Name Description num_classes Number of classes of your model. Do not include the background class. name Target model that you wish to use: 'resnet50_pytorch', 'resnet50_caffe', 'detrresnet50_pytorch', 'smca_resnet50'. Returns: Type Description None A keras.Model instance. Raises: Type Description NotImplementedError If the provided isn't supported View Source def build_model ( num_classes : int , name : str = KerodModel . faster_rcnn_resnet50_pytorch . value ) -> tf . keras . Model : \"\"\"Build a localization model with all the `tf.keras.layers.BatchNormalization` frozen and all the layers before second residual block. Args: num_classes: Number of classes of your model. Do not include the background class. name: Target model that you wish to use: ' resnet50_pytorch ', ' resnet50_caffe ', ' detrresnet50_pytorch ', ' smca_resnet50 '. Returns: A `keras.Model` instance. Raises: NotImplementedError: If the provided isn' t supported \"\"\" if name == KerodModel.faster_rcnn_resnet50_pytorch: model = FasterRcnnFPNResnet50Pytorch(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' resnet50 / group0 / block2 / last_relu ') return model elif name == KerodModel.faster_rcnn_resnet50_caffe: model = FasterRcnnFPNResnet50Caffe(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' conv2_block3_out ') return model elif name == KerodModel.detr_resnet50: model = DeTrResnet50Pytorch(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' resnet50 / group0 / block2 / last_relu ') return model elif name == KerodModel.detr_resnet50_caffe: model = DeTrResnet50(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' conv2_block3_out ') return model elif name == KerodModel.smca_r50: model = SMCAR50Pytorch(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' resnet50 / group0 / block2 / last_relu ') return model raise NotImplementedError(f' Name : { name } is not implemented . ')","title":"build_model"},{"location":"reference/kerod/model/factory/#classes","text":"","title":"Classes"},{"location":"reference/kerod/model/factory/#kerodmodel","text":"class KerodModel ( / , * args , ** kwargs )","title":"KerodModel"},{"location":"reference/kerod/model/factory/#ancestors-in-mro","text":"builtins.str enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/factory/#class-variables","text":"detr_resnet50 detr_resnet50_caffe faster_rcnn_resnet50_caffe faster_rcnn_resnet50_pytorch name smca_r50 value","title":"Class variables"},{"location":"reference/kerod/model/faster_rcnn/","text":"Module kerod.model.faster_rcnn None None View Source import tensorflow as tf from kerod.core.standard_fields import BoxField , DatasetField from kerod.model.backbone.fpn import FPN from kerod.model.backbone.resnet import ResNet50 , ResNet50PytorchStyle from kerod.layers import FastRCNN , RegionProposalNetwork from kerod.layers.post_processing import ( post_process_fast_rcnn_boxes , post_process_rpn ) from kerod.utils.documentation import remove_unwanted_doc from kerod.utils.training import apply_kernel_regularization from tensorflow.python.keras.engine import data_adapter __pdoc__ = {} class FasterRcnnFPN ( tf . keras . Model ): \"\"\"Build a FPN Resnet 50 Faster RCNN network ready to use for training. You can use it as follow: ```python model_faster_rcnn = FasterRcnnFPNResnet50(80) base_lr = 0.1 optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr) model_faster_rcnn.compile(optimizer=optimizer, loss=None) model_faster_rcnn.fit(ds_train, validation_data=ds_test, epochs=11,) ``` Arguments: num_classes: The number of classes of your dataset (**do not include the background class** it is handle for you) backbone: A tensorflow Model. \"\"\" def __init__ ( self , num_classes , backbone , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_classes = num_classes self . l2 = tf . keras . regularizers . l2 ( 1e-4 ) self . backbone = backbone self . fpn = FPN ( kernel_regularizer = self . l2 ) self . rpn = RegionProposalNetwork ( kernel_regularizer = self . l2 ) self . fast_rcnn = FastRCNN ( self . num_classes + 1 , kernel_regularizer = self . l2 ) # See docstring self.export_for_serving for usage self . _serving = False def call ( self , inputs , training = None ): \"\"\"Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ]) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ([ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]], axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ([ pyramid , rois ]) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois def train_step ( self , data ): # These are the only transformations `Model.fit` applies to user-input # data when a `tf.data.Dataset` is provided. These utilities will be exposed # publicly. data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : x [ 'ground_truths' ] = y y_pred = self ( x , training = True ) # All the losses are computed in the call. It can seems weird but it those # the job in a clean way. They are automatically added to self.losses loss = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return { m . name : m . result () for m in self . metrics } def test_step ( self , data ): data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) x [ 'ground_truths' ] = y # In our graph all the metrics are computed inside the call method # So we set training to True to benefit from those metrics # Of course there is no backpropagation at the test step y_pred = self ( x , training = True ) _ = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) return { m . name : m . result () for m in self . metrics } def predict_step ( self , data ): data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) classification_pred , localization_pred , rois = self ( x , training = False ) # Remove the background classes classification_pred = classification_pred [:, :, 1 :] return post_process_fast_rcnn_boxes ( classification_pred , localization_pred , rois , x [ DatasetField . IMAGES_INFO ], self . num_classes ) @tf . function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ]) def serving_step ( self , images , images_info ): \"\"\"Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\"\" return self . predict_step ({ DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info }) def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ): try : super () . save ( filepath , overwrite = overwrite , include_optimizer = include_optimizer , save_format = save_format , signatures = signatures , options = options ) except Exception as e : raise Exception ( 'Saving does not work with dynamic inputs the ground_truths are injected in the inputs. ' 'Please use export_model method instead to bypass this error.' ) def export_for_serving ( self , filepath ): \"\"\"Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\"\" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output }) self . _serving = False class FasterRcnnFPNResnet50Caffe ( FasterRcnnFPN ): def __init__ ( self , num_classes , ** kwargs ): resnet = ResNet50 ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , ** kwargs ) class FasterRcnnFPNResnet50Pytorch ( FasterRcnnFPN ): def __init__ ( self , num_classes , ** kwargs ): resnet = ResNet50PytorchStyle ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , ** kwargs ) remove_unwanted_doc ( FasterRcnnFPN , __pdoc__ ) remove_unwanted_doc ( FasterRcnnFPNResnet50Caffe , __pdoc__ ) remove_unwanted_doc ( FasterRcnnFPNResnet50Pytorch , __pdoc__ ) Classes FasterRcnnFPN class FasterRcnnFPN ( num_classes , backbone , ** kwargs ) You can use it as follow: model_faster_rcnn = FasterRcnnFPNResnet50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model_faster_rcnn . compile ( optimizer = optimizer , loss = None ) model_faster_rcnn . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A tensorflow Model. Ancestors (in MRO) keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector Descendants kerod.model.faster_rcnn.FasterRcnnFPNResnet50Caffe kerod.model.faster_rcnn.FasterRcnnFPNResnet50Pytorch Methods call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs A dict with the following schema: images : A Tensor of shape [batch_size, height, width, 3] image_informations : A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). ground_truths : A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training Is automatically set to True in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with add_loss and add_metrics . In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the self.compiled_loss method. Returns: Type Description Tuple - classification_pred : A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\" \" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ] ) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ( [ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]] , axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ( [ pyramid , rois ] ) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois compile def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile compute_loss def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: class MyModel ( tf . keras . Model ): def __init__ ( self , * args , ** kwargs ): super ( MyModel , self ) . __init__ ( * args , ** kwargs ) self . loss_tracker = tf . keras . metrics . Mean ( name = 'loss' ) def compute_loss ( self , x , y , y_pred , sample_weight ): loss = tf . reduce_mean ( tf . math . squared_difference ( y_pred , y )) loss += tf . add_n ( self . losses ) self . loss_tracker . update_state ( loss ) return loss def reset_metrics ( self ): self . loss_tracker . reset_states () @property def metrics ( self ): return [ self . loss_tracker ] tensors = tf . random . uniform (( 10 , 10 )), tf . random . uniform (( 10 ,)) dataset = tf . data . Dataset . from_tensor_slices ( tensors ) . repeat () . batch ( 1 ) inputs = tf . keras . layers . Input ( shape = ( 10 ,), name = 'my_input' ) outputs = tf . keras . layers . Dense ( 10 )( inputs ) model = MyModel ( inputs , outputs ) model . add_loss ( tf . reduce_sum ( outputs )) optimizer = tf . keras . optimizers . SGD () model . compile ( optimizer , loss = 'mse' , steps_per_execution = 10 ) model . fit ( dataset , epochs = 2 , steps_per_epoch = 10 ) print ( 'My custom loss: ' , model . loss_tracker . result () . numpy ()) Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model(x) ) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a tf.Tensor , or None if no loss results (which is the case when called by Model.test_step ). View Source def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) : \" \"\" Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: ```python class MyModel(tf.keras.Model): def __init__(self, *args, **kwargs): super(MyModel, self).__init__(*args, **kwargs) self.loss_tracker = tf.keras.metrics.Mean(name='loss') def compute_loss(self, x, y, y_pred, sample_weight): loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y)) loss += tf.add_n(self.losses) self.loss_tracker.update_state(loss) return loss def reset_metrics(self): self.loss_tracker.reset_states() @property def metrics(self): return [self.loss_tracker] tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,)) dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1) inputs = tf.keras.layers.Input(shape=(10,), name='my_input') outputs = tf.keras.layers.Dense(10)(inputs) model = MyModel(inputs, outputs) model.add_loss(tf.reduce_sum(outputs)) optimizer = tf.keras.optimizers.SGD() model.compile(optimizer, loss='mse', steps_per_execution=10) model.fit(dataset, epochs=2, steps_per_epoch=10) print('My custom loss: ', model.loss_tracker.result().numpy()) ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model(x)`) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a `tf.Tensor`, or `None` if no loss results (which is the case when called by `Model.test_step`). \"\" \" del x # The default implementation does not use `x`. return self . compiled_loss ( y , y_pred , sample_weight , regularization_losses = self . losses ) compute_metrics def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics evaluate def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) evaluate_generator def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) export_for_serving def export_for_serving ( self , filepath ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the training arguments is defined int the method call , tf.save_model.save method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. View Source def export_for_serving ( self , filepath ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\" \" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output } ) self . _serving = False fit def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history fit_generator def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch ) get_layer def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' ) load_weights def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status make_predict_function def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function make_test_function def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function make_train_function def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function predict def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs ) predict_generator def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) predict_on_batch def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs ) predict_step def predict_step ( self , data ) View Source def predict_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) classification_pred , localization_pred , rois = self ( x , training = False ) # Remove the background classes classification_pred = classification_pred [:, :, 1 :] return post_process_fast_rcnn_boxes ( classification_pred , localization_pred , rois , x [ DatasetField . IMAGES_INFO ], self . num_classes ) reset_metrics def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state () reset_states def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states () save def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ) View Source def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ): try : super () . save ( filepath , overwrite = overwrite , include_optimizer = include_optimizer , save_format = save_format , signatures = signatures , options = options ) except Exception as e : raise Exception ( 'Saving does not work with dynamic inputs the ground_truths are injected in the inputs. ' 'Please use export_model method instead to bypass this error.' ) save_spec def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False ) save_weights def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ]) serving_step def serving_step ( self , images , images_info ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. View Source @tf.function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ] ) def serving_step ( self , images , images_info ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\" \" return self . predict_step ( { DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info } ) summary def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable ) test_on_batch def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) test_step def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) x [' ground_truths '] = y # In our graph all the metrics are computed inside the call method # So we set training to True to benefit from those metrics # Of course there is no backpropagation at the test step y_pred = self ( x , training = True ) _ = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) return { m . name: m . result () for m in self . metrics } to_json def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs ) to_yaml def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' ) train_on_batch def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) train_step def train_step ( self , data ) View Source def train_step ( self , data ): # These are the only transformations `Model.fit` applies to user-input # data when a `tf.data.Dataset` is provided. These utilities will be exposed # publicly. data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : x [ 'ground_truths' ] = y y_pred = self ( x , training = True ) # All the losses are computed in the call. It can seems weird but it those # the job in a clean way. They are automatically added to self.losses loss = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return { m . name : m . result () for m in self . metrics } FasterRcnnFPNResnet50Caffe class FasterRcnnFPNResnet50Caffe ( num_classes , ** kwargs ) You can use it as follow: model_faster_rcnn = FasterRcnnFPNResnet50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model_faster_rcnn . compile ( optimizer = optimizer , loss = None ) model_faster_rcnn . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A tensorflow Model. Ancestors (in MRO) kerod.model.faster_rcnn.FasterRcnnFPN keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector Methods call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs A dict with the following schema: images : A Tensor of shape [batch_size, height, width, 3] image_informations : A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). ground_truths : A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training Is automatically set to True in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with add_loss and add_metrics . In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the self.compiled_loss method. Returns: Type Description Tuple - classification_pred : A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\" \" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ] ) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ( [ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]] , axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ( [ pyramid , rois ] ) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois compile def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile compute_loss def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: class MyModel ( tf . keras . Model ): def __init__ ( self , * args , ** kwargs ): super ( MyModel , self ) . __init__ ( * args , ** kwargs ) self . loss_tracker = tf . keras . metrics . Mean ( name = 'loss' ) def compute_loss ( self , x , y , y_pred , sample_weight ): loss = tf . reduce_mean ( tf . math . squared_difference ( y_pred , y )) loss += tf . add_n ( self . losses ) self . loss_tracker . update_state ( loss ) return loss def reset_metrics ( self ): self . loss_tracker . reset_states () @property def metrics ( self ): return [ self . loss_tracker ] tensors = tf . random . uniform (( 10 , 10 )), tf . random . uniform (( 10 ,)) dataset = tf . data . Dataset . from_tensor_slices ( tensors ) . repeat () . batch ( 1 ) inputs = tf . keras . layers . Input ( shape = ( 10 ,), name = 'my_input' ) outputs = tf . keras . layers . Dense ( 10 )( inputs ) model = MyModel ( inputs , outputs ) model . add_loss ( tf . reduce_sum ( outputs )) optimizer = tf . keras . optimizers . SGD () model . compile ( optimizer , loss = 'mse' , steps_per_execution = 10 ) model . fit ( dataset , epochs = 2 , steps_per_epoch = 10 ) print ( 'My custom loss: ' , model . loss_tracker . result () . numpy ()) Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model(x) ) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a tf.Tensor , or None if no loss results (which is the case when called by Model.test_step ). View Source def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) : \" \"\" Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: ```python class MyModel(tf.keras.Model): def __init__(self, *args, **kwargs): super(MyModel, self).__init__(*args, **kwargs) self.loss_tracker = tf.keras.metrics.Mean(name='loss') def compute_loss(self, x, y, y_pred, sample_weight): loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y)) loss += tf.add_n(self.losses) self.loss_tracker.update_state(loss) return loss def reset_metrics(self): self.loss_tracker.reset_states() @property def metrics(self): return [self.loss_tracker] tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,)) dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1) inputs = tf.keras.layers.Input(shape=(10,), name='my_input') outputs = tf.keras.layers.Dense(10)(inputs) model = MyModel(inputs, outputs) model.add_loss(tf.reduce_sum(outputs)) optimizer = tf.keras.optimizers.SGD() model.compile(optimizer, loss='mse', steps_per_execution=10) model.fit(dataset, epochs=2, steps_per_epoch=10) print('My custom loss: ', model.loss_tracker.result().numpy()) ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model(x)`) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a `tf.Tensor`, or `None` if no loss results (which is the case when called by `Model.test_step`). \"\" \" del x # The default implementation does not use `x`. return self . compiled_loss ( y , y_pred , sample_weight , regularization_losses = self . losses ) compute_metrics def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics evaluate def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) evaluate_generator def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) export_for_serving def export_for_serving ( self , filepath ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the training arguments is defined int the method call , tf.save_model.save method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. View Source def export_for_serving ( self , filepath ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\" \" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output } ) self . _serving = False fit def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history fit_generator def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch ) get_layer def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' ) load_weights def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status make_predict_function def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function make_test_function def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function make_train_function def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function predict def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs ) predict_generator def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) predict_on_batch def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs ) predict_step def predict_step ( self , data ) View Source def predict_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) classification_pred , localization_pred , rois = self ( x , training = False ) # Remove the background classes classification_pred = classification_pred [:, :, 1 :] return post_process_fast_rcnn_boxes ( classification_pred , localization_pred , rois , x [ DatasetField . IMAGES_INFO ], self . num_classes ) reset_metrics def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state () reset_states def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states () save def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ) View Source def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ): try : super () . save ( filepath , overwrite = overwrite , include_optimizer = include_optimizer , save_format = save_format , signatures = signatures , options = options ) except Exception as e : raise Exception ( 'Saving does not work with dynamic inputs the ground_truths are injected in the inputs. ' 'Please use export_model method instead to bypass this error.' ) save_spec def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False ) save_weights def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ]) serving_step def serving_step ( self , images , images_info ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. View Source @tf.function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ] ) def serving_step ( self , images , images_info ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\" \" return self . predict_step ( { DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info } ) summary def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable ) test_on_batch def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) test_step def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) x [' ground_truths '] = y # In our graph all the metrics are computed inside the call method # So we set training to True to benefit from those metrics # Of course there is no backpropagation at the test step y_pred = self ( x , training = True ) _ = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) return { m . name: m . result () for m in self . metrics } to_json def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs ) to_yaml def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' ) train_on_batch def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) train_step def train_step ( self , data ) View Source def train_step ( self , data ): # These are the only transformations `Model.fit` applies to user-input # data when a `tf.data.Dataset` is provided. These utilities will be exposed # publicly. data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : x [ 'ground_truths' ] = y y_pred = self ( x , training = True ) # All the losses are computed in the call. It can seems weird but it those # the job in a clean way. They are automatically added to self.losses loss = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return { m . name : m . result () for m in self . metrics } FasterRcnnFPNResnet50Pytorch class FasterRcnnFPNResnet50Pytorch ( num_classes , ** kwargs ) You can use it as follow: model_faster_rcnn = FasterRcnnFPNResnet50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model_faster_rcnn . compile ( optimizer = optimizer , loss = None ) model_faster_rcnn . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A tensorflow Model. Ancestors (in MRO) kerod.model.faster_rcnn.FasterRcnnFPN keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector Methods call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs A dict with the following schema: images : A Tensor of shape [batch_size, height, width, 3] image_informations : A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). ground_truths : A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training Is automatically set to True in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with add_loss and add_metrics . In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the self.compiled_loss method. Returns: Type Description Tuple - classification_pred : A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\" \" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ] ) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ( [ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]] , axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ( [ pyramid , rois ] ) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois compile def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile compute_loss def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: class MyModel ( tf . keras . Model ): def __init__ ( self , * args , ** kwargs ): super ( MyModel , self ) . __init__ ( * args , ** kwargs ) self . loss_tracker = tf . keras . metrics . Mean ( name = 'loss' ) def compute_loss ( self , x , y , y_pred , sample_weight ): loss = tf . reduce_mean ( tf . math . squared_difference ( y_pred , y )) loss += tf . add_n ( self . losses ) self . loss_tracker . update_state ( loss ) return loss def reset_metrics ( self ): self . loss_tracker . reset_states () @property def metrics ( self ): return [ self . loss_tracker ] tensors = tf . random . uniform (( 10 , 10 )), tf . random . uniform (( 10 ,)) dataset = tf . data . Dataset . from_tensor_slices ( tensors ) . repeat () . batch ( 1 ) inputs = tf . keras . layers . Input ( shape = ( 10 ,), name = 'my_input' ) outputs = tf . keras . layers . Dense ( 10 )( inputs ) model = MyModel ( inputs , outputs ) model . add_loss ( tf . reduce_sum ( outputs )) optimizer = tf . keras . optimizers . SGD () model . compile ( optimizer , loss = 'mse' , steps_per_execution = 10 ) model . fit ( dataset , epochs = 2 , steps_per_epoch = 10 ) print ( 'My custom loss: ' , model . loss_tracker . result () . numpy ()) Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model(x) ) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a tf.Tensor , or None if no loss results (which is the case when called by Model.test_step ). View Source def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) : \" \"\" Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: ```python class MyModel(tf.keras.Model): def __init__(self, *args, **kwargs): super(MyModel, self).__init__(*args, **kwargs) self.loss_tracker = tf.keras.metrics.Mean(name='loss') def compute_loss(self, x, y, y_pred, sample_weight): loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y)) loss += tf.add_n(self.losses) self.loss_tracker.update_state(loss) return loss def reset_metrics(self): self.loss_tracker.reset_states() @property def metrics(self): return [self.loss_tracker] tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,)) dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1) inputs = tf.keras.layers.Input(shape=(10,), name='my_input') outputs = tf.keras.layers.Dense(10)(inputs) model = MyModel(inputs, outputs) model.add_loss(tf.reduce_sum(outputs)) optimizer = tf.keras.optimizers.SGD() model.compile(optimizer, loss='mse', steps_per_execution=10) model.fit(dataset, epochs=2, steps_per_epoch=10) print('My custom loss: ', model.loss_tracker.result().numpy()) ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model(x)`) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a `tf.Tensor`, or `None` if no loss results (which is the case when called by `Model.test_step`). \"\" \" del x # The default implementation does not use `x`. return self . compiled_loss ( y , y_pred , sample_weight , regularization_losses = self . losses ) compute_metrics def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics evaluate def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) evaluate_generator def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) export_for_serving def export_for_serving ( self , filepath ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the training arguments is defined int the method call , tf.save_model.save method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. View Source def export_for_serving ( self , filepath ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\" \" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output } ) self . _serving = False fit def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history fit_generator def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch ) get_layer def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' ) load_weights def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status make_predict_function def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function make_test_function def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function make_train_function def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function predict def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs ) predict_generator def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) predict_on_batch def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs ) predict_step def predict_step ( self , data ) View Source def predict_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) classification_pred , localization_pred , rois = self ( x , training = False ) # Remove the background classes classification_pred = classification_pred [:, :, 1 :] return post_process_fast_rcnn_boxes ( classification_pred , localization_pred , rois , x [ DatasetField . IMAGES_INFO ], self . num_classes ) reset_metrics def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state () reset_states def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states () save def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ) View Source def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ): try : super () . save ( filepath , overwrite = overwrite , include_optimizer = include_optimizer , save_format = save_format , signatures = signatures , options = options ) except Exception as e : raise Exception ( 'Saving does not work with dynamic inputs the ground_truths are injected in the inputs. ' 'Please use export_model method instead to bypass this error.' ) save_spec def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False ) save_weights def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ]) serving_step def serving_step ( self , images , images_info ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. View Source @tf.function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ] ) def serving_step ( self , images , images_info ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\" \" return self . predict_step ( { DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info } ) summary def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable ) test_on_batch def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) test_step def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) x [' ground_truths '] = y # In our graph all the metrics are computed inside the call method # So we set training to True to benefit from those metrics # Of course there is no backpropagation at the test step y_pred = self ( x , training = True ) _ = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) return { m . name: m . result () for m in self . metrics } to_json def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs ) to_yaml def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' ) train_on_batch def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) train_step def train_step ( self , data ) View Source def train_step ( self , data ): # These are the only transformations `Model.fit` applies to user-input # data when a `tf.data.Dataset` is provided. These utilities will be exposed # publicly. data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : x [ 'ground_truths' ] = y y_pred = self ( x , training = True ) # All the losses are computed in the call. It can seems weird but it those # the job in a clean way. They are automatically added to self.losses loss = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return { m . name : m . result () for m in self . metrics }","title":"Faster Rcnn"},{"location":"reference/kerod/model/faster_rcnn/#module-kerodmodelfaster_rcnn","text":"None None View Source import tensorflow as tf from kerod.core.standard_fields import BoxField , DatasetField from kerod.model.backbone.fpn import FPN from kerod.model.backbone.resnet import ResNet50 , ResNet50PytorchStyle from kerod.layers import FastRCNN , RegionProposalNetwork from kerod.layers.post_processing import ( post_process_fast_rcnn_boxes , post_process_rpn ) from kerod.utils.documentation import remove_unwanted_doc from kerod.utils.training import apply_kernel_regularization from tensorflow.python.keras.engine import data_adapter __pdoc__ = {} class FasterRcnnFPN ( tf . keras . Model ): \"\"\"Build a FPN Resnet 50 Faster RCNN network ready to use for training. You can use it as follow: ```python model_faster_rcnn = FasterRcnnFPNResnet50(80) base_lr = 0.1 optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr) model_faster_rcnn.compile(optimizer=optimizer, loss=None) model_faster_rcnn.fit(ds_train, validation_data=ds_test, epochs=11,) ``` Arguments: num_classes: The number of classes of your dataset (**do not include the background class** it is handle for you) backbone: A tensorflow Model. \"\"\" def __init__ ( self , num_classes , backbone , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_classes = num_classes self . l2 = tf . keras . regularizers . l2 ( 1e-4 ) self . backbone = backbone self . fpn = FPN ( kernel_regularizer = self . l2 ) self . rpn = RegionProposalNetwork ( kernel_regularizer = self . l2 ) self . fast_rcnn = FastRCNN ( self . num_classes + 1 , kernel_regularizer = self . l2 ) # See docstring self.export_for_serving for usage self . _serving = False def call ( self , inputs , training = None ): \"\"\"Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ]) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ([ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]], axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ([ pyramid , rois ]) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois def train_step ( self , data ): # These are the only transformations `Model.fit` applies to user-input # data when a `tf.data.Dataset` is provided. These utilities will be exposed # publicly. data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : x [ 'ground_truths' ] = y y_pred = self ( x , training = True ) # All the losses are computed in the call. It can seems weird but it those # the job in a clean way. They are automatically added to self.losses loss = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return { m . name : m . result () for m in self . metrics } def test_step ( self , data ): data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) x [ 'ground_truths' ] = y # In our graph all the metrics are computed inside the call method # So we set training to True to benefit from those metrics # Of course there is no backpropagation at the test step y_pred = self ( x , training = True ) _ = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) return { m . name : m . result () for m in self . metrics } def predict_step ( self , data ): data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) classification_pred , localization_pred , rois = self ( x , training = False ) # Remove the background classes classification_pred = classification_pred [:, :, 1 :] return post_process_fast_rcnn_boxes ( classification_pred , localization_pred , rois , x [ DatasetField . IMAGES_INFO ], self . num_classes ) @tf . function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ]) def serving_step ( self , images , images_info ): \"\"\"Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\"\" return self . predict_step ({ DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info }) def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ): try : super () . save ( filepath , overwrite = overwrite , include_optimizer = include_optimizer , save_format = save_format , signatures = signatures , options = options ) except Exception as e : raise Exception ( 'Saving does not work with dynamic inputs the ground_truths are injected in the inputs. ' 'Please use export_model method instead to bypass this error.' ) def export_for_serving ( self , filepath ): \"\"\"Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\"\" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output }) self . _serving = False class FasterRcnnFPNResnet50Caffe ( FasterRcnnFPN ): def __init__ ( self , num_classes , ** kwargs ): resnet = ResNet50 ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , ** kwargs ) class FasterRcnnFPNResnet50Pytorch ( FasterRcnnFPN ): def __init__ ( self , num_classes , ** kwargs ): resnet = ResNet50PytorchStyle ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , ** kwargs ) remove_unwanted_doc ( FasterRcnnFPN , __pdoc__ ) remove_unwanted_doc ( FasterRcnnFPNResnet50Caffe , __pdoc__ ) remove_unwanted_doc ( FasterRcnnFPNResnet50Pytorch , __pdoc__ )","title":"Module kerod.model.faster_rcnn"},{"location":"reference/kerod/model/faster_rcnn/#classes","text":"","title":"Classes"},{"location":"reference/kerod/model/faster_rcnn/#fasterrcnnfpn","text":"class FasterRcnnFPN ( num_classes , backbone , ** kwargs ) You can use it as follow: model_faster_rcnn = FasterRcnnFPNResnet50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model_faster_rcnn . compile ( optimizer = optimizer , loss = None ) model_faster_rcnn . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"FasterRcnnFPN"},{"location":"reference/kerod/model/faster_rcnn/#arguments","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A tensorflow Model.","title":"Arguments"},{"location":"reference/kerod/model/faster_rcnn/#ancestors-in-mro","text":"keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/faster_rcnn/#descendants","text":"kerod.model.faster_rcnn.FasterRcnnFPNResnet50Caffe kerod.model.faster_rcnn.FasterRcnnFPNResnet50Pytorch","title":"Descendants"},{"location":"reference/kerod/model/faster_rcnn/#methods","text":"","title":"Methods"},{"location":"reference/kerod/model/faster_rcnn/#call","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs A dict with the following schema: images : A Tensor of shape [batch_size, height, width, 3] image_informations : A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). ground_truths : A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training Is automatically set to True in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with add_loss and add_metrics . In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the self.compiled_loss method. Returns: Type Description Tuple - classification_pred : A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\" \" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ] ) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ( [ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]] , axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ( [ pyramid , rois ] ) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois","title":"call"},{"location":"reference/kerod/model/faster_rcnn/#compile","text":"def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile","title":"compile"},{"location":"reference/kerod/model/faster_rcnn/#compute_loss","text":"def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: class MyModel ( tf . keras . Model ): def __init__ ( self , * args , ** kwargs ): super ( MyModel , self ) . __init__ ( * args , ** kwargs ) self . loss_tracker = tf . keras . metrics . Mean ( name = 'loss' ) def compute_loss ( self , x , y , y_pred , sample_weight ): loss = tf . reduce_mean ( tf . math . squared_difference ( y_pred , y )) loss += tf . add_n ( self . losses ) self . loss_tracker . update_state ( loss ) return loss def reset_metrics ( self ): self . loss_tracker . reset_states () @property def metrics ( self ): return [ self . loss_tracker ] tensors = tf . random . uniform (( 10 , 10 )), tf . random . uniform (( 10 ,)) dataset = tf . data . Dataset . from_tensor_slices ( tensors ) . repeat () . batch ( 1 ) inputs = tf . keras . layers . Input ( shape = ( 10 ,), name = 'my_input' ) outputs = tf . keras . layers . Dense ( 10 )( inputs ) model = MyModel ( inputs , outputs ) model . add_loss ( tf . reduce_sum ( outputs )) optimizer = tf . keras . optimizers . SGD () model . compile ( optimizer , loss = 'mse' , steps_per_execution = 10 ) model . fit ( dataset , epochs = 2 , steps_per_epoch = 10 ) print ( 'My custom loss: ' , model . loss_tracker . result () . numpy ()) Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model(x) ) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a tf.Tensor , or None if no loss results (which is the case when called by Model.test_step ). View Source def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) : \" \"\" Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: ```python class MyModel(tf.keras.Model): def __init__(self, *args, **kwargs): super(MyModel, self).__init__(*args, **kwargs) self.loss_tracker = tf.keras.metrics.Mean(name='loss') def compute_loss(self, x, y, y_pred, sample_weight): loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y)) loss += tf.add_n(self.losses) self.loss_tracker.update_state(loss) return loss def reset_metrics(self): self.loss_tracker.reset_states() @property def metrics(self): return [self.loss_tracker] tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,)) dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1) inputs = tf.keras.layers.Input(shape=(10,), name='my_input') outputs = tf.keras.layers.Dense(10)(inputs) model = MyModel(inputs, outputs) model.add_loss(tf.reduce_sum(outputs)) optimizer = tf.keras.optimizers.SGD() model.compile(optimizer, loss='mse', steps_per_execution=10) model.fit(dataset, epochs=2, steps_per_epoch=10) print('My custom loss: ', model.loss_tracker.result().numpy()) ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model(x)`) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a `tf.Tensor`, or `None` if no loss results (which is the case when called by `Model.test_step`). \"\" \" del x # The default implementation does not use `x`. return self . compiled_loss ( y , y_pred , sample_weight , regularization_losses = self . losses )","title":"compute_loss"},{"location":"reference/kerod/model/faster_rcnn/#compute_metrics","text":"def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics","title":"compute_metrics"},{"location":"reference/kerod/model/faster_rcnn/#evaluate","text":"def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"evaluate"},{"location":"reference/kerod/model/faster_rcnn/#evaluate_generator","text":"def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"evaluate_generator"},{"location":"reference/kerod/model/faster_rcnn/#export_for_serving","text":"def export_for_serving ( self , filepath ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the training arguments is defined int the method call , tf.save_model.save method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. View Source def export_for_serving ( self , filepath ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\" \" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output } ) self . _serving = False","title":"export_for_serving"},{"location":"reference/kerod/model/faster_rcnn/#fit","text":"def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history","title":"fit"},{"location":"reference/kerod/model/faster_rcnn/#fit_generator","text":"def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch )","title":"fit_generator"},{"location":"reference/kerod/model/faster_rcnn/#get_layer","text":"def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' )","title":"get_layer"},{"location":"reference/kerod/model/faster_rcnn/#load_weights","text":"def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status","title":"load_weights"},{"location":"reference/kerod/model/faster_rcnn/#make_predict_function","text":"def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function","title":"make_predict_function"},{"location":"reference/kerod/model/faster_rcnn/#make_test_function","text":"def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function","title":"make_test_function"},{"location":"reference/kerod/model/faster_rcnn/#make_train_function","text":"def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function","title":"make_train_function"},{"location":"reference/kerod/model/faster_rcnn/#predict","text":"def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs )","title":"predict"},{"location":"reference/kerod/model/faster_rcnn/#predict_generator","text":"def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"predict_generator"},{"location":"reference/kerod/model/faster_rcnn/#predict_on_batch","text":"def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs )","title":"predict_on_batch"},{"location":"reference/kerod/model/faster_rcnn/#predict_step","text":"def predict_step ( self , data ) View Source def predict_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) classification_pred , localization_pred , rois = self ( x , training = False ) # Remove the background classes classification_pred = classification_pred [:, :, 1 :] return post_process_fast_rcnn_boxes ( classification_pred , localization_pred , rois , x [ DatasetField . IMAGES_INFO ], self . num_classes )","title":"predict_step"},{"location":"reference/kerod/model/faster_rcnn/#reset_metrics","text":"def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state ()","title":"reset_metrics"},{"location":"reference/kerod/model/faster_rcnn/#reset_states","text":"def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states ()","title":"reset_states"},{"location":"reference/kerod/model/faster_rcnn/#save","text":"def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ) View Source def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ): try : super () . save ( filepath , overwrite = overwrite , include_optimizer = include_optimizer , save_format = save_format , signatures = signatures , options = options ) except Exception as e : raise Exception ( 'Saving does not work with dynamic inputs the ground_truths are injected in the inputs. ' 'Please use export_model method instead to bypass this error.' )","title":"save"},{"location":"reference/kerod/model/faster_rcnn/#save_spec","text":"def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False )","title":"save_spec"},{"location":"reference/kerod/model/faster_rcnn/#save_weights","text":"def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ])","title":"save_weights"},{"location":"reference/kerod/model/faster_rcnn/#serving_step","text":"def serving_step ( self , images , images_info ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. View Source @tf.function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ] ) def serving_step ( self , images , images_info ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\" \" return self . predict_step ( { DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info } )","title":"serving_step"},{"location":"reference/kerod/model/faster_rcnn/#summary","text":"def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable )","title":"summary"},{"location":"reference/kerod/model/faster_rcnn/#test_on_batch","text":"def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"test_on_batch"},{"location":"reference/kerod/model/faster_rcnn/#test_step","text":"def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) x [' ground_truths '] = y # In our graph all the metrics are computed inside the call method # So we set training to True to benefit from those metrics # Of course there is no backpropagation at the test step y_pred = self ( x , training = True ) _ = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) return { m . name: m . result () for m in self . metrics }","title":"test_step"},{"location":"reference/kerod/model/faster_rcnn/#to_json","text":"def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs )","title":"to_json"},{"location":"reference/kerod/model/faster_rcnn/#to_yaml","text":"def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' )","title":"to_yaml"},{"location":"reference/kerod/model/faster_rcnn/#train_on_batch","text":"def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"train_on_batch"},{"location":"reference/kerod/model/faster_rcnn/#train_step","text":"def train_step ( self , data ) View Source def train_step ( self , data ): # These are the only transformations `Model.fit` applies to user-input # data when a `tf.data.Dataset` is provided. These utilities will be exposed # publicly. data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : x [ 'ground_truths' ] = y y_pred = self ( x , training = True ) # All the losses are computed in the call. It can seems weird but it those # the job in a clean way. They are automatically added to self.losses loss = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return { m . name : m . result () for m in self . metrics }","title":"train_step"},{"location":"reference/kerod/model/faster_rcnn/#fasterrcnnfpnresnet50caffe","text":"class FasterRcnnFPNResnet50Caffe ( num_classes , ** kwargs ) You can use it as follow: model_faster_rcnn = FasterRcnnFPNResnet50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model_faster_rcnn . compile ( optimizer = optimizer , loss = None ) model_faster_rcnn . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"FasterRcnnFPNResnet50Caffe"},{"location":"reference/kerod/model/faster_rcnn/#arguments_1","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A tensorflow Model.","title":"Arguments"},{"location":"reference/kerod/model/faster_rcnn/#ancestors-in-mro_1","text":"kerod.model.faster_rcnn.FasterRcnnFPN keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/faster_rcnn/#methods_1","text":"","title":"Methods"},{"location":"reference/kerod/model/faster_rcnn/#call_1","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs A dict with the following schema: images : A Tensor of shape [batch_size, height, width, 3] image_informations : A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). ground_truths : A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training Is automatically set to True in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with add_loss and add_metrics . In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the self.compiled_loss method. Returns: Type Description Tuple - classification_pred : A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\" \" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ] ) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ( [ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]] , axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ( [ pyramid , rois ] ) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois","title":"call"},{"location":"reference/kerod/model/faster_rcnn/#compile_1","text":"def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile","title":"compile"},{"location":"reference/kerod/model/faster_rcnn/#compute_loss_1","text":"def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: class MyModel ( tf . keras . Model ): def __init__ ( self , * args , ** kwargs ): super ( MyModel , self ) . __init__ ( * args , ** kwargs ) self . loss_tracker = tf . keras . metrics . Mean ( name = 'loss' ) def compute_loss ( self , x , y , y_pred , sample_weight ): loss = tf . reduce_mean ( tf . math . squared_difference ( y_pred , y )) loss += tf . add_n ( self . losses ) self . loss_tracker . update_state ( loss ) return loss def reset_metrics ( self ): self . loss_tracker . reset_states () @property def metrics ( self ): return [ self . loss_tracker ] tensors = tf . random . uniform (( 10 , 10 )), tf . random . uniform (( 10 ,)) dataset = tf . data . Dataset . from_tensor_slices ( tensors ) . repeat () . batch ( 1 ) inputs = tf . keras . layers . Input ( shape = ( 10 ,), name = 'my_input' ) outputs = tf . keras . layers . Dense ( 10 )( inputs ) model = MyModel ( inputs , outputs ) model . add_loss ( tf . reduce_sum ( outputs )) optimizer = tf . keras . optimizers . SGD () model . compile ( optimizer , loss = 'mse' , steps_per_execution = 10 ) model . fit ( dataset , epochs = 2 , steps_per_epoch = 10 ) print ( 'My custom loss: ' , model . loss_tracker . result () . numpy ()) Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model(x) ) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a tf.Tensor , or None if no loss results (which is the case when called by Model.test_step ). View Source def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) : \" \"\" Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: ```python class MyModel(tf.keras.Model): def __init__(self, *args, **kwargs): super(MyModel, self).__init__(*args, **kwargs) self.loss_tracker = tf.keras.metrics.Mean(name='loss') def compute_loss(self, x, y, y_pred, sample_weight): loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y)) loss += tf.add_n(self.losses) self.loss_tracker.update_state(loss) return loss def reset_metrics(self): self.loss_tracker.reset_states() @property def metrics(self): return [self.loss_tracker] tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,)) dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1) inputs = tf.keras.layers.Input(shape=(10,), name='my_input') outputs = tf.keras.layers.Dense(10)(inputs) model = MyModel(inputs, outputs) model.add_loss(tf.reduce_sum(outputs)) optimizer = tf.keras.optimizers.SGD() model.compile(optimizer, loss='mse', steps_per_execution=10) model.fit(dataset, epochs=2, steps_per_epoch=10) print('My custom loss: ', model.loss_tracker.result().numpy()) ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model(x)`) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a `tf.Tensor`, or `None` if no loss results (which is the case when called by `Model.test_step`). \"\" \" del x # The default implementation does not use `x`. return self . compiled_loss ( y , y_pred , sample_weight , regularization_losses = self . losses )","title":"compute_loss"},{"location":"reference/kerod/model/faster_rcnn/#compute_metrics_1","text":"def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics","title":"compute_metrics"},{"location":"reference/kerod/model/faster_rcnn/#evaluate_1","text":"def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"evaluate"},{"location":"reference/kerod/model/faster_rcnn/#evaluate_generator_1","text":"def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"evaluate_generator"},{"location":"reference/kerod/model/faster_rcnn/#export_for_serving_1","text":"def export_for_serving ( self , filepath ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the training arguments is defined int the method call , tf.save_model.save method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. View Source def export_for_serving ( self , filepath ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\" \" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output } ) self . _serving = False","title":"export_for_serving"},{"location":"reference/kerod/model/faster_rcnn/#fit_1","text":"def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history","title":"fit"},{"location":"reference/kerod/model/faster_rcnn/#fit_generator_1","text":"def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch )","title":"fit_generator"},{"location":"reference/kerod/model/faster_rcnn/#get_layer_1","text":"def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' )","title":"get_layer"},{"location":"reference/kerod/model/faster_rcnn/#load_weights_1","text":"def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status","title":"load_weights"},{"location":"reference/kerod/model/faster_rcnn/#make_predict_function_1","text":"def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function","title":"make_predict_function"},{"location":"reference/kerod/model/faster_rcnn/#make_test_function_1","text":"def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function","title":"make_test_function"},{"location":"reference/kerod/model/faster_rcnn/#make_train_function_1","text":"def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function","title":"make_train_function"},{"location":"reference/kerod/model/faster_rcnn/#predict_1","text":"def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs )","title":"predict"},{"location":"reference/kerod/model/faster_rcnn/#predict_generator_1","text":"def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"predict_generator"},{"location":"reference/kerod/model/faster_rcnn/#predict_on_batch_1","text":"def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs )","title":"predict_on_batch"},{"location":"reference/kerod/model/faster_rcnn/#predict_step_1","text":"def predict_step ( self , data ) View Source def predict_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) classification_pred , localization_pred , rois = self ( x , training = False ) # Remove the background classes classification_pred = classification_pred [:, :, 1 :] return post_process_fast_rcnn_boxes ( classification_pred , localization_pred , rois , x [ DatasetField . IMAGES_INFO ], self . num_classes )","title":"predict_step"},{"location":"reference/kerod/model/faster_rcnn/#reset_metrics_1","text":"def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state ()","title":"reset_metrics"},{"location":"reference/kerod/model/faster_rcnn/#reset_states_1","text":"def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states ()","title":"reset_states"},{"location":"reference/kerod/model/faster_rcnn/#save_1","text":"def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ) View Source def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ): try : super () . save ( filepath , overwrite = overwrite , include_optimizer = include_optimizer , save_format = save_format , signatures = signatures , options = options ) except Exception as e : raise Exception ( 'Saving does not work with dynamic inputs the ground_truths are injected in the inputs. ' 'Please use export_model method instead to bypass this error.' )","title":"save"},{"location":"reference/kerod/model/faster_rcnn/#save_spec_1","text":"def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False )","title":"save_spec"},{"location":"reference/kerod/model/faster_rcnn/#save_weights_1","text":"def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ])","title":"save_weights"},{"location":"reference/kerod/model/faster_rcnn/#serving_step_1","text":"def serving_step ( self , images , images_info ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. View Source @tf.function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ] ) def serving_step ( self , images , images_info ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\" \" return self . predict_step ( { DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info } )","title":"serving_step"},{"location":"reference/kerod/model/faster_rcnn/#summary_1","text":"def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable )","title":"summary"},{"location":"reference/kerod/model/faster_rcnn/#test_on_batch_1","text":"def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"test_on_batch"},{"location":"reference/kerod/model/faster_rcnn/#test_step_1","text":"def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) x [' ground_truths '] = y # In our graph all the metrics are computed inside the call method # So we set training to True to benefit from those metrics # Of course there is no backpropagation at the test step y_pred = self ( x , training = True ) _ = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) return { m . name: m . result () for m in self . metrics }","title":"test_step"},{"location":"reference/kerod/model/faster_rcnn/#to_json_1","text":"def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs )","title":"to_json"},{"location":"reference/kerod/model/faster_rcnn/#to_yaml_1","text":"def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' )","title":"to_yaml"},{"location":"reference/kerod/model/faster_rcnn/#train_on_batch_1","text":"def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"train_on_batch"},{"location":"reference/kerod/model/faster_rcnn/#train_step_1","text":"def train_step ( self , data ) View Source def train_step ( self , data ): # These are the only transformations `Model.fit` applies to user-input # data when a `tf.data.Dataset` is provided. These utilities will be exposed # publicly. data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : x [ 'ground_truths' ] = y y_pred = self ( x , training = True ) # All the losses are computed in the call. It can seems weird but it those # the job in a clean way. They are automatically added to self.losses loss = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return { m . name : m . result () for m in self . metrics }","title":"train_step"},{"location":"reference/kerod/model/faster_rcnn/#fasterrcnnfpnresnet50pytorch","text":"class FasterRcnnFPNResnet50Pytorch ( num_classes , ** kwargs ) You can use it as follow: model_faster_rcnn = FasterRcnnFPNResnet50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model_faster_rcnn . compile ( optimizer = optimizer , loss = None ) model_faster_rcnn . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"FasterRcnnFPNResnet50Pytorch"},{"location":"reference/kerod/model/faster_rcnn/#arguments_2","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A tensorflow Model.","title":"Arguments"},{"location":"reference/kerod/model/faster_rcnn/#ancestors-in-mro_2","text":"kerod.model.faster_rcnn.FasterRcnnFPN keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/faster_rcnn/#methods_2","text":"","title":"Methods"},{"location":"reference/kerod/model/faster_rcnn/#call_2","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs A dict with the following schema: images : A Tensor of shape [batch_size, height, width, 3] image_informations : A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). ground_truths : A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training Is automatically set to True in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with add_loss and add_metrics . In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the self.compiled_loss method. Returns: Type Description Tuple - classification_pred : A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\" \" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ] ) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ( [ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]] , axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ( [ pyramid , rois ] ) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois","title":"call"},{"location":"reference/kerod/model/faster_rcnn/#compile_2","text":"def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile","title":"compile"},{"location":"reference/kerod/model/faster_rcnn/#compute_loss_2","text":"def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: class MyModel ( tf . keras . Model ): def __init__ ( self , * args , ** kwargs ): super ( MyModel , self ) . __init__ ( * args , ** kwargs ) self . loss_tracker = tf . keras . metrics . Mean ( name = 'loss' ) def compute_loss ( self , x , y , y_pred , sample_weight ): loss = tf . reduce_mean ( tf . math . squared_difference ( y_pred , y )) loss += tf . add_n ( self . losses ) self . loss_tracker . update_state ( loss ) return loss def reset_metrics ( self ): self . loss_tracker . reset_states () @property def metrics ( self ): return [ self . loss_tracker ] tensors = tf . random . uniform (( 10 , 10 )), tf . random . uniform (( 10 ,)) dataset = tf . data . Dataset . from_tensor_slices ( tensors ) . repeat () . batch ( 1 ) inputs = tf . keras . layers . Input ( shape = ( 10 ,), name = 'my_input' ) outputs = tf . keras . layers . Dense ( 10 )( inputs ) model = MyModel ( inputs , outputs ) model . add_loss ( tf . reduce_sum ( outputs )) optimizer = tf . keras . optimizers . SGD () model . compile ( optimizer , loss = 'mse' , steps_per_execution = 10 ) model . fit ( dataset , epochs = 2 , steps_per_epoch = 10 ) print ( 'My custom loss: ' , model . loss_tracker . result () . numpy ()) Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model(x) ) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a tf.Tensor , or None if no loss results (which is the case when called by Model.test_step ). View Source def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) : \" \"\" Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: ```python class MyModel(tf.keras.Model): def __init__(self, *args, **kwargs): super(MyModel, self).__init__(*args, **kwargs) self.loss_tracker = tf.keras.metrics.Mean(name='loss') def compute_loss(self, x, y, y_pred, sample_weight): loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y)) loss += tf.add_n(self.losses) self.loss_tracker.update_state(loss) return loss def reset_metrics(self): self.loss_tracker.reset_states() @property def metrics(self): return [self.loss_tracker] tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,)) dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1) inputs = tf.keras.layers.Input(shape=(10,), name='my_input') outputs = tf.keras.layers.Dense(10)(inputs) model = MyModel(inputs, outputs) model.add_loss(tf.reduce_sum(outputs)) optimizer = tf.keras.optimizers.SGD() model.compile(optimizer, loss='mse', steps_per_execution=10) model.fit(dataset, epochs=2, steps_per_epoch=10) print('My custom loss: ', model.loss_tracker.result().numpy()) ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model(x)`) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a `tf.Tensor`, or `None` if no loss results (which is the case when called by `Model.test_step`). \"\" \" del x # The default implementation does not use `x`. return self . compiled_loss ( y , y_pred , sample_weight , regularization_losses = self . losses )","title":"compute_loss"},{"location":"reference/kerod/model/faster_rcnn/#compute_metrics_2","text":"def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics","title":"compute_metrics"},{"location":"reference/kerod/model/faster_rcnn/#evaluate_2","text":"def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"evaluate"},{"location":"reference/kerod/model/faster_rcnn/#evaluate_generator_2","text":"def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"evaluate_generator"},{"location":"reference/kerod/model/faster_rcnn/#export_for_serving_2","text":"def export_for_serving ( self , filepath ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the training arguments is defined int the method call , tf.save_model.save method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. View Source def export_for_serving ( self , filepath ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\" \" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output } ) self . _serving = False","title":"export_for_serving"},{"location":"reference/kerod/model/faster_rcnn/#fit_2","text":"def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history","title":"fit"},{"location":"reference/kerod/model/faster_rcnn/#fit_generator_2","text":"def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch )","title":"fit_generator"},{"location":"reference/kerod/model/faster_rcnn/#get_layer_2","text":"def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' )","title":"get_layer"},{"location":"reference/kerod/model/faster_rcnn/#load_weights_2","text":"def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status","title":"load_weights"},{"location":"reference/kerod/model/faster_rcnn/#make_predict_function_2","text":"def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function","title":"make_predict_function"},{"location":"reference/kerod/model/faster_rcnn/#make_test_function_2","text":"def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function","title":"make_test_function"},{"location":"reference/kerod/model/faster_rcnn/#make_train_function_2","text":"def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function","title":"make_train_function"},{"location":"reference/kerod/model/faster_rcnn/#predict_2","text":"def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs )","title":"predict"},{"location":"reference/kerod/model/faster_rcnn/#predict_generator_2","text":"def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"predict_generator"},{"location":"reference/kerod/model/faster_rcnn/#predict_on_batch_2","text":"def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs )","title":"predict_on_batch"},{"location":"reference/kerod/model/faster_rcnn/#predict_step_2","text":"def predict_step ( self , data ) View Source def predict_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) classification_pred , localization_pred , rois = self ( x , training = False ) # Remove the background classes classification_pred = classification_pred [:, :, 1 :] return post_process_fast_rcnn_boxes ( classification_pred , localization_pred , rois , x [ DatasetField . IMAGES_INFO ], self . num_classes )","title":"predict_step"},{"location":"reference/kerod/model/faster_rcnn/#reset_metrics_2","text":"def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state ()","title":"reset_metrics"},{"location":"reference/kerod/model/faster_rcnn/#reset_states_2","text":"def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states ()","title":"reset_states"},{"location":"reference/kerod/model/faster_rcnn/#save_2","text":"def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ) View Source def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ): try : super () . save ( filepath , overwrite = overwrite , include_optimizer = include_optimizer , save_format = save_format , signatures = signatures , options = options ) except Exception as e : raise Exception ( 'Saving does not work with dynamic inputs the ground_truths are injected in the inputs. ' 'Please use export_model method instead to bypass this error.' )","title":"save"},{"location":"reference/kerod/model/faster_rcnn/#save_spec_2","text":"def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False )","title":"save_spec"},{"location":"reference/kerod/model/faster_rcnn/#save_weights_2","text":"def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ])","title":"save_weights"},{"location":"reference/kerod/model/faster_rcnn/#serving_step_2","text":"def serving_step ( self , images , images_info ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. View Source @tf.function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ] ) def serving_step ( self , images , images_info ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\" \" return self . predict_step ( { DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info } )","title":"serving_step"},{"location":"reference/kerod/model/faster_rcnn/#summary_2","text":"def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable )","title":"summary"},{"location":"reference/kerod/model/faster_rcnn/#test_on_batch_2","text":"def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"test_on_batch"},{"location":"reference/kerod/model/faster_rcnn/#test_step_2","text":"def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) x [' ground_truths '] = y # In our graph all the metrics are computed inside the call method # So we set training to True to benefit from those metrics # Of course there is no backpropagation at the test step y_pred = self ( x , training = True ) _ = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) return { m . name: m . result () for m in self . metrics }","title":"test_step"},{"location":"reference/kerod/model/faster_rcnn/#to_json_2","text":"def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs )","title":"to_json"},{"location":"reference/kerod/model/faster_rcnn/#to_yaml_2","text":"def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' )","title":"to_yaml"},{"location":"reference/kerod/model/faster_rcnn/#train_on_batch_2","text":"def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"train_on_batch"},{"location":"reference/kerod/model/faster_rcnn/#train_step_2","text":"def train_step ( self , data ) View Source def train_step ( self , data ): # These are the only transformations `Model.fit` applies to user-input # data when a `tf.data.Dataset` is provided. These utilities will be exposed # publicly. data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : x [ 'ground_truths' ] = y y_pred = self ( x , training = True ) # All the losses are computed in the call. It can seems weird but it those # the job in a clean way. They are automatically added to self.losses loss = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return { m . name : m . result () for m in self . metrics }","title":"train_step"},{"location":"reference/kerod/model/smca_detr/","text":"Module kerod.model.smca_detr None None View Source from typing import Dict import tensorflow as tf import tensorflow_addons as tfa from kerod.core.box_ops import ( convert_to_center_coordinates , convert_to_xyxy_coordinates ) from kerod.core.losses import L1Loss from kerod.core.matcher import hungarian_matching from kerod.core.similarity import DetrSimilarity from kerod.core.standard_fields import BoxField , DatasetField from kerod.core.target_assigner import TargetAssigner from kerod.model.backbone.resnet import ResNet50 , ResNet50PytorchStyle from kerod.model.detr import compute_detr_metrics from kerod.layers import ( DynamicalWeightMaps , PositionEmbeddingSine , Transformer , SMCAReferencePoints ) from kerod.layers.post_processing.post_processing_detr import \\ post_processing as detr_postprocessing from kerod.utils.documentation import remove_unwanted_doc from tensorflow.python.keras.engine import data_adapter __pdoc__ = {} class SMCA ( tf . keras . Model ): \"\"\"Build a single scale SCMA model according to the paper [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: ```python model = SMCAR50(80) base_lr = 0.1 optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr) model.compile(optimizer=optimizer, loss=None) model.fit(ds_train, validation_data=ds_test, epochs=11,) ``` Arguments: num_classes: The number of classes of your dataset (**do not include the background class** it is handle for you) backbone: A vision model like ResNet50. num_queries: number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries. Call arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Call returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" def __init__ ( self , num_classes , backbone , num_queries = 300 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_classes = num_classes self . num_queries = num_queries self . hidden_dim = 256 self . backbone = backbone self . input_proj = tf . keras . layers . Conv2D ( self . hidden_dim , 1 ) self . pos_embed = PositionEmbeddingSine ( output_dim = self . hidden_dim ) num_heads = 8 self . transformer_num_layers = 6 self . transformer = Transformer ( num_layers = self . transformer_num_layers , d_model = self . hidden_dim , num_heads = num_heads , dim_feedforward = 2048 ) # MCMA layers self . dyn_weight_map = DynamicalWeightMaps () self . ref_points = SMCAReferencePoints ( self . hidden_dim , num_heads ) self . bbox_embed = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( 4 , dtype = tf . float32 ) # (x1, y1, x2, y2) ]) self . class_embed = tf . keras . layers . Dense ( num_classes + 1 , dtype = tf . float32 ) # Will create a learnable embedding matrix for all our queries # It is a matrix of [num_queries, self.hidden_dim] # The embedding layers self . query_embed = tf . keras . layers . Embedding ( num_queries , self . hidden_dim , embeddings_initializer = tf . keras . initializers . RandomNormal ( mean = 0. , stddev = 1. )) self . all_the_queries = tf . range ( num_queries ) # Loss computation self . weight_class , self . weight_l1 , self . weight_giou = 2 , 5 , 2 similarity_func = DetrSimilarity ( self . weight_class , self . weight_l1 , self . weight_giou ) self . target_assigner = TargetAssigner ( similarity_func , hungarian_matching , lambda gt , pred : gt , negative_class_weight = 1.0 ) # Losses self . giou = tfa . losses . GIoULoss ( reduction = tf . keras . losses . Reduction . NONE ) self . l1 = L1Loss ( reduction = tf . keras . losses . Reduction . NONE ) self . focal_loss = tfa . losses . SigmoidFocalCrossEntropy ( alpha = 0.25 , gamma = 2 , reduction = tf . keras . losses . Reduction . NONE , from_logits = True ) # Metrics self . giou_metric = tf . keras . metrics . Mean ( name = \"giou_last_layer\" ) self . l1_metric = tf . keras . metrics . Mean ( name = \"l1_last_layer\" ) self . focal_loss_metric = tf . keras . metrics . Mean ( name = \"focal_loss_last_layer\" ) self . loss_metric = tf . keras . metrics . Mean ( name = \"loss\" ) self . precision_metric = tf . keras . metrics . SparseCategoricalAccuracy () # Object recall = foreground self . recall_metric = tf . keras . metrics . Mean ( name = \"object_recall\" ) @property def metrics ( self ): return [ self . loss_metric , self . giou_metric , self . l1_metric , self . focal_loss_metric , self . precision_metric , self . recall_metric ] def call ( self , inputs , training = None ): \"\"\"Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images )[ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images )[ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ... , None ], tf . float32 ), tf . shape ( x )[ 1 : 3 ], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ], ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x )[ 1 ], tf . shape ( x )[ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid )[ 1 ], 2 ))], axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } def compute_loss ( self , ground_truths : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ], input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField . BOXES ] / tf . tile ( input_shape [ None ], [ 1 , 2 ]) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField . BOXES ], self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField . SCORES ], self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [{ BoxField . BOXES : boxes , BoxField . SCORES : logits } for boxes , logits in zip ( boxes_per_lvl , logits_per_lvl )] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField . NUM_BOXES ]), tf . float32 ) loss = 0 # Compute the Giou, L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ): # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss def _compute_loss ( self , y_pred : Dict [ str , tf . Tensor ], ground_truths : Dict [ str , tf . Tensor ], num_boxes : int , compute_metrics = False , ): y_true , weights = self . target_assigner . assign ( y_pred , ground_truths ) # Caveats GIoU is buggy and if the batch_size is 1 and the sample_weight # is provided will raise an error giou = self . giou ( convert_to_xyxy_coordinates ( y_true [ BoxField . BOXES ]), convert_to_xyxy_coordinates ( y_pred [ BoxField . BOXES ]), sample_weight = weights [ BoxField . BOXES ]) l1 = self . l1 ( y_true [ BoxField . BOXES ], y_pred [ BoxField . BOXES ], sample_weight = weights [ BoxField . BOXES ]) cls_labels = tf . one_hot ( y_true [ BoxField . LABELS ], depth = self . num_classes + 1 , dtype = tf . float32 , ) focal_loss = self . focal_loss ( cls_labels , y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) giou = self . weight_giou * tf . reduce_sum ( giou ) / num_boxes l1 = self . weight_l1 * tf . reduce_sum ( l1 ) / num_boxes focal_loss = self . weight_class * tf . reduce_sum ( focal_loss ) / tf . reduce_sum ( weights [ BoxField . LABELS ]) if compute_metrics : self . giou_metric . update_state ( giou ) self . l1_metric . update_state ( l1 ) self . focal_loss_metric . update_state ( focal_loss ) self . precision_metric . update_state ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) recall = compute_detr_metrics ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ]) self . recall_metric . update_state ( recall ) return giou + l1 + focal_loss def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def test_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def predict_step ( self , data ): \"\"\"Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Tuple: - `boxes`: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - `scores`: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - `classes`: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\"\" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ], y_pred [ BoxField . SCORES ], x [ DatasetField . IMAGES_INFO ], tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], ) return boxes_without_padding , scores , labels class SMCAR50 ( SMCA ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50 ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) class SMCAR50Pytorch ( SMCA ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50PytorchStyle ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) remove_unwanted_doc ( SMCA , __pdoc__ ) remove_unwanted_doc ( SMCAR50 , __pdoc__ ) remove_unwanted_doc ( SMCAR50Pytorch , __pdoc__ ) Classes SMCA class SMCA ( num_classes , backbone , num_queries = 300 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: model = SMCAR50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries. Call arguments Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Call returns Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. Ancestors (in MRO) keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector Descendants kerod.model.smca_detr.SMCAR50 kerod.model.smca_detr.SMCAR50Pytorch Methods call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x ) [ 1 ] , tf . shape ( x ) [ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid ) [ 1 ] , 2 )) ] , axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } compile def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile compute_loss def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> int Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss compute_metrics def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics evaluate def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) evaluate_generator def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) fit def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history fit_generator def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch ) get_layer def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' ) load_weights def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status make_predict_function def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function make_test_function def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function make_train_function def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function predict def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs ) predict_generator def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) predict_on_batch def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs ) predict_step def predict_step ( self , data ) Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Type Description Tuple - boxes : A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - scores : A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - classes : A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def predict_step ( self , data ) : \" \"\" Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \" To optimize for AP , we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4. Experiments of Object Detection with Transformers Returns: Tuple: - `boxes`: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - `scores`: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - `classes`: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\" \" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ] , y_pred [ BoxField . SCORES ] , x [ DatasetField . IMAGES_INFO ] , tf . shape ( x [ DatasetField . IMAGES ] ) [ 1 : 3 ] , ) return boxes_without_padding , scores , labels reset_metrics def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state () reset_states def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states () save def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. Example: from keras.models import load_model model . save ( 'my_model.h5' ) # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model ( 'my_model.h5' ) View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) : # pylint: disable=line-too-long \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" # pylint: enable=line-too-long save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces ) save_spec def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False ) save_weights def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ]) summary def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable ) test_on_batch def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) test_step def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name: m . result () for m in self . metrics } to_json def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs ) to_yaml def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' ) train_on_batch def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) train_step def train_step ( self , data ) View Source def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } SMCAR50 class SMCAR50 ( num_classes , num_queries = 100 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: model = SMCAR50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries. Call arguments Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Call returns Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. Ancestors (in MRO) kerod.model.smca_detr.SMCA keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector Methods call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x ) [ 1 ] , tf . shape ( x ) [ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid ) [ 1 ] , 2 )) ] , axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } compile def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile compute_loss def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> int Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss compute_metrics def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics evaluate def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) evaluate_generator def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) fit def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history fit_generator def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch ) get_layer def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' ) load_weights def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status make_predict_function def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function make_test_function def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function make_train_function def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function predict def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs ) predict_generator def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) predict_on_batch def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs ) predict_step def predict_step ( self , data ) Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Type Description Tuple - boxes : A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - scores : A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - classes : A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def predict_step ( self , data ) : \" \"\" Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \" To optimize for AP , we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4. Experiments of Object Detection with Transformers Returns: Tuple: - `boxes`: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - `scores`: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - `classes`: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\" \" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ] , y_pred [ BoxField . SCORES ] , x [ DatasetField . IMAGES_INFO ] , tf . shape ( x [ DatasetField . IMAGES ] ) [ 1 : 3 ] , ) return boxes_without_padding , scores , labels reset_metrics def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state () reset_states def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states () save def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. Example: from keras.models import load_model model . save ( 'my_model.h5' ) # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model ( 'my_model.h5' ) View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) : # pylint: disable=line-too-long \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" # pylint: enable=line-too-long save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces ) save_spec def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False ) save_weights def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ]) summary def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable ) test_on_batch def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) test_step def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name: m . result () for m in self . metrics } to_json def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs ) to_yaml def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' ) train_on_batch def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) train_step def train_step ( self , data ) View Source def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } SMCAR50Pytorch class SMCAR50Pytorch ( num_classes , num_queries = 100 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: model = SMCAR50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries. Call arguments Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Call returns Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. Ancestors (in MRO) kerod.model.smca_detr.SMCA keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector Methods call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x ) [ 1 ] , tf . shape ( x ) [ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid ) [ 1 ] , 2 )) ] , axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } compile def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile compute_loss def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> int Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss compute_metrics def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics evaluate def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) evaluate_generator def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) fit def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history fit_generator def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch ) get_layer def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' ) load_weights def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status make_predict_function def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function make_test_function def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function make_train_function def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function predict def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs ) predict_generator def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks ) predict_on_batch def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs ) predict_step def predict_step ( self , data ) Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Type Description Tuple - boxes : A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - scores : A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - classes : A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def predict_step ( self , data ) : \" \"\" Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \" To optimize for AP , we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4. Experiments of Object Detection with Transformers Returns: Tuple: - `boxes`: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - `scores`: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - `classes`: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\" \" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ] , y_pred [ BoxField . SCORES ] , x [ DatasetField . IMAGES_INFO ] , tf . shape ( x [ DatasetField . IMAGES ] ) [ 1 : 3 ] , ) return boxes_without_padding , scores , labels reset_metrics def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state () reset_states def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states () save def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. Example: from keras.models import load_model model . save ( 'my_model.h5' ) # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model ( 'my_model.h5' ) View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) : # pylint: disable=line-too-long \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" # pylint: enable=line-too-long save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces ) save_spec def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False ) save_weights def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ]) summary def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable ) test_on_batch def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) test_step def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name: m . result () for m in self . metrics } to_json def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs ) to_yaml def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' ) train_on_batch def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) train_step def train_step ( self , data ) View Source def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics }","title":"Smca Detr"},{"location":"reference/kerod/model/smca_detr/#module-kerodmodelsmca_detr","text":"None None View Source from typing import Dict import tensorflow as tf import tensorflow_addons as tfa from kerod.core.box_ops import ( convert_to_center_coordinates , convert_to_xyxy_coordinates ) from kerod.core.losses import L1Loss from kerod.core.matcher import hungarian_matching from kerod.core.similarity import DetrSimilarity from kerod.core.standard_fields import BoxField , DatasetField from kerod.core.target_assigner import TargetAssigner from kerod.model.backbone.resnet import ResNet50 , ResNet50PytorchStyle from kerod.model.detr import compute_detr_metrics from kerod.layers import ( DynamicalWeightMaps , PositionEmbeddingSine , Transformer , SMCAReferencePoints ) from kerod.layers.post_processing.post_processing_detr import \\ post_processing as detr_postprocessing from kerod.utils.documentation import remove_unwanted_doc from tensorflow.python.keras.engine import data_adapter __pdoc__ = {} class SMCA ( tf . keras . Model ): \"\"\"Build a single scale SCMA model according to the paper [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: ```python model = SMCAR50(80) base_lr = 0.1 optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr) model.compile(optimizer=optimizer, loss=None) model.fit(ds_train, validation_data=ds_test, epochs=11,) ``` Arguments: num_classes: The number of classes of your dataset (**do not include the background class** it is handle for you) backbone: A vision model like ResNet50. num_queries: number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries. Call arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Call returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" def __init__ ( self , num_classes , backbone , num_queries = 300 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_classes = num_classes self . num_queries = num_queries self . hidden_dim = 256 self . backbone = backbone self . input_proj = tf . keras . layers . Conv2D ( self . hidden_dim , 1 ) self . pos_embed = PositionEmbeddingSine ( output_dim = self . hidden_dim ) num_heads = 8 self . transformer_num_layers = 6 self . transformer = Transformer ( num_layers = self . transformer_num_layers , d_model = self . hidden_dim , num_heads = num_heads , dim_feedforward = 2048 ) # MCMA layers self . dyn_weight_map = DynamicalWeightMaps () self . ref_points = SMCAReferencePoints ( self . hidden_dim , num_heads ) self . bbox_embed = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( 4 , dtype = tf . float32 ) # (x1, y1, x2, y2) ]) self . class_embed = tf . keras . layers . Dense ( num_classes + 1 , dtype = tf . float32 ) # Will create a learnable embedding matrix for all our queries # It is a matrix of [num_queries, self.hidden_dim] # The embedding layers self . query_embed = tf . keras . layers . Embedding ( num_queries , self . hidden_dim , embeddings_initializer = tf . keras . initializers . RandomNormal ( mean = 0. , stddev = 1. )) self . all_the_queries = tf . range ( num_queries ) # Loss computation self . weight_class , self . weight_l1 , self . weight_giou = 2 , 5 , 2 similarity_func = DetrSimilarity ( self . weight_class , self . weight_l1 , self . weight_giou ) self . target_assigner = TargetAssigner ( similarity_func , hungarian_matching , lambda gt , pred : gt , negative_class_weight = 1.0 ) # Losses self . giou = tfa . losses . GIoULoss ( reduction = tf . keras . losses . Reduction . NONE ) self . l1 = L1Loss ( reduction = tf . keras . losses . Reduction . NONE ) self . focal_loss = tfa . losses . SigmoidFocalCrossEntropy ( alpha = 0.25 , gamma = 2 , reduction = tf . keras . losses . Reduction . NONE , from_logits = True ) # Metrics self . giou_metric = tf . keras . metrics . Mean ( name = \"giou_last_layer\" ) self . l1_metric = tf . keras . metrics . Mean ( name = \"l1_last_layer\" ) self . focal_loss_metric = tf . keras . metrics . Mean ( name = \"focal_loss_last_layer\" ) self . loss_metric = tf . keras . metrics . Mean ( name = \"loss\" ) self . precision_metric = tf . keras . metrics . SparseCategoricalAccuracy () # Object recall = foreground self . recall_metric = tf . keras . metrics . Mean ( name = \"object_recall\" ) @property def metrics ( self ): return [ self . loss_metric , self . giou_metric , self . l1_metric , self . focal_loss_metric , self . precision_metric , self . recall_metric ] def call ( self , inputs , training = None ): \"\"\"Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images )[ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images )[ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ... , None ], tf . float32 ), tf . shape ( x )[ 1 : 3 ], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ], ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x )[ 1 ], tf . shape ( x )[ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid )[ 1 ], 2 ))], axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } def compute_loss ( self , ground_truths : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ], input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField . BOXES ] / tf . tile ( input_shape [ None ], [ 1 , 2 ]) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField . BOXES ], self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField . SCORES ], self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [{ BoxField . BOXES : boxes , BoxField . SCORES : logits } for boxes , logits in zip ( boxes_per_lvl , logits_per_lvl )] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField . NUM_BOXES ]), tf . float32 ) loss = 0 # Compute the Giou, L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ): # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss def _compute_loss ( self , y_pred : Dict [ str , tf . Tensor ], ground_truths : Dict [ str , tf . Tensor ], num_boxes : int , compute_metrics = False , ): y_true , weights = self . target_assigner . assign ( y_pred , ground_truths ) # Caveats GIoU is buggy and if the batch_size is 1 and the sample_weight # is provided will raise an error giou = self . giou ( convert_to_xyxy_coordinates ( y_true [ BoxField . BOXES ]), convert_to_xyxy_coordinates ( y_pred [ BoxField . BOXES ]), sample_weight = weights [ BoxField . BOXES ]) l1 = self . l1 ( y_true [ BoxField . BOXES ], y_pred [ BoxField . BOXES ], sample_weight = weights [ BoxField . BOXES ]) cls_labels = tf . one_hot ( y_true [ BoxField . LABELS ], depth = self . num_classes + 1 , dtype = tf . float32 , ) focal_loss = self . focal_loss ( cls_labels , y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) giou = self . weight_giou * tf . reduce_sum ( giou ) / num_boxes l1 = self . weight_l1 * tf . reduce_sum ( l1 ) / num_boxes focal_loss = self . weight_class * tf . reduce_sum ( focal_loss ) / tf . reduce_sum ( weights [ BoxField . LABELS ]) if compute_metrics : self . giou_metric . update_state ( giou ) self . l1_metric . update_state ( l1 ) self . focal_loss_metric . update_state ( focal_loss ) self . precision_metric . update_state ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) recall = compute_detr_metrics ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ]) self . recall_metric . update_state ( recall ) return giou + l1 + focal_loss def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def test_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def predict_step ( self , data ): \"\"\"Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Tuple: - `boxes`: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - `scores`: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - `classes`: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\"\" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ], y_pred [ BoxField . SCORES ], x [ DatasetField . IMAGES_INFO ], tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], ) return boxes_without_padding , scores , labels class SMCAR50 ( SMCA ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50 ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) class SMCAR50Pytorch ( SMCA ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50PytorchStyle ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) remove_unwanted_doc ( SMCA , __pdoc__ ) remove_unwanted_doc ( SMCAR50 , __pdoc__ ) remove_unwanted_doc ( SMCAR50Pytorch , __pdoc__ )","title":"Module kerod.model.smca_detr"},{"location":"reference/kerod/model/smca_detr/#classes","text":"","title":"Classes"},{"location":"reference/kerod/model/smca_detr/#smca","text":"class SMCA ( num_classes , backbone , num_queries = 300 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: model = SMCAR50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"SMCA"},{"location":"reference/kerod/model/smca_detr/#arguments","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries.","title":"Arguments"},{"location":"reference/kerod/model/smca_detr/#call-arguments","text":"Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode","title":"Call arguments"},{"location":"reference/kerod/model/smca_detr/#call-returns","text":"Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise.","title":"Call returns"},{"location":"reference/kerod/model/smca_detr/#ancestors-in-mro","text":"keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/smca_detr/#descendants","text":"kerod.model.smca_detr.SMCAR50 kerod.model.smca_detr.SMCAR50Pytorch","title":"Descendants"},{"location":"reference/kerod/model/smca_detr/#methods","text":"","title":"Methods"},{"location":"reference/kerod/model/smca_detr/#call","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x ) [ 1 ] , tf . shape ( x ) [ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid ) [ 1 ] , 2 )) ] , axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , }","title":"call"},{"location":"reference/kerod/model/smca_detr/#compile","text":"def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile","title":"compile"},{"location":"reference/kerod/model/smca_detr/#compute_loss","text":"def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> int Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss","title":"compute_loss"},{"location":"reference/kerod/model/smca_detr/#compute_metrics","text":"def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics","title":"compute_metrics"},{"location":"reference/kerod/model/smca_detr/#evaluate","text":"def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"evaluate"},{"location":"reference/kerod/model/smca_detr/#evaluate_generator","text":"def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"evaluate_generator"},{"location":"reference/kerod/model/smca_detr/#fit","text":"def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history","title":"fit"},{"location":"reference/kerod/model/smca_detr/#fit_generator","text":"def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch )","title":"fit_generator"},{"location":"reference/kerod/model/smca_detr/#get_layer","text":"def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' )","title":"get_layer"},{"location":"reference/kerod/model/smca_detr/#load_weights","text":"def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status","title":"load_weights"},{"location":"reference/kerod/model/smca_detr/#make_predict_function","text":"def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function","title":"make_predict_function"},{"location":"reference/kerod/model/smca_detr/#make_test_function","text":"def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function","title":"make_test_function"},{"location":"reference/kerod/model/smca_detr/#make_train_function","text":"def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function","title":"make_train_function"},{"location":"reference/kerod/model/smca_detr/#predict","text":"def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs )","title":"predict"},{"location":"reference/kerod/model/smca_detr/#predict_generator","text":"def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"predict_generator"},{"location":"reference/kerod/model/smca_detr/#predict_on_batch","text":"def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs )","title":"predict_on_batch"},{"location":"reference/kerod/model/smca_detr/#predict_step","text":"def predict_step ( self , data ) Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Type Description Tuple - boxes : A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - scores : A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - classes : A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def predict_step ( self , data ) : \" \"\" Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \" To optimize for AP , we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4. Experiments of Object Detection with Transformers Returns: Tuple: - `boxes`: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - `scores`: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - `classes`: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\" \" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ] , y_pred [ BoxField . SCORES ] , x [ DatasetField . IMAGES_INFO ] , tf . shape ( x [ DatasetField . IMAGES ] ) [ 1 : 3 ] , ) return boxes_without_padding , scores , labels","title":"predict_step"},{"location":"reference/kerod/model/smca_detr/#reset_metrics","text":"def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state ()","title":"reset_metrics"},{"location":"reference/kerod/model/smca_detr/#reset_states","text":"def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states ()","title":"reset_states"},{"location":"reference/kerod/model/smca_detr/#save","text":"def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. Example: from keras.models import load_model model . save ( 'my_model.h5' ) # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model ( 'my_model.h5' ) View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) : # pylint: disable=line-too-long \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" # pylint: enable=line-too-long save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces )","title":"save"},{"location":"reference/kerod/model/smca_detr/#save_spec","text":"def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False )","title":"save_spec"},{"location":"reference/kerod/model/smca_detr/#save_weights","text":"def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ])","title":"save_weights"},{"location":"reference/kerod/model/smca_detr/#summary","text":"def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable )","title":"summary"},{"location":"reference/kerod/model/smca_detr/#test_on_batch","text":"def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"test_on_batch"},{"location":"reference/kerod/model/smca_detr/#test_step","text":"def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name: m . result () for m in self . metrics }","title":"test_step"},{"location":"reference/kerod/model/smca_detr/#to_json","text":"def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs )","title":"to_json"},{"location":"reference/kerod/model/smca_detr/#to_yaml","text":"def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' )","title":"to_yaml"},{"location":"reference/kerod/model/smca_detr/#train_on_batch","text":"def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"train_on_batch"},{"location":"reference/kerod/model/smca_detr/#train_step","text":"def train_step ( self , data ) View Source def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics }","title":"train_step"},{"location":"reference/kerod/model/smca_detr/#smcar50","text":"class SMCAR50 ( num_classes , num_queries = 100 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: model = SMCAR50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"SMCAR50"},{"location":"reference/kerod/model/smca_detr/#arguments_1","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries.","title":"Arguments"},{"location":"reference/kerod/model/smca_detr/#call-arguments_1","text":"Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode","title":"Call arguments"},{"location":"reference/kerod/model/smca_detr/#call-returns_1","text":"Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise.","title":"Call returns"},{"location":"reference/kerod/model/smca_detr/#ancestors-in-mro_1","text":"kerod.model.smca_detr.SMCA keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/smca_detr/#methods_1","text":"","title":"Methods"},{"location":"reference/kerod/model/smca_detr/#call_1","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x ) [ 1 ] , tf . shape ( x ) [ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid ) [ 1 ] , 2 )) ] , axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , }","title":"call"},{"location":"reference/kerod/model/smca_detr/#compile_1","text":"def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile","title":"compile"},{"location":"reference/kerod/model/smca_detr/#compute_loss_1","text":"def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> int Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss","title":"compute_loss"},{"location":"reference/kerod/model/smca_detr/#compute_metrics_1","text":"def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics","title":"compute_metrics"},{"location":"reference/kerod/model/smca_detr/#evaluate_1","text":"def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"evaluate"},{"location":"reference/kerod/model/smca_detr/#evaluate_generator_1","text":"def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"evaluate_generator"},{"location":"reference/kerod/model/smca_detr/#fit_1","text":"def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history","title":"fit"},{"location":"reference/kerod/model/smca_detr/#fit_generator_1","text":"def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch )","title":"fit_generator"},{"location":"reference/kerod/model/smca_detr/#get_layer_1","text":"def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' )","title":"get_layer"},{"location":"reference/kerod/model/smca_detr/#load_weights_1","text":"def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status","title":"load_weights"},{"location":"reference/kerod/model/smca_detr/#make_predict_function_1","text":"def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function","title":"make_predict_function"},{"location":"reference/kerod/model/smca_detr/#make_test_function_1","text":"def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function","title":"make_test_function"},{"location":"reference/kerod/model/smca_detr/#make_train_function_1","text":"def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function","title":"make_train_function"},{"location":"reference/kerod/model/smca_detr/#predict_1","text":"def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs )","title":"predict"},{"location":"reference/kerod/model/smca_detr/#predict_generator_1","text":"def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"predict_generator"},{"location":"reference/kerod/model/smca_detr/#predict_on_batch_1","text":"def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs )","title":"predict_on_batch"},{"location":"reference/kerod/model/smca_detr/#predict_step_1","text":"def predict_step ( self , data ) Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Type Description Tuple - boxes : A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - scores : A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - classes : A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def predict_step ( self , data ) : \" \"\" Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \" To optimize for AP , we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4. Experiments of Object Detection with Transformers Returns: Tuple: - `boxes`: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - `scores`: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - `classes`: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\" \" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ] , y_pred [ BoxField . SCORES ] , x [ DatasetField . IMAGES_INFO ] , tf . shape ( x [ DatasetField . IMAGES ] ) [ 1 : 3 ] , ) return boxes_without_padding , scores , labels","title":"predict_step"},{"location":"reference/kerod/model/smca_detr/#reset_metrics_1","text":"def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state ()","title":"reset_metrics"},{"location":"reference/kerod/model/smca_detr/#reset_states_1","text":"def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states ()","title":"reset_states"},{"location":"reference/kerod/model/smca_detr/#save_1","text":"def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. Example: from keras.models import load_model model . save ( 'my_model.h5' ) # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model ( 'my_model.h5' ) View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) : # pylint: disable=line-too-long \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" # pylint: enable=line-too-long save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces )","title":"save"},{"location":"reference/kerod/model/smca_detr/#save_spec_1","text":"def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False )","title":"save_spec"},{"location":"reference/kerod/model/smca_detr/#save_weights_1","text":"def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ])","title":"save_weights"},{"location":"reference/kerod/model/smca_detr/#summary_1","text":"def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable )","title":"summary"},{"location":"reference/kerod/model/smca_detr/#test_on_batch_1","text":"def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"test_on_batch"},{"location":"reference/kerod/model/smca_detr/#test_step_1","text":"def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name: m . result () for m in self . metrics }","title":"test_step"},{"location":"reference/kerod/model/smca_detr/#to_json_1","text":"def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs )","title":"to_json"},{"location":"reference/kerod/model/smca_detr/#to_yaml_1","text":"def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' )","title":"to_yaml"},{"location":"reference/kerod/model/smca_detr/#train_on_batch_1","text":"def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"train_on_batch"},{"location":"reference/kerod/model/smca_detr/#train_step_1","text":"def train_step ( self , data ) View Source def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics }","title":"train_step"},{"location":"reference/kerod/model/smca_detr/#smcar50pytorch","text":"class SMCAR50Pytorch ( num_classes , num_queries = 100 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: model = SMCAR50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"SMCAR50Pytorch"},{"location":"reference/kerod/model/smca_detr/#arguments_2","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries.","title":"Arguments"},{"location":"reference/kerod/model/smca_detr/#call-arguments_2","text":"Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode","title":"Call arguments"},{"location":"reference/kerod/model/smca_detr/#call-returns_2","text":"Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise.","title":"Call returns"},{"location":"reference/kerod/model/smca_detr/#ancestors-in-mro_2","text":"kerod.model.smca_detr.SMCA keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/smca_detr/#methods_2","text":"","title":"Methods"},{"location":"reference/kerod/model/smca_detr/#call_2","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Tuple 1. images: A 4-D tensor of float32 and shape [batch_size, None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. 3. images_padding_mask: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x ) [ 1 ] , tf . shape ( x ) [ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid ) [ 1 ] , 2 )) ] , axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , }","title":"call"},{"location":"reference/kerod/model/smca_detr/#compile_2","text":"def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Example: model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) Args: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: Loss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true, y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. run_eagerly: Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . steps_per_execution: Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). jit_compile: If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True is may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. **kwargs: Arguments supported for backwards compatibility only. View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = ' rmsprop ' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . Maybe be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = { ' output_a ' : ' accuracy ' , ' output_b ' : [ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the loss function used and the model output shape . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` is may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( ' compile ' ). set ( True ) with self . distribute_strategy . scope () : if ' experimental_steps_per_execution ' in kwargs : logging . warning ( ' The argument ` steps_per_execution ` is no longer ' ' experimental . Pass ` steps_per_execution ` instead of ' '` experimental_steps_per_execution ` . ' ) if not steps_per_execution : steps_per_execution = kwargs . pop ( ' experimental_steps_per_execution ' ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for multi-output # models, which have prefixes added for each corresponding output name). from_serialized = kwargs . pop ( ' from_serialized ' , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( ' You cannot enable ` run_eagerly ` and ` jit_compile ` ' ' at the same time . ' ) else : self . _jit_compile = jit_compile","title":"compile"},{"location":"reference/kerod/model/smca_detr/#compute_loss_2","text":"def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> int Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> int : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss","title":"compute_loss"},{"location":"reference/kerod/model/smca_detr/#compute_metrics_2","text":"def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: class MyModel ( tf . keras . Sequential ): def compute_metrics ( self , x , y , y_pred , sample_weight ): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super ( MyModel , self ) . compute_metrics ( x , y , y_pred , sample_weight ) # Note that `self.custom_metric` is not listed in `self.metrics`. self . custom_metric . update_state ( x , y , y_pred , sample_weight ) metric_results [ 'custom_metric_name' ] = self . custom_metric . result () return metric_results Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of model.call(x) ) sample_weight: Sample weights for weighting the loss function. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns results # for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics","title":"compute_metrics"},{"location":"reference/kerod/model/smca_detr/#evaluate_2","text":"def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. **kwargs Unused at this time. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = 1 , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( 'evaluate' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'evaluate' ) self . _assert_compile_was_called () self . _check_call_args ( 'evaluate' ) _disallow_inside_tf_function ( 'evaluate' ) use_cached_eval_dataset = kwargs . pop ( '_use_cached_eval_dataset' , False ) if kwargs : raise TypeError ( f 'Invalid keyword arguments: {list(kwargs.keys())}' ) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , '_eval_data_handler' , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( 'test' , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"evaluate"},{"location":"reference/kerod/model/smca_detr/#evaluate_generator_2","text":"def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.evaluate_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.evaluate`, which supports generators.' , stacklevel = 2 ) self . _check_call_args ( 'evaluate_generator' ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"evaluate_generator"},{"location":"reference/kerod/model/smca_detr/#fit_2","text":"def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( 'fit' ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( 'Model' , 'fit' ) self . _assert_compile_was_called () self . _check_call_args ( 'fit' ) _disallow_inside_tf_function ( 'fit' ) if verbose == 'auto' : if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access verbose = 2 # Default to epoch-level logging for PSStrategy. else : verbose = 1 # Default to batch-level logging otherwise. elif verbose == 1 and self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access raise ValueError ( '`verbose=1` is not allowed with `ParameterServerStrategy` for ' f 'performance reasons. Received: `verbose`={verbose}' ) if validation_split : # Create the validation data using the training data. Only supported for # `Tensor` and `NumPy` input. ( x , y , sample_weight ), validation_data = ( data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split )) if validation_data : val_x , val_y , val_sample_weight = ( data_adapter . unpack_x_y_sample_weight ( validation_data )) if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access self . _cluster_coordinator = tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. data_handler . _initial_epoch = ( # pylint: disable=protected-access self . _maybe_load_initial_epoch_from_ckpt ( initial_epoch )) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( 'train' , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () logs = tmp_logs # No error, now safe to assign to logs. end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( 'Unexpected result of `train_function` ' '(Empty logs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , '_eval_data_handler' , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True ) val_logs = { 'val_' + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , '_eval_data_handler' , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history","title":"fit"},{"location":"reference/kerod/model/smca_detr/#fit_generator_2","text":"def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.fit_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.fit`, which supports generators.' , stacklevel = 2 ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch )","title":"fit_generator"},{"location":"reference/kerod/model/smca_detr/#get_layer_2","text":"def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Description name String, name of layer. index Integer, index of layer. Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( 'Provide only a layer name or a layer index. Received: ' f 'index={index}, name={name}.' ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f 'Was asked to retrieve layer at index {index}' f ' but model only has {len(self.layers)}' ' layers.' ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f 'No such layer: {name}. Existing layers are: ' f '{list(layer.name for layer in self.layers)}.' ) raise ValueError ( 'Provide either a layer name or layer index at ' '`get_layer`.' )","title":"get_layer"},{"location":"reference/kerod/model/smca_detr/#load_weights_2","text":"def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Description filepath String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . by_name Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). options Optional tf.train.CheckpointOptions object that specifies options for loading weights. Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if ( self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ))): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( 'Load weights is not implemented with TPUStrategy ' 'with `steps_per_run` greater than 1. The ' f '`steps_per_run` is {spr}' ) if skip_mismatch and not by_name : raise ValueError ( 'When calling model.load_weights, skip_mismatch can only be set to ' 'True when by_name is True.' ) filepath , save_format = _detect_save_format ( filepath ) if save_format == 'tf' : status = self . _trackable_saver . restore ( filepath , options ) if by_name : raise NotImplementedError ( 'Weights may only be loaded based on topology into Models when ' 'loading TensorFlow-formatted weights (got by_name=True to ' 'load_weights).' ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( '`load_weights` requires h5py package when loading weights from ' 'HDF5. Try installing h5py.' ) if not self . _is_graph_network and not self . built : raise ValueError ( 'Unable to load weights saved in HDF5 format into a subclassed ' 'Model which has not created its variables yet. Call the Model ' 'first, then load the weights.' ) self . _assert_weights_created () with h5py . File ( filepath , 'r' ) as f : if 'layer_names' not in f . attrs and 'model_weights' in f : f = f [ 'model_weights' ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status","title":"load_weights"},{"location":"reference/kerod/model/smca_detr/#make_predict_function_2","text":"def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the predict function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'concat' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( t , tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape ) for t in tf . nest . flatten ( outputs ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , experimental_relax_shapes = True ) self . predict_function = predict_function return self . predict_function","title":"make_predict_function"},{"location":"reference/kerod/model/smca_detr/#make_test_function_2","text":"def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the test function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) if self . _cluster_coordinator : self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it ,)) else : self . test_function = test_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda test_function , args = ( it , self . _steps_per_execution . value ())) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , experimental_relax_shapes = True ) self . test_function = test_function return self . test_function","title":"make_test_function"},{"location":"reference/kerod/model/smca_detr/#make_train_function_2","text":"def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Description force Whether to regenerate the train function and skip the cached function if available. Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) # pylint: disable=protected-access return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , experimental_relax_shapes = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = 'first' ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it ,)) else : self . train_function = train_function # If we're using a coordinator, use the value of self._steps_per_execution # at the time the function is called/scheduled, and not when it is actually # executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( # pylint: disable=g-long-lambda train_function , args = ( it , self . _steps_per_execution . value ())) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , experimental_relax_shapes = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function","title":"make_train_function"},{"location":"reference/kerod/model/smca_detr/#predict_2","text":"def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Description x Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior<br>for iterator-like inputs section of Model.fit . batch_size Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose Verbosity mode, 0 or 1. steps Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. callbacks List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = 0 , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( 'predict' ). set ( True ) version_utils . disallow_legacy_graph ( 'Model' , 'predict' ) self . _check_call_args ( 'predict' ) _disallow_inside_tf_function ( 'predict' ) # TODO(yashkatariya): Cache model on the coordinator for faster prediction. # If running under PSS, then swap it with OneDeviceStrategy so that # execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : # pylint: disable=protected-access original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy )) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = data_option x = x . with_options ( options ) except ValueError : warnings . warn ( 'Using Model.predict with MultiWorkerMirroredStrategy or ' 'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order ' 'result. Consider setting it to AutoShardPolicy.DATA.' , stacklevel = 2 ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = tmp_batch_outputs # No error, now safe to assign. if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { 'outputs' : batch_outputs } ) if batch_outputs is None : raise ValueError ( 'Unexpected result of `predict_function` ' '(Empty batch_outputs). Please use ' '`Model.compile(..., run_eagerly=True)`, or ' '`tf.config.run_functions_eagerly(True)` for more ' 'information of where went wrong, or file a ' 'issue/bug to `tf.keras`.' ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , concat , outputs ) # If originally PSS strategy was used, then replace it back since predict # is running under `OneDeviceStrategy` after the swap and once its done # we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs )","title":"predict"},{"location":"reference/kerod/model/smca_detr/#predict_generator_2","text":"def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( '`Model.predict_generator` is deprecated and ' 'will be removed in a future version. ' 'Please use `Model.predict`, which supports generators.' , stacklevel = 2 ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks )","title":"predict_generator"},{"location":"reference/kerod/model/smca_detr/#predict_on_batch_2","text":"def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\"\" Returns predictions for a single batch of samples. Args : x : Input data . It could be : - A Numpy array ( or array - like ) , or a list of arrays ( in case the model has multiple inputs ) . - A TensorFlow tensor , or a list of tensors ( in case the model has multiple inputs ) . Returns : Numpy array ( s ) of predictions . Raises : RuntimeError : If ` model . predict_on_batch ` is wrapped in a ` tf . function `. \"\"\" self . _check_call_args ( ' predict_on_batch ' ) _disallow_inside_tf_function ( ' predict_on_batch ' ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs )","title":"predict_on_batch"},{"location":"reference/kerod/model/smca_detr/#predict_step_2","text":"def predict_step ( self , data ) Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Type Description Tuple - boxes : A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - scores : A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - classes : A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def predict_step ( self , data ) : \" \"\" Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \" To optimize for AP , we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4. Experiments of Object Detection with Transformers Returns: Tuple: - `boxes`: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - `scores`: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - `classes`: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\" \" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ] , y_pred [ BoxField . SCORES ] , x [ DatasetField . IMAGES_INFO ] , tf . shape ( x [ DatasetField . IMAGES ] ) [ 1 : 3 ] , ) return boxes_without_padding , scores , labels","title":"predict_step"},{"location":"reference/kerod/model/smca_detr/#reset_metrics_2","text":"def reset_metrics ( self ) Resets the state of all the metrics in the model. Examples: inputs = tf.keras.layers.Input(shape=(3,)) outputs = tf.keras.layers.Dense(2)(inputs) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) x = np.random.random((2, 3)) y = np.random.randint(0, 2, (2, 2)) _ = model.fit(x, y, verbose=0) assert all(float(m.result()) for m in model.metrics) model.reset_metrics() assert all(float(m.result()) == 0 for m in model.metrics) View Source def reset_metrics ( self ) : \"\"\" Resets the state of all the metrics in the model. Examples : >>> inputs = tf . keras . layers . Input ( shape = ( 3 , )) >>> outputs = tf . keras . layers . Dense ( 2 )( inputs ) >>> model = tf . keras . models . Model ( inputs = inputs , outputs = outputs ) >>> model . compile ( optimizer = \" Adam \" , loss = \" mse \" , metrics = [ \" mae \" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\"\" for m in self . metrics : m . reset_state ()","title":"reset_metrics"},{"location":"reference/kerod/model/smca_detr/#reset_states_2","text":"def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , ' reset_states ' ) and getattr ( layer , ' stateful ' , False ) : layer . reset_states ()","title":"reset_states"},{"location":"reference/kerod/model/smca_detr/#save_2","text":"def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. Example: from keras.models import load_model model . save ( 'my_model.h5' ) # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model ( 'my_model.h5' ) View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) : # pylint: disable=line-too-long \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" # pylint: enable=line-too-long save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces )","title":"save"},{"location":"reference/kerod/model/smca_detr/#save_spec_2","text":"def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Description dynamic_batch Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is # an empty dict since functional models do not use keyword arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False )","title":"save_spec"},{"location":"reference/kerod/model/smca_detr/#save_weights_2","text":"def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Description filepath String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. options Optional tf.train.CheckpointOptions object that specifies options for saving weights. Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = 'h5' else : save_format = 'tf' else : user_format = save_format . lower () . strip () if user_format in ( 'tensorflow' , 'tf' ): save_format = 'tf' elif user_format in ( 'hdf5' , 'h5' , 'keras' ): save_format = 'h5' else : raise ValueError ( f 'Unknown format. Received: `save_format`={save_format}. Was ' 'expecting one of {\"tf\", \"h5\"}.' ) if save_format == 'tf' and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f 'filepath ({filepath}) looks like an HDF5 file. ' 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == 'h5' and h5py is None : raise ImportError ( '`save_weights` requires h5py when saving in hdf5, but h5py is not ' 'available. Try installing h5py package.' ) if save_format == 'tf' : check_filepath = filepath + '.index' else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == 'h5' : with h5py . File ( filepath , 'w' ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if tf . executing_eagerly (): session = None else : session = backend . get_session () self . _trackable_saver . save ( filepath , session = session , options = options ) # Record this checkpoint so it's visible from tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ])","title":"save_weights"},{"location":"reference/kerod/model/smca_detr/#summary_2","text":"def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) Prints a string summary of the network. Parameters: Name Description line_length Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested Whether to expand the nested models. If not provided, defaults to False . show_trainable Whether to show if a layer is trainable. If not provided, defaults to False . Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( 'This model has not yet been built. ' 'Build the model first by calling `build()` or by calling ' 'the model on a batch of data.' ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable )","title":"summary"},{"location":"reference/kerod/model/smca_detr/#test_on_batch_2","text":"def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'test_on_batch' ) _disallow_inside_tf_function ( 'test_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"test_on_batch"},{"location":"reference/kerod/model/smca_detr/#test_step_2","text":"def test_step ( self , data ) View Source def test_step ( self , data ) : data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name: m . result () for m in self . metrics }","title":"test_step"},{"location":"reference/kerod/model/smca_detr/#to_json_2","text":"def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Description **kwargs Additional keyword arguments to be passed to json.dumps() . Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs )","title":"to_json"},{"location":"reference/kerod/model/smca_detr/#to_yaml_2","text":"def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Description **kwargs Additional keyword arguments to be passed to yaml.dump() . Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( 'Method `model.to_yaml()` has been removed due to security risk of ' 'arbitrary code execution. Please use `model.to_json()` instead.' )","title":"to_yaml"},{"location":"reference/kerod/model/smca_detr/#train_on_batch_2","text":"def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Description x Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( 'train_on_batch' ) _disallow_inside_tf_function ( 'train_on_batch' ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), \\ training_utils . RespectCompiledTrainableState ( self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"train_on_batch"},{"location":"reference/kerod/model/smca_detr/#train_step_2","text":"def train_step ( self , data ) View Source def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics }","title":"train_step"},{"location":"reference/kerod/model/backbone/","text":"Module kerod.model.backbone None None Sub-modules kerod.model.backbone.fpn kerod.model.backbone.resnet","title":"Index"},{"location":"reference/kerod/model/backbone/#module-kerodmodelbackbone","text":"None None","title":"Module kerod.model.backbone"},{"location":"reference/kerod/model/backbone/#sub-modules","text":"kerod.model.backbone.fpn kerod.model.backbone.resnet","title":"Sub-modules"},{"location":"reference/kerod/model/backbone/fpn/","text":"Module kerod.model.backbone.fpn None None View Source import tensorflow as tf from kerod . utils . documentation import remove_unwanted_doc from tensorflow . keras import layers from tensorflow . keras . initializers import VarianceScaling __ pdoc__ = {} class FPN ( layers . Layer ) : \"\"\"Over your backbone feature build a FPN (inspired from tensorpack)\"\"\" def __ init__ ( self , dim = 256 , kernel_regularizer = None , **kwargs ) : super (). __ init__ ( **kwargs ) self . _ dim = dim self . _ kernel_regularizer = kernel_regularizer def build ( self , input_shape ) : num_level_pyramid = len ( input_shape [ 0 ]) self . lateral_connection_2345 = [ layers . Conv2D ( self . _ dim , ( 1 , 1 ), padding='same' , kernel_initializer = VarianceScaling ( scale = 1. ), kernel_regularizer = self . _ kernel_regularizer ) for _ in range ( num_level_pyramid ) ] self . anti_aliasing_conv = [ layers . Conv2D ( self . _ dim , ( 3 , 3 ), padding='same' , kernel_initializer = VarianceScaling ( scale = 1. ), kernel_regularizer = self . _ kernel_regularizer ) for _ in range ( num_level_pyramid ) ] super (). build ( input_shape ) def call ( self , inputs ) : \"\"\"Over your backbone feature build a FPN (inspired from tensorpack) Arguments: inputs: A list of tensors of shape [N, height, widht, channels] Returns: A list of tensors of shape [N + 1, height, width, channels] \"\"\" lateral_connection_2345 = [ conv ( tensor ) for tensor , conv in zip ( inputs , self . lateral_connection_2345 ) ] lat_sum_5432 = [] for idx , block in enumerate ( lateral_connection_2345 [ ::- 1 ]) : if idx > 0 : up_shape = tf . shape ( block ) block = block + tf . image . resize ( lat_sum_5432 [ - 1 ], [ up_shape [ 1 ], up_shape [ 2 ]], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) lat_sum_5432 . append ( block ) # 3 \u00d73convolution on each merged map to generate the final feature map , # which is to reduce the aliasing effect of upsampling . lateral_connection_2345 = [ conv ( tensor ) for conv , tensor in zip ( self . anti_aliasing_conv , lat_sum_5432 [ ::- 1 ]) ] p6 = layers . MaxPool2D ()( lateral_connection_2345 [ - 1 ]) return lateral_connection_2345 + [ p6 ] def get_config ( self ) : base_config = super (). get_config () base_config [ 'dim' ] = self . _ dim return base_config remove_unwanted_doc ( FPN , __ pdoc__ ) Classes FPN class FPN ( dim = 256 , kernel_regularizer = None , ** kwargs ) Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , inputs ) Over your backbone feature build a FPN (inspired from tensorpack) Parameters: Name Description inputs A list of tensors of shape [N, height, widht, channels] Returns: Type Description None A list of tensors of shape [N + 1, height, width, channels] View Source def call ( self , inputs ) : \"\"\"Over your backbone feature build a FPN (inspired from tensorpack) Arguments: inputs: A list of tensors of shape [N, height, widht, channels] Returns: A list of tensors of shape [N + 1, height, width, channels] \"\"\" lateral_connection_2345 = [ conv ( tensor ) for tensor , conv in zip ( inputs , self . lateral_connection_2345 ) ] lat_sum_5432 = [] for idx , block in enumerate ( lateral_connection_2345 [ ::- 1 ]) : if idx > 0 : up_shape = tf . shape ( block ) block = block + tf . image . resize ( lat_sum_5432 [ - 1 ], [ up_shape [ 1 ], up_shape [ 2 ]], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) lat_sum_5432 . append ( block ) # 3 \u00d73convolution on each merged map to generate the final feature map , # which is to reduce the aliasing effect of upsampling . lateral_connection_2345 = [ conv ( tensor ) for conv , tensor in zip ( self . anti_aliasing_conv , lat_sum_5432 [ ::- 1 ]) ] p6 = layers . MaxPool2D ()( lateral_connection_2345 [ - 1 ]) return lateral_connection_2345 + [ p6 ]","title":"Fpn"},{"location":"reference/kerod/model/backbone/fpn/#module-kerodmodelbackbonefpn","text":"None None View Source import tensorflow as tf from kerod . utils . documentation import remove_unwanted_doc from tensorflow . keras import layers from tensorflow . keras . initializers import VarianceScaling __ pdoc__ = {} class FPN ( layers . Layer ) : \"\"\"Over your backbone feature build a FPN (inspired from tensorpack)\"\"\" def __ init__ ( self , dim = 256 , kernel_regularizer = None , **kwargs ) : super (). __ init__ ( **kwargs ) self . _ dim = dim self . _ kernel_regularizer = kernel_regularizer def build ( self , input_shape ) : num_level_pyramid = len ( input_shape [ 0 ]) self . lateral_connection_2345 = [ layers . Conv2D ( self . _ dim , ( 1 , 1 ), padding='same' , kernel_initializer = VarianceScaling ( scale = 1. ), kernel_regularizer = self . _ kernel_regularizer ) for _ in range ( num_level_pyramid ) ] self . anti_aliasing_conv = [ layers . Conv2D ( self . _ dim , ( 3 , 3 ), padding='same' , kernel_initializer = VarianceScaling ( scale = 1. ), kernel_regularizer = self . _ kernel_regularizer ) for _ in range ( num_level_pyramid ) ] super (). build ( input_shape ) def call ( self , inputs ) : \"\"\"Over your backbone feature build a FPN (inspired from tensorpack) Arguments: inputs: A list of tensors of shape [N, height, widht, channels] Returns: A list of tensors of shape [N + 1, height, width, channels] \"\"\" lateral_connection_2345 = [ conv ( tensor ) for tensor , conv in zip ( inputs , self . lateral_connection_2345 ) ] lat_sum_5432 = [] for idx , block in enumerate ( lateral_connection_2345 [ ::- 1 ]) : if idx > 0 : up_shape = tf . shape ( block ) block = block + tf . image . resize ( lat_sum_5432 [ - 1 ], [ up_shape [ 1 ], up_shape [ 2 ]], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) lat_sum_5432 . append ( block ) # 3 \u00d73convolution on each merged map to generate the final feature map , # which is to reduce the aliasing effect of upsampling . lateral_connection_2345 = [ conv ( tensor ) for conv , tensor in zip ( self . anti_aliasing_conv , lat_sum_5432 [ ::- 1 ]) ] p6 = layers . MaxPool2D ()( lateral_connection_2345 [ - 1 ]) return lateral_connection_2345 + [ p6 ] def get_config ( self ) : base_config = super (). get_config () base_config [ 'dim' ] = self . _ dim return base_config remove_unwanted_doc ( FPN , __ pdoc__ )","title":"Module kerod.model.backbone.fpn"},{"location":"reference/kerod/model/backbone/fpn/#classes","text":"","title":"Classes"},{"location":"reference/kerod/model/backbone/fpn/#fpn","text":"class FPN ( dim = 256 , kernel_regularizer = None , ** kwargs )","title":"FPN"},{"location":"reference/kerod/model/backbone/fpn/#ancestors-in-mro","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.autotrackable.AutoTrackable tensorflow.python.training.tracking.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/backbone/fpn/#methods","text":"","title":"Methods"},{"location":"reference/kerod/model/backbone/fpn/#call","text":"def call ( self , inputs ) Over your backbone feature build a FPN (inspired from tensorpack) Parameters: Name Description inputs A list of tensors of shape [N, height, widht, channels] Returns: Type Description None A list of tensors of shape [N + 1, height, width, channels] View Source def call ( self , inputs ) : \"\"\"Over your backbone feature build a FPN (inspired from tensorpack) Arguments: inputs: A list of tensors of shape [N, height, widht, channels] Returns: A list of tensors of shape [N + 1, height, width, channels] \"\"\" lateral_connection_2345 = [ conv ( tensor ) for tensor , conv in zip ( inputs , self . lateral_connection_2345 ) ] lat_sum_5432 = [] for idx , block in enumerate ( lateral_connection_2345 [ ::- 1 ]) : if idx > 0 : up_shape = tf . shape ( block ) block = block + tf . image . resize ( lat_sum_5432 [ - 1 ], [ up_shape [ 1 ], up_shape [ 2 ]], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) lat_sum_5432 . append ( block ) # 3 \u00d73convolution on each merged map to generate the final feature map , # which is to reduce the aliasing effect of upsampling . lateral_connection_2345 = [ conv ( tensor ) for conv , tensor in zip ( self . anti_aliasing_conv , lat_sum_5432 [ ::- 1 ]) ] p6 = layers . MaxPool2D ()( lateral_connection_2345 [ - 1 ]) return lateral_connection_2345 + [ p6 ]","title":"call"},{"location":"reference/kerod/model/backbone/resnet/","text":"Module kerod.model.backbone.resnet ResNet models for Keras. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) View Source # Copyright 2015 The TensorFlow Authors and Modified by Emilien Garreau. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== \"\"\"ResNet models for Keras. Reference paper: - [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) \"\"\" import os from typing import Callable import tensorflow as tf from tensorflow.keras import layers from tensorflow.python.keras import backend from tensorflow.keras.applications import resnet from tensorflow.python.keras.engine import training from tensorflow.python.keras.utils import data_utils , layer_utils OFFICIAL_WEIGHTS_PATH = ( 'https://storage.googleapis.com/tensorflow/keras-applications/resnet/' ) CUSTOM_WEIGHTS_PATH = ( 'https://files.heuritech.com/raw_files/' ) WEIGHTS_HASHES = { 'resnet50' : ( '4d473c1dd8becc155b73f8504c6f6626' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet50_pytorch' : ( '3ffd584081cc56435a3689d12afd7cf9' , CUSTOM_WEIGHTS_PATH , 'resnet50_tensorpack_conversion.h5' ), 'resnet101' : ( '88cf7a10940856eca736dc7b7e228a21' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet152' : ( 'ee4c566cf9a93f14d82f913c2dc6dd0c' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet50v2' : ( 'fac2f116257151a9d068a22e544a4917' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet101v2' : ( 'c0ed64b8031c3730f411d2eb4eea35b5' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet152v2' : ( 'ed17cf2e0169df9d443503ef94b23b33' , OFFICIAL_WEIGHTS_PATH , None ), 'resnext50' : ( '62527c363bdd9ec598bed41947b379fc' , OFFICIAL_WEIGHTS_PATH , None ), 'resnext101' : ( '0f678c91647380debd923963594981b3' , OFFICIAL_WEIGHTS_PATH , None ) } def padd_for_aligning_pixels ( inputs : tf . Tensor ): \"\"\"This padding operation is here to make the pixels of the output perfectly aligned. It will make the output perfectly aligned at stride 32. \"\"\" chan = inputs . shape [ 3 ] b4_stride = 32.0 shape2d = tf . shape ( inputs )[ 1 : 3 ] new_shape2d = tf . cast ( tf . math . ceil ( tf . cast ( shape2d , tf . float32 ) / b4_stride ) * b4_stride , tf . int32 ) pad_shape2d = new_shape2d - shape2d inputs = tf . pad ( inputs , tf . stack ([[ 0 , 0 ], [ 3 , 2 + pad_shape2d [ 0 ]], [ 3 , 2 + pad_shape2d [ 1 ]], [ 0 , 0 ]]), name = 'conv1_pad' ) # yapf: disable inputs . set_shape ([ None , None , None , chan ]) return inputs def ResNet ( stack_fn : Callable , preprocessing_func : Callable , preact : bool , use_bias : bool , model_name = 'resnet' , weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) -> tf . keras . Model : \"\"\"Instantiates the ResNet, ResNetV2, and ResNeXt architecture. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) Optionally loads weights pre-trained on ImageNet. Note that the data format convention used by the model is the one specified in your Keras config at `~/.keras/keras.json`. Caution: Be sure to properly pre-process your inputs to the application. Please see `applications.resnet.preprocess_input` for an example. Arguments: stack_fn: a function that returns output tensor for the stacked residual blocks. preprocessing_func: a function that returns the corresponding preprocessing of the network. preact: whether to use pre-activation or not (True for ResNetV2, False for ResNet and ResNeXt). use_bias: whether to use biases for convolutional layers or not (True for ResNet and ResNetV2, False for ResNeXt). model_name: string, model name. include_top: whether to include the fully-connected layer at the top of the network. weights: one of `None` (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model. input_shape: optional shape tuple, only to be specified if `include_top` is False (otherwise the input shape has to be `(224, 224, 3)` (with `channels_last` data format) or `(3, 224, 224)` (with `channels_first` data format). It should have exactly 3 inputs channels. kwargs: For backwards compatibility only. Returns: A `keras.Model` instance. Raises: *ValueError*: in case of invalid argument for `weights`, or invalid input shape. \"\"\" if kwargs : raise ValueError ( 'Unknown argument(s): %s ' % ( kwargs ,)) if not ( weights in { 'imagenet' , None } or os . path . exists ( weights )): raise ValueError ( 'The `weights` argument should be either ' '`None` (random initialization), `imagenet` ' '(pre-training on ImageNet), ' 'or the path to the weights file to be loaded.' ) if input_tensor is None : img_input = layers . Input ( shape = input_shape ) else : if not backend . is_keras_tensor ( input_tensor ): img_input = layers . Input ( tensor = input_tensor , shape = input_shape ) else : img_input = input_tensor bn_axis = 3 if backend . image_data_format () == 'channels_last' else 1 x = layers . Lambda ( preprocessing_func , name = \"preprocess_input\" )( img_input ) x = layers . Lambda ( padd_for_aligning_pixels , name = \"padd_for_aligning_pixels\" )( x ) x = layers . Conv2D ( 64 , 7 , strides = 2 , use_bias = use_bias , name = 'conv1_conv' )( x ) if not preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'conv1_bn' )( x ) x = layers . Activation ( 'relu' , name = 'conv1_relu' )( x ) x = layers . ZeroPadding2D ( padding = (( 1 , 0 ), ( 1 , 0 )), name = 'pool1_pad' )( x ) x = layers . MaxPooling2D ( 3 , strides = 2 , name = 'pool1_pool' )( x ) outputs = stack_fn ( x ) if preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'post_bn' )( outputs [ - 1 ]) outputs [ - 1 ] = layers . Activation ( 'relu' , name = 'post_relu' )( x ) # Ensure that the model takes into account # any potential predecessors of `input_tensor`. if input_tensor is not None : inputs = layer_utils . get_source_inputs ( input_tensor ) else : inputs = img_input # Create model. model = training . Model ( inputs , outputs , name = model_name ) # Load weights. if ( weights == 'imagenet' ) and ( model_name in WEIGHTS_HASHES ): file_hash , base_weight_path , file_name = WEIGHTS_HASHES [ model_name ] if file_name is None : file_name = model_name + '_weights_tf_dim_ordering_tf_kernels_notop.h5' weights_path = data_utils . get_file ( file_name , base_weight_path + file_name , cache_subdir = 'models' , file_hash = file_hash ) model . load_weights ( weights_path ) elif weights is not None : model . load_weights ( weights ) return model def ResNet50 ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ): \"\"\"Instantiates the ResNet50 architecture.\"\"\" # Or set to None resnet . layers = tf . keras . layers def stack_fn ( x ): c2 = resnet . stack1 ( x , 64 , 3 , stride1 = 1 , name = 'conv2' ) c3 = resnet . stack1 ( c2 , 128 , 4 , name = 'conv3' ) c4 = resnet . stack1 ( c3 , 256 , 6 , name = 'conv4' ) c5 = resnet . stack1 ( c4 , 512 , 3 , name = 'conv5' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , resnet . preprocess_input , False , True , 'resnet50' , weights , input_tensor , input_shape , ** kwargs ) def ResNet50PytorchStyle ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ): \"\"\"Instantiates the ResNet50 with the pytorch style architecture. In the bottleneck residual block, pytorch-style ResNet uses a 1x1 stride-1 convolutional layer followed by a 3x3 stride-2 convolutional layer. **Warning**: Do not forget to use `bgr` instead of `rgb`. ```python import functools import tensorflow_datasets as tfds from kerod.preprocessing import preprocess ds_train, ds_info = tfds.load(name=\"coco/2017\", split=\"train\", shuffle_files=True, with_info=True) ds_train = ds_train.map(functools.partial(preprocess, bgr=True), num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` \"\"\" def stack_fn ( x ): c2 = Group ( x , 64 , 3 , strides = 1 , name = 'resnet50/group0' ) c3 = Group ( c2 , 128 , 4 , strides = 2 , name = 'resnet50/group1' ) c4 = Group ( c3 , 256 , 6 , strides = 2 , name = 'resnet50/group2' ) c5 = Group ( c4 , 512 , 3 , strides = 2 , name = 'resnet50/group3' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , preprocess_input_pytorch , False , False , 'resnet50_pytorch' , weights , input_tensor , input_shape , ** kwargs ) def preprocess_input_pytorch ( images : tf . Tensor ): \"\"\"Will preprocess the images for the resnet trained on Tensorpack. The network has been trained using BGR. \"\"\" mean = tf . constant ([ 103.53 , 116.28 , 123.675 ], dtype = images . dtype ) std = tf . constant ([ 57.375 , 57.12 , 58.395 ], dtype = images . dtype ) images = ( images - mean ) / std return images def Group ( inputs : tf . Tensor , filters : int , blocks : int , strides : int , name = None ): \"\"\"A set of stacked residual blocks with the pytorch style Arguments: filters: integer, filters of the bottleneck layer in a block. blocks: number of blocks in the stacked blocks. strides: Stride of the second conv layer in the first block. \"\"\" x = Block ( inputs , filters , strides , use_conv_shortcut = True , name = f ' { name } /block0' ) for i in range ( 1 , blocks ): x = Block ( x , filters , 1 , use_conv_shortcut = False , name = f ' { name } /block { i } ' ) return x def Block ( inputs : tf . Tensor , filters : int , strides : int = 1 , use_conv_shortcut = True , name = None ): \"\"\"A residual block with the pytorch_style Arguments: inputs: The inputs tensor filters: integer, filters of the bottleneck layer. strides: default 1, stride of the second convolution layer. In the official Keras implementation the stride is performed on the first convolution. This is different in the pytorch implementation. use_conv_shortcut: Use convolution shortcut if True, otherwise identity shortcut. \"\"\" bn_axis = 3 if backend . image_data_format () == 'channels_last' else 1 if use_conv_shortcut : shortcut = layers . Conv2D ( 4 * filters , 1 , strides = strides , use_bias = False , padding = 'same' , name = f ' { name } /convshortcut' )( inputs ) shortcut = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /convshortcut/bn' )( shortcut ) else : shortcut = inputs x = layers . Conv2D ( filters , 1 , strides = 1 , use_bias = False , padding = 'same' , name = f ' { name } /conv1' )( inputs ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /conv1/bn' )( x ) x = layers . Activation ( 'relu' , name = f ' { name } /conv1/relu' )( x ) if strides == 2 : x = layers . ZeroPadding2D ( padding = (( 1 , 0 ), ( 1 , 0 )), name = f ' { name } /pad2' )( x ) x = layers . Conv2D ( filters , 3 , padding = 'valid' , use_bias = False , strides = strides , name = f ' { name } /conv2' )( x ) else : x = layers . Conv2D ( filters , 3 , use_bias = False , padding = 'same' , strides = strides , name = f ' { name } /conv2' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /conv2/bn' )( x ) x = layers . Activation ( 'relu' , name = f ' { name } /conv2/relu' )( x ) x = layers . Conv2D ( filters * 4 , 1 , use_bias = False , padding = 'same' , name = f ' { name } /conv3' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /conv3/bn' )( x ) x = layers . Add ()([ shortcut , x ]) return layers . Activation ( 'relu' , name = f ' { name } /last_relu' )( x ) Variables CUSTOM_WEIGHTS_PATH OFFICIAL_WEIGHTS_PATH WEIGHTS_HASHES Functions Block def Block ( inputs : tensorflow . python . framework . ops . Tensor , filters : int , strides : int = 1 , use_conv_shortcut = True , name = None ) A residual block with the pytorch_style Parameters: Name Description inputs The inputs tensor filters integer, filters of the bottleneck layer. strides default 1, stride of the second convolution layer. In the official Keras implementation the stride is performed on the first convolution. This is different in the pytorch implementation. use_conv_shortcut Use convolution shortcut if True, otherwise identity shortcut. View Source def Block ( inputs : tf . Tensor , filters : int , strides : int = 1 , use_conv_shortcut = True , name = None ) : \"\"\" A residual block with the pytorch_style Arguments : inputs : The inputs tensor filters : integer , filters of the bottleneck layer . strides : default 1 , stride of the second convolution layer . In the official Keras implementation the stride is performed on the first convolution . This is different in the pytorch implementation . use_conv_shortcut : Use convolution shortcut if True , otherwise identity shortcut . \"\"\" bn_axis = 3 if backend . image_data_format () == ' channels_last ' else 1 if use_conv_shortcut : shortcut = layers . Conv2D ( 4 * filters , 1 , strides = strides , use_bias = False , padding = ' same ' , name = f ' {name}/convshortcut ' )( inputs ) shortcut = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/convshortcut/bn ' )( shortcut ) else : shortcut = inputs x = layers . Conv2D ( filters , 1 , strides = 1 , use_bias = False , padding = ' same ' , name = f ' {name}/conv1 ' )( inputs ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/conv1/bn ' )( x ) x = layers . Activation ( ' relu ' , name = f ' {name}/conv1/relu ' )( x ) if strides == 2 : x = layers . ZeroPadding2D ( padding = (( 1 , 0 ) , ( 1 , 0 )) , name = f ' {name}/pad2 ' )( x ) x = layers . Conv2D ( filters , 3 , padding = ' valid ' , use_bias = False , strides = strides , name = f ' {name}/conv2 ' )( x ) else : x = layers . Conv2D ( filters , 3 , use_bias = False , padding = ' same ' , strides = strides , name = f ' {name}/conv2 ' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/conv2/bn ' )( x ) x = layers . Activation ( ' relu ' , name = f ' {name}/conv2/relu ' )( x ) x = layers . Conv2D ( filters * 4 , 1 , use_bias = False , padding = ' same ' , name = f ' {name}/conv3 ' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/conv3/bn ' )( x ) x = layers . Add ()( [ shortcut , x ] ) return layers . Activation ( ' relu ' , name = f ' {name}/last_relu ' )( x ) Group def Group ( inputs : tensorflow . python . framework . ops . Tensor , filters : int , blocks : int , strides : int , name = None ) A set of stacked residual blocks with the pytorch style Parameters: Name Description filters integer, filters of the bottleneck layer in a block. blocks number of blocks in the stacked blocks. strides Stride of the second conv layer in the first block. View Source def Group ( inputs : tf . Tensor , filters : int , blocks : int , strides : int , name = None ) : \"\"\" A set of stacked residual blocks with the pytorch style Arguments : filters : integer , filters of the bottleneck layer in a block . blocks : number of blocks in the stacked blocks . strides : Stride of the second conv layer in the first block . \"\"\" x = Block ( inputs , filters , strides , use_conv_shortcut = True , name = f ' {name}/block0 ' ) for i in range ( 1 , blocks ) : x = Block ( x , filters , 1 , use_conv_shortcut = False , name = f ' {name}/block{i} ' ) return x ResNet def ResNet ( stack_fn : Callable , preprocessing_func : Callable , preact : bool , use_bias : bool , model_name = 'resnet' , weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) -> keras . engine . training . Model Instantiates the ResNet, ResNetV2, and ResNeXt architecture. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) Optionally loads weights pre-trained on ImageNet. Note that the data format convention used by the model is the one specified in your Keras config at ~/.keras/keras.json . Caution: Be sure to properly pre-process your inputs to the application. Please see applications.resnet.preprocess_input for an example. Parameters: Name Description stack_fn a function that returns output tensor for the stacked residual blocks. preprocessing_func a function that returns the corresponding preprocessing of the network. preact whether to use pre-activation or not (True for ResNetV2, False for ResNet and ResNeXt). use_bias whether to use biases for convolutional layers or not (True for ResNet and ResNetV2, False for ResNeXt). model_name string, model name. include_top whether to include the fully-connected layer at the top of the network. weights one of None (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor optional Keras tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels. kwargs For backwards compatibility only. Returns: Type Description None A keras.Model instance. Raises: Type Description ValueError in case of invalid argument for weights , or invalid input shape. View Source def ResNet ( stack_fn : Callable , preprocessing_func : Callable , preact : bool , use_bias : bool , model_name = 'resnet' , weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) -> tf . keras . Model : \" \"\" Instantiates the ResNet, ResNetV2, and ResNeXt architecture. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) Optionally loads weights pre-trained on ImageNet. Note that the data format convention used by the model is the one specified in your Keras config at `~/.keras/keras.json`. Caution: Be sure to properly pre-process your inputs to the application. Please see `applications.resnet.preprocess_input` for an example. Arguments: stack_fn: a function that returns output tensor for the stacked residual blocks. preprocessing_func: a function that returns the corresponding preprocessing of the network. preact: whether to use pre-activation or not (True for ResNetV2, False for ResNet and ResNeXt). use_bias: whether to use biases for convolutional layers or not (True for ResNet and ResNetV2, False for ResNeXt). model_name: string, model name. include_top: whether to include the fully-connected layer at the top of the network. weights: one of `None` (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model. input_shape: optional shape tuple, only to be specified if `include_top` is False (otherwise the input shape has to be `(224, 224, 3)` (with `channels_last` data format) or `(3, 224, 224)` (with `channels_first` data format). It should have exactly 3 inputs channels. kwargs: For backwards compatibility only. Returns: A `keras.Model` instance. Raises: *ValueError*: in case of invalid argument for `weights`, or invalid input shape. \"\" \" if kwargs : raise ValueError ( 'Unknown argument(s): %s' % ( kwargs ,)) if not ( weights in { 'imagenet' , None } or os . path . exists ( weights )) : raise ValueError ( 'The `weights` argument should be either ' '`None` (random initialization), `imagenet` ' '(pre-training on ImageNet), ' 'or the path to the weights file to be loaded.' ) if input_tensor is None : img_input = layers . Input ( shape = input_shape ) else : if not backend . is_keras_tensor ( input_tensor ) : img_input = layers . Input ( tensor = input_tensor , shape = input_shape ) else : img_input = input_tensor bn_axis = 3 if backend . image_data_format () == 'channels_last' else 1 x = layers . Lambda ( preprocessing_func , name = \"preprocess_input\" )( img_input ) x = layers . Lambda ( padd_for_aligning_pixels , name = \"padd_for_aligning_pixels\" )( x ) x = layers . Conv2D ( 64 , 7 , strides = 2 , use_bias = use_bias , name = 'conv1_conv' )( x ) if not preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'conv1_bn' )( x ) x = layers . Activation ( 'relu' , name = 'conv1_relu' )( x ) x = layers . ZeroPadding2D ( padding = (( 1 , 0 ), ( 1 , 0 )), name = 'pool1_pad' )( x ) x = layers . MaxPooling2D ( 3 , strides = 2 , name = 'pool1_pool' )( x ) outputs = stack_fn ( x ) if preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'post_bn' )( outputs [ - 1 ] ) outputs [ - 1 ] = layers . Activation ( 'relu' , name = 'post_relu' )( x ) # Ensure that the model takes into account # any potential predecessors of `input_tensor`. if input_tensor is not None : inputs = layer_utils . get_source_inputs ( input_tensor ) else : inputs = img_input # Create model. model = training . Model ( inputs , outputs , name = model_name ) # Load weights. if ( weights == 'imagenet' ) and ( model_name in WEIGHTS_HASHES ) : file_hash , base_weight_path , file_name = WEIGHTS_HASHES [ model_name ] if file_name is None : file_name = model_name + '_weights_tf_dim_ordering_tf_kernels_notop.h5' weights_path = data_utils . get_file ( file_name , base_weight_path + file_name , cache_subdir = 'models' , file_hash = file_hash ) model . load_weights ( weights_path ) elif weights is not None : model . load_weights ( weights ) return model ResNet50 def ResNet50 ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) Instantiates the ResNet50 architecture. View Source def ResNet50 ( weights = ' imagenet ' , input_tensor = None , input_shape = None , ** kwargs ) : \"\"\" Instantiates the ResNet50 architecture. \"\"\" # Or set to None resnet . layers = tf . keras . layers def stack_fn ( x ) : c2 = resnet . stack1 ( x , 64 , 3 , stride1 = 1 , name = ' conv2 ' ) c3 = resnet . stack1 ( c2 , 128 , 4 , name = ' conv3 ' ) c4 = resnet . stack1 ( c3 , 256 , 6 , name = ' conv4 ' ) c5 = resnet . stack1 ( c4 , 512 , 3 , name = ' conv5 ' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , resnet . preprocess_input , False , True , ' resnet50 ' , weights , input_tensor , input_shape , ** kwargs ) ResNet50PytorchStyle def ResNet50PytorchStyle ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) Instantiates the ResNet50 with the pytorch style architecture. In the bottleneck residual block, pytorch-style ResNet uses a 1x1 stride-1 convolutional layer followed by a 3x3 stride-2 convolutional layer. Warning : Do not forget to use bgr instead of rgb . import functools import tensorflow_datasets as tfds from kerod.preprocessing import preprocess ds_train , ds_info = tfds . load ( name = \"coco/2017\" , split = \"train\" , shuffle_files = True , with_info = True ) ds_train = ds_train . map ( functools . partial ( preprocess , bgr = True ), num_parallel_calls = tf . data . experimental . AUTOTUNE ) View Source def ResNet50PytorchStyle ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ): \"\"\"Instantiates the ResNet50 with the pytorch style architecture. In the bottleneck residual block, pytorch-style ResNet uses a 1x1 stride-1 convolutional layer followed by a 3x3 stride-2 convolutional layer. **Warning**: Do not forget to use `bgr` instead of `rgb`. ```python import functools import tensorflow_datasets as tfds from kerod.preprocessing import preprocess ds_train, ds_info = tfds.load(name=\"coco/2017\", split=\"train\", shuffle_files=True, with_info=True) ds_train = ds_train.map(functools.partial(preprocess, bgr=True), num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` \"\"\" def stack_fn ( x ): c2 = Group ( x , 64 , 3 , strides = 1 , name = 'resnet50/group0' ) c3 = Group ( c2 , 128 , 4 , strides = 2 , name = 'resnet50/group1' ) c4 = Group ( c3 , 256 , 6 , strides = 2 , name = 'resnet50/group2' ) c5 = Group ( c4 , 512 , 3 , strides = 2 , name = 'resnet50/group3' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , preprocess_input_pytorch , False , False , 'resnet50_pytorch' , weights , input_tensor , input_shape , ** kwargs ) padd_for_aligning_pixels def padd_for_aligning_pixels ( inputs : tensorflow . python . framework . ops . Tensor ) This padding operation is here to make the pixels of the output perfectly aligned. It will make the output perfectly aligned at stride 32. View Source def padd_for_aligning_pixels ( inputs : tf . Tensor ) : \"\"\" This padding operation is here to make the pixels of the output perfectly aligned. It will make the output perfectly aligned at stride 32 . \"\"\" chan = inputs . shape [ 3 ] b4_stride = 32 . 0 shape2d = tf . shape ( inputs ) [ 1 : 3 ] new_shape2d = tf . cast ( tf . math . ceil ( tf . cast ( shape2d , tf . float32 ) / b4_stride ) * b4_stride , tf . int32 ) pad_shape2d = new_shape2d - shape2d inputs = tf . pad ( inputs , tf . stack ( [[ 0 , 0 ], [ 3 , 2 + pad_shape2d [ 0 ]], [ 3 , 2 + pad_shape2d [ 1 ]], [ 0 , 0 ]] ) , name = ' conv1_pad ' ) # yapf : disable inputs . set_shape ( [ None , None , None , chan ] ) return inputs preprocess_input_pytorch def preprocess_input_pytorch ( images : tensorflow . python . framework . ops . Tensor ) Will preprocess the images for the resnet trained on Tensorpack. The network has been trained using BGR. View Source def preprocess_input_pytorch ( images : tf . Tensor ): \"\"\"Will preprocess the images for the resnet trained on Tensorpack. The network has been trained using BGR. \"\"\" mean = tf . constant ([ 103.53 , 116.28 , 123.675 ], dtype = images . dtype ) std = tf . constant ([ 57.375 , 57.12 , 58.395 ], dtype = images . dtype ) images = ( images - mean ) / std return images","title":"Resnet"},{"location":"reference/kerod/model/backbone/resnet/#module-kerodmodelbackboneresnet","text":"ResNet models for Keras. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) View Source # Copyright 2015 The TensorFlow Authors and Modified by Emilien Garreau. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== \"\"\"ResNet models for Keras. Reference paper: - [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) \"\"\" import os from typing import Callable import tensorflow as tf from tensorflow.keras import layers from tensorflow.python.keras import backend from tensorflow.keras.applications import resnet from tensorflow.python.keras.engine import training from tensorflow.python.keras.utils import data_utils , layer_utils OFFICIAL_WEIGHTS_PATH = ( 'https://storage.googleapis.com/tensorflow/keras-applications/resnet/' ) CUSTOM_WEIGHTS_PATH = ( 'https://files.heuritech.com/raw_files/' ) WEIGHTS_HASHES = { 'resnet50' : ( '4d473c1dd8becc155b73f8504c6f6626' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet50_pytorch' : ( '3ffd584081cc56435a3689d12afd7cf9' , CUSTOM_WEIGHTS_PATH , 'resnet50_tensorpack_conversion.h5' ), 'resnet101' : ( '88cf7a10940856eca736dc7b7e228a21' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet152' : ( 'ee4c566cf9a93f14d82f913c2dc6dd0c' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet50v2' : ( 'fac2f116257151a9d068a22e544a4917' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet101v2' : ( 'c0ed64b8031c3730f411d2eb4eea35b5' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet152v2' : ( 'ed17cf2e0169df9d443503ef94b23b33' , OFFICIAL_WEIGHTS_PATH , None ), 'resnext50' : ( '62527c363bdd9ec598bed41947b379fc' , OFFICIAL_WEIGHTS_PATH , None ), 'resnext101' : ( '0f678c91647380debd923963594981b3' , OFFICIAL_WEIGHTS_PATH , None ) } def padd_for_aligning_pixels ( inputs : tf . Tensor ): \"\"\"This padding operation is here to make the pixels of the output perfectly aligned. It will make the output perfectly aligned at stride 32. \"\"\" chan = inputs . shape [ 3 ] b4_stride = 32.0 shape2d = tf . shape ( inputs )[ 1 : 3 ] new_shape2d = tf . cast ( tf . math . ceil ( tf . cast ( shape2d , tf . float32 ) / b4_stride ) * b4_stride , tf . int32 ) pad_shape2d = new_shape2d - shape2d inputs = tf . pad ( inputs , tf . stack ([[ 0 , 0 ], [ 3 , 2 + pad_shape2d [ 0 ]], [ 3 , 2 + pad_shape2d [ 1 ]], [ 0 , 0 ]]), name = 'conv1_pad' ) # yapf: disable inputs . set_shape ([ None , None , None , chan ]) return inputs def ResNet ( stack_fn : Callable , preprocessing_func : Callable , preact : bool , use_bias : bool , model_name = 'resnet' , weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) -> tf . keras . Model : \"\"\"Instantiates the ResNet, ResNetV2, and ResNeXt architecture. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) Optionally loads weights pre-trained on ImageNet. Note that the data format convention used by the model is the one specified in your Keras config at `~/.keras/keras.json`. Caution: Be sure to properly pre-process your inputs to the application. Please see `applications.resnet.preprocess_input` for an example. Arguments: stack_fn: a function that returns output tensor for the stacked residual blocks. preprocessing_func: a function that returns the corresponding preprocessing of the network. preact: whether to use pre-activation or not (True for ResNetV2, False for ResNet and ResNeXt). use_bias: whether to use biases for convolutional layers or not (True for ResNet and ResNetV2, False for ResNeXt). model_name: string, model name. include_top: whether to include the fully-connected layer at the top of the network. weights: one of `None` (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model. input_shape: optional shape tuple, only to be specified if `include_top` is False (otherwise the input shape has to be `(224, 224, 3)` (with `channels_last` data format) or `(3, 224, 224)` (with `channels_first` data format). It should have exactly 3 inputs channels. kwargs: For backwards compatibility only. Returns: A `keras.Model` instance. Raises: *ValueError*: in case of invalid argument for `weights`, or invalid input shape. \"\"\" if kwargs : raise ValueError ( 'Unknown argument(s): %s ' % ( kwargs ,)) if not ( weights in { 'imagenet' , None } or os . path . exists ( weights )): raise ValueError ( 'The `weights` argument should be either ' '`None` (random initialization), `imagenet` ' '(pre-training on ImageNet), ' 'or the path to the weights file to be loaded.' ) if input_tensor is None : img_input = layers . Input ( shape = input_shape ) else : if not backend . is_keras_tensor ( input_tensor ): img_input = layers . Input ( tensor = input_tensor , shape = input_shape ) else : img_input = input_tensor bn_axis = 3 if backend . image_data_format () == 'channels_last' else 1 x = layers . Lambda ( preprocessing_func , name = \"preprocess_input\" )( img_input ) x = layers . Lambda ( padd_for_aligning_pixels , name = \"padd_for_aligning_pixels\" )( x ) x = layers . Conv2D ( 64 , 7 , strides = 2 , use_bias = use_bias , name = 'conv1_conv' )( x ) if not preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'conv1_bn' )( x ) x = layers . Activation ( 'relu' , name = 'conv1_relu' )( x ) x = layers . ZeroPadding2D ( padding = (( 1 , 0 ), ( 1 , 0 )), name = 'pool1_pad' )( x ) x = layers . MaxPooling2D ( 3 , strides = 2 , name = 'pool1_pool' )( x ) outputs = stack_fn ( x ) if preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'post_bn' )( outputs [ - 1 ]) outputs [ - 1 ] = layers . Activation ( 'relu' , name = 'post_relu' )( x ) # Ensure that the model takes into account # any potential predecessors of `input_tensor`. if input_tensor is not None : inputs = layer_utils . get_source_inputs ( input_tensor ) else : inputs = img_input # Create model. model = training . Model ( inputs , outputs , name = model_name ) # Load weights. if ( weights == 'imagenet' ) and ( model_name in WEIGHTS_HASHES ): file_hash , base_weight_path , file_name = WEIGHTS_HASHES [ model_name ] if file_name is None : file_name = model_name + '_weights_tf_dim_ordering_tf_kernels_notop.h5' weights_path = data_utils . get_file ( file_name , base_weight_path + file_name , cache_subdir = 'models' , file_hash = file_hash ) model . load_weights ( weights_path ) elif weights is not None : model . load_weights ( weights ) return model def ResNet50 ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ): \"\"\"Instantiates the ResNet50 architecture.\"\"\" # Or set to None resnet . layers = tf . keras . layers def stack_fn ( x ): c2 = resnet . stack1 ( x , 64 , 3 , stride1 = 1 , name = 'conv2' ) c3 = resnet . stack1 ( c2 , 128 , 4 , name = 'conv3' ) c4 = resnet . stack1 ( c3 , 256 , 6 , name = 'conv4' ) c5 = resnet . stack1 ( c4 , 512 , 3 , name = 'conv5' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , resnet . preprocess_input , False , True , 'resnet50' , weights , input_tensor , input_shape , ** kwargs ) def ResNet50PytorchStyle ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ): \"\"\"Instantiates the ResNet50 with the pytorch style architecture. In the bottleneck residual block, pytorch-style ResNet uses a 1x1 stride-1 convolutional layer followed by a 3x3 stride-2 convolutional layer. **Warning**: Do not forget to use `bgr` instead of `rgb`. ```python import functools import tensorflow_datasets as tfds from kerod.preprocessing import preprocess ds_train, ds_info = tfds.load(name=\"coco/2017\", split=\"train\", shuffle_files=True, with_info=True) ds_train = ds_train.map(functools.partial(preprocess, bgr=True), num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` \"\"\" def stack_fn ( x ): c2 = Group ( x , 64 , 3 , strides = 1 , name = 'resnet50/group0' ) c3 = Group ( c2 , 128 , 4 , strides = 2 , name = 'resnet50/group1' ) c4 = Group ( c3 , 256 , 6 , strides = 2 , name = 'resnet50/group2' ) c5 = Group ( c4 , 512 , 3 , strides = 2 , name = 'resnet50/group3' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , preprocess_input_pytorch , False , False , 'resnet50_pytorch' , weights , input_tensor , input_shape , ** kwargs ) def preprocess_input_pytorch ( images : tf . Tensor ): \"\"\"Will preprocess the images for the resnet trained on Tensorpack. The network has been trained using BGR. \"\"\" mean = tf . constant ([ 103.53 , 116.28 , 123.675 ], dtype = images . dtype ) std = tf . constant ([ 57.375 , 57.12 , 58.395 ], dtype = images . dtype ) images = ( images - mean ) / std return images def Group ( inputs : tf . Tensor , filters : int , blocks : int , strides : int , name = None ): \"\"\"A set of stacked residual blocks with the pytorch style Arguments: filters: integer, filters of the bottleneck layer in a block. blocks: number of blocks in the stacked blocks. strides: Stride of the second conv layer in the first block. \"\"\" x = Block ( inputs , filters , strides , use_conv_shortcut = True , name = f ' { name } /block0' ) for i in range ( 1 , blocks ): x = Block ( x , filters , 1 , use_conv_shortcut = False , name = f ' { name } /block { i } ' ) return x def Block ( inputs : tf . Tensor , filters : int , strides : int = 1 , use_conv_shortcut = True , name = None ): \"\"\"A residual block with the pytorch_style Arguments: inputs: The inputs tensor filters: integer, filters of the bottleneck layer. strides: default 1, stride of the second convolution layer. In the official Keras implementation the stride is performed on the first convolution. This is different in the pytorch implementation. use_conv_shortcut: Use convolution shortcut if True, otherwise identity shortcut. \"\"\" bn_axis = 3 if backend . image_data_format () == 'channels_last' else 1 if use_conv_shortcut : shortcut = layers . Conv2D ( 4 * filters , 1 , strides = strides , use_bias = False , padding = 'same' , name = f ' { name } /convshortcut' )( inputs ) shortcut = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /convshortcut/bn' )( shortcut ) else : shortcut = inputs x = layers . Conv2D ( filters , 1 , strides = 1 , use_bias = False , padding = 'same' , name = f ' { name } /conv1' )( inputs ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /conv1/bn' )( x ) x = layers . Activation ( 'relu' , name = f ' { name } /conv1/relu' )( x ) if strides == 2 : x = layers . ZeroPadding2D ( padding = (( 1 , 0 ), ( 1 , 0 )), name = f ' { name } /pad2' )( x ) x = layers . Conv2D ( filters , 3 , padding = 'valid' , use_bias = False , strides = strides , name = f ' { name } /conv2' )( x ) else : x = layers . Conv2D ( filters , 3 , use_bias = False , padding = 'same' , strides = strides , name = f ' { name } /conv2' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /conv2/bn' )( x ) x = layers . Activation ( 'relu' , name = f ' { name } /conv2/relu' )( x ) x = layers . Conv2D ( filters * 4 , 1 , use_bias = False , padding = 'same' , name = f ' { name } /conv3' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /conv3/bn' )( x ) x = layers . Add ()([ shortcut , x ]) return layers . Activation ( 'relu' , name = f ' { name } /last_relu' )( x )","title":"Module kerod.model.backbone.resnet"},{"location":"reference/kerod/model/backbone/resnet/#variables","text":"CUSTOM_WEIGHTS_PATH OFFICIAL_WEIGHTS_PATH WEIGHTS_HASHES","title":"Variables"},{"location":"reference/kerod/model/backbone/resnet/#functions","text":"","title":"Functions"},{"location":"reference/kerod/model/backbone/resnet/#block","text":"def Block ( inputs : tensorflow . python . framework . ops . Tensor , filters : int , strides : int = 1 , use_conv_shortcut = True , name = None ) A residual block with the pytorch_style Parameters: Name Description inputs The inputs tensor filters integer, filters of the bottleneck layer. strides default 1, stride of the second convolution layer. In the official Keras implementation the stride is performed on the first convolution. This is different in the pytorch implementation. use_conv_shortcut Use convolution shortcut if True, otherwise identity shortcut. View Source def Block ( inputs : tf . Tensor , filters : int , strides : int = 1 , use_conv_shortcut = True , name = None ) : \"\"\" A residual block with the pytorch_style Arguments : inputs : The inputs tensor filters : integer , filters of the bottleneck layer . strides : default 1 , stride of the second convolution layer . In the official Keras implementation the stride is performed on the first convolution . This is different in the pytorch implementation . use_conv_shortcut : Use convolution shortcut if True , otherwise identity shortcut . \"\"\" bn_axis = 3 if backend . image_data_format () == ' channels_last ' else 1 if use_conv_shortcut : shortcut = layers . Conv2D ( 4 * filters , 1 , strides = strides , use_bias = False , padding = ' same ' , name = f ' {name}/convshortcut ' )( inputs ) shortcut = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/convshortcut/bn ' )( shortcut ) else : shortcut = inputs x = layers . Conv2D ( filters , 1 , strides = 1 , use_bias = False , padding = ' same ' , name = f ' {name}/conv1 ' )( inputs ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/conv1/bn ' )( x ) x = layers . Activation ( ' relu ' , name = f ' {name}/conv1/relu ' )( x ) if strides == 2 : x = layers . ZeroPadding2D ( padding = (( 1 , 0 ) , ( 1 , 0 )) , name = f ' {name}/pad2 ' )( x ) x = layers . Conv2D ( filters , 3 , padding = ' valid ' , use_bias = False , strides = strides , name = f ' {name}/conv2 ' )( x ) else : x = layers . Conv2D ( filters , 3 , use_bias = False , padding = ' same ' , strides = strides , name = f ' {name}/conv2 ' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/conv2/bn ' )( x ) x = layers . Activation ( ' relu ' , name = f ' {name}/conv2/relu ' )( x ) x = layers . Conv2D ( filters * 4 , 1 , use_bias = False , padding = ' same ' , name = f ' {name}/conv3 ' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/conv3/bn ' )( x ) x = layers . Add ()( [ shortcut , x ] ) return layers . Activation ( ' relu ' , name = f ' {name}/last_relu ' )( x )","title":"Block"},{"location":"reference/kerod/model/backbone/resnet/#group","text":"def Group ( inputs : tensorflow . python . framework . ops . Tensor , filters : int , blocks : int , strides : int , name = None ) A set of stacked residual blocks with the pytorch style Parameters: Name Description filters integer, filters of the bottleneck layer in a block. blocks number of blocks in the stacked blocks. strides Stride of the second conv layer in the first block. View Source def Group ( inputs : tf . Tensor , filters : int , blocks : int , strides : int , name = None ) : \"\"\" A set of stacked residual blocks with the pytorch style Arguments : filters : integer , filters of the bottleneck layer in a block . blocks : number of blocks in the stacked blocks . strides : Stride of the second conv layer in the first block . \"\"\" x = Block ( inputs , filters , strides , use_conv_shortcut = True , name = f ' {name}/block0 ' ) for i in range ( 1 , blocks ) : x = Block ( x , filters , 1 , use_conv_shortcut = False , name = f ' {name}/block{i} ' ) return x","title":"Group"},{"location":"reference/kerod/model/backbone/resnet/#resnet","text":"def ResNet ( stack_fn : Callable , preprocessing_func : Callable , preact : bool , use_bias : bool , model_name = 'resnet' , weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) -> keras . engine . training . Model Instantiates the ResNet, ResNetV2, and ResNeXt architecture. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) Optionally loads weights pre-trained on ImageNet. Note that the data format convention used by the model is the one specified in your Keras config at ~/.keras/keras.json . Caution: Be sure to properly pre-process your inputs to the application. Please see applications.resnet.preprocess_input for an example. Parameters: Name Description stack_fn a function that returns output tensor for the stacked residual blocks. preprocessing_func a function that returns the corresponding preprocessing of the network. preact whether to use pre-activation or not (True for ResNetV2, False for ResNet and ResNeXt). use_bias whether to use biases for convolutional layers or not (True for ResNet and ResNetV2, False for ResNeXt). model_name string, model name. include_top whether to include the fully-connected layer at the top of the network. weights one of None (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor optional Keras tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels. kwargs For backwards compatibility only. Returns: Type Description None A keras.Model instance. Raises: Type Description ValueError in case of invalid argument for weights , or invalid input shape. View Source def ResNet ( stack_fn : Callable , preprocessing_func : Callable , preact : bool , use_bias : bool , model_name = 'resnet' , weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) -> tf . keras . Model : \" \"\" Instantiates the ResNet, ResNetV2, and ResNeXt architecture. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) Optionally loads weights pre-trained on ImageNet. Note that the data format convention used by the model is the one specified in your Keras config at `~/.keras/keras.json`. Caution: Be sure to properly pre-process your inputs to the application. Please see `applications.resnet.preprocess_input` for an example. Arguments: stack_fn: a function that returns output tensor for the stacked residual blocks. preprocessing_func: a function that returns the corresponding preprocessing of the network. preact: whether to use pre-activation or not (True for ResNetV2, False for ResNet and ResNeXt). use_bias: whether to use biases for convolutional layers or not (True for ResNet and ResNetV2, False for ResNeXt). model_name: string, model name. include_top: whether to include the fully-connected layer at the top of the network. weights: one of `None` (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model. input_shape: optional shape tuple, only to be specified if `include_top` is False (otherwise the input shape has to be `(224, 224, 3)` (with `channels_last` data format) or `(3, 224, 224)` (with `channels_first` data format). It should have exactly 3 inputs channels. kwargs: For backwards compatibility only. Returns: A `keras.Model` instance. Raises: *ValueError*: in case of invalid argument for `weights`, or invalid input shape. \"\" \" if kwargs : raise ValueError ( 'Unknown argument(s): %s' % ( kwargs ,)) if not ( weights in { 'imagenet' , None } or os . path . exists ( weights )) : raise ValueError ( 'The `weights` argument should be either ' '`None` (random initialization), `imagenet` ' '(pre-training on ImageNet), ' 'or the path to the weights file to be loaded.' ) if input_tensor is None : img_input = layers . Input ( shape = input_shape ) else : if not backend . is_keras_tensor ( input_tensor ) : img_input = layers . Input ( tensor = input_tensor , shape = input_shape ) else : img_input = input_tensor bn_axis = 3 if backend . image_data_format () == 'channels_last' else 1 x = layers . Lambda ( preprocessing_func , name = \"preprocess_input\" )( img_input ) x = layers . Lambda ( padd_for_aligning_pixels , name = \"padd_for_aligning_pixels\" )( x ) x = layers . Conv2D ( 64 , 7 , strides = 2 , use_bias = use_bias , name = 'conv1_conv' )( x ) if not preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'conv1_bn' )( x ) x = layers . Activation ( 'relu' , name = 'conv1_relu' )( x ) x = layers . ZeroPadding2D ( padding = (( 1 , 0 ), ( 1 , 0 )), name = 'pool1_pad' )( x ) x = layers . MaxPooling2D ( 3 , strides = 2 , name = 'pool1_pool' )( x ) outputs = stack_fn ( x ) if preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'post_bn' )( outputs [ - 1 ] ) outputs [ - 1 ] = layers . Activation ( 'relu' , name = 'post_relu' )( x ) # Ensure that the model takes into account # any potential predecessors of `input_tensor`. if input_tensor is not None : inputs = layer_utils . get_source_inputs ( input_tensor ) else : inputs = img_input # Create model. model = training . Model ( inputs , outputs , name = model_name ) # Load weights. if ( weights == 'imagenet' ) and ( model_name in WEIGHTS_HASHES ) : file_hash , base_weight_path , file_name = WEIGHTS_HASHES [ model_name ] if file_name is None : file_name = model_name + '_weights_tf_dim_ordering_tf_kernels_notop.h5' weights_path = data_utils . get_file ( file_name , base_weight_path + file_name , cache_subdir = 'models' , file_hash = file_hash ) model . load_weights ( weights_path ) elif weights is not None : model . load_weights ( weights ) return model","title":"ResNet"},{"location":"reference/kerod/model/backbone/resnet/#resnet50","text":"def ResNet50 ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) Instantiates the ResNet50 architecture. View Source def ResNet50 ( weights = ' imagenet ' , input_tensor = None , input_shape = None , ** kwargs ) : \"\"\" Instantiates the ResNet50 architecture. \"\"\" # Or set to None resnet . layers = tf . keras . layers def stack_fn ( x ) : c2 = resnet . stack1 ( x , 64 , 3 , stride1 = 1 , name = ' conv2 ' ) c3 = resnet . stack1 ( c2 , 128 , 4 , name = ' conv3 ' ) c4 = resnet . stack1 ( c3 , 256 , 6 , name = ' conv4 ' ) c5 = resnet . stack1 ( c4 , 512 , 3 , name = ' conv5 ' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , resnet . preprocess_input , False , True , ' resnet50 ' , weights , input_tensor , input_shape , ** kwargs )","title":"ResNet50"},{"location":"reference/kerod/model/backbone/resnet/#resnet50pytorchstyle","text":"def ResNet50PytorchStyle ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) Instantiates the ResNet50 with the pytorch style architecture. In the bottleneck residual block, pytorch-style ResNet uses a 1x1 stride-1 convolutional layer followed by a 3x3 stride-2 convolutional layer. Warning : Do not forget to use bgr instead of rgb . import functools import tensorflow_datasets as tfds from kerod.preprocessing import preprocess ds_train , ds_info = tfds . load ( name = \"coco/2017\" , split = \"train\" , shuffle_files = True , with_info = True ) ds_train = ds_train . map ( functools . partial ( preprocess , bgr = True ), num_parallel_calls = tf . data . experimental . AUTOTUNE ) View Source def ResNet50PytorchStyle ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ): \"\"\"Instantiates the ResNet50 with the pytorch style architecture. In the bottleneck residual block, pytorch-style ResNet uses a 1x1 stride-1 convolutional layer followed by a 3x3 stride-2 convolutional layer. **Warning**: Do not forget to use `bgr` instead of `rgb`. ```python import functools import tensorflow_datasets as tfds from kerod.preprocessing import preprocess ds_train, ds_info = tfds.load(name=\"coco/2017\", split=\"train\", shuffle_files=True, with_info=True) ds_train = ds_train.map(functools.partial(preprocess, bgr=True), num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` \"\"\" def stack_fn ( x ): c2 = Group ( x , 64 , 3 , strides = 1 , name = 'resnet50/group0' ) c3 = Group ( c2 , 128 , 4 , strides = 2 , name = 'resnet50/group1' ) c4 = Group ( c3 , 256 , 6 , strides = 2 , name = 'resnet50/group2' ) c5 = Group ( c4 , 512 , 3 , strides = 2 , name = 'resnet50/group3' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , preprocess_input_pytorch , False , False , 'resnet50_pytorch' , weights , input_tensor , input_shape , ** kwargs )","title":"ResNet50PytorchStyle"},{"location":"reference/kerod/model/backbone/resnet/#padd_for_aligning_pixels","text":"def padd_for_aligning_pixels ( inputs : tensorflow . python . framework . ops . Tensor ) This padding operation is here to make the pixels of the output perfectly aligned. It will make the output perfectly aligned at stride 32. View Source def padd_for_aligning_pixels ( inputs : tf . Tensor ) : \"\"\" This padding operation is here to make the pixels of the output perfectly aligned. It will make the output perfectly aligned at stride 32 . \"\"\" chan = inputs . shape [ 3 ] b4_stride = 32 . 0 shape2d = tf . shape ( inputs ) [ 1 : 3 ] new_shape2d = tf . cast ( tf . math . ceil ( tf . cast ( shape2d , tf . float32 ) / b4_stride ) * b4_stride , tf . int32 ) pad_shape2d = new_shape2d - shape2d inputs = tf . pad ( inputs , tf . stack ( [[ 0 , 0 ], [ 3 , 2 + pad_shape2d [ 0 ]], [ 3 , 2 + pad_shape2d [ 1 ]], [ 0 , 0 ]] ) , name = ' conv1_pad ' ) # yapf : disable inputs . set_shape ( [ None , None , None , chan ] ) return inputs","title":"padd_for_aligning_pixels"},{"location":"reference/kerod/model/backbone/resnet/#preprocess_input_pytorch","text":"def preprocess_input_pytorch ( images : tensorflow . python . framework . ops . Tensor ) Will preprocess the images for the resnet trained on Tensorpack. The network has been trained using BGR. View Source def preprocess_input_pytorch ( images : tf . Tensor ): \"\"\"Will preprocess the images for the resnet trained on Tensorpack. The network has been trained using BGR. \"\"\" mean = tf . constant ([ 103.53 , 116.28 , 123.675 ], dtype = images . dtype ) std = tf . constant ([ 57.375 , 57.12 , 58.395 ], dtype = images . dtype ) images = ( images - mean ) / std return images","title":"preprocess_input_pytorch"},{"location":"reference/kerod/utils/","text":"Module kerod.utils None None View Source from kerod.utils.ops import item_assignment , get_full_indices Sub-modules kerod.utils.documentation kerod.utils.drawing kerod.utils.ops kerod.utils.training","title":"Index"},{"location":"reference/kerod/utils/#module-kerodutils","text":"None None View Source from kerod.utils.ops import item_assignment , get_full_indices","title":"Module kerod.utils"},{"location":"reference/kerod/utils/#sub-modules","text":"kerod.utils.documentation kerod.utils.drawing kerod.utils.ops kerod.utils.training","title":"Sub-modules"},{"location":"reference/kerod/utils/documentation/","text":"Module kerod.utils.documentation None None View Source import tensorflow as tf from tensorflow.python.keras.engine.training import Model def remove_unwanted_doc ( _class , pdoc : dict ): \"\"\"Remove unwanted documentation from tensorflow keras inheritance.\"\"\" if issubclass ( _class , Model ): doc_class_to_remove = Model elif issubclass ( _class , tf . keras . layers . Layer ): doc_class_to_remove = tf . keras . layers . Layer for k in doc_class_to_remove . __dict__ : if k not in [ 'call' , '__doc__' , '__module__' , '__init__' , '__name__' ]: pdoc [ f ' { _class . __name__ } . { k } ' ] = None pdoc [ f ' { _class . __name__ } .with_name_scope' ] = None Functions remove_unwanted_doc def remove_unwanted_doc ( _class , pdoc : dict ) Remove unwanted documentation from tensorflow keras inheritance. View Source def remove_unwanted_doc ( _class , pdoc : dict ) : \"\"\" Remove unwanted documentation from tensorflow keras inheritance. \"\"\" if issubclass ( _class , Model ) : doc_class_to_remove = Model elif issubclass ( _class , tf . keras . layers . Layer ) : doc_class_to_remove = tf . keras . layers . Layer for k in doc_class_to_remove . __dict__ : if k not in [ ' call ' , ' __doc__ ' , ' __module__ ' , ' __init__ ' , ' __name__ ' ]: pdoc [ f ' {_class.__name__}.{k} ' ] = None pdoc [ f ' {_class.__name__}.with_name_scope ' ] = None","title":"Documentation"},{"location":"reference/kerod/utils/documentation/#module-kerodutilsdocumentation","text":"None None View Source import tensorflow as tf from tensorflow.python.keras.engine.training import Model def remove_unwanted_doc ( _class , pdoc : dict ): \"\"\"Remove unwanted documentation from tensorflow keras inheritance.\"\"\" if issubclass ( _class , Model ): doc_class_to_remove = Model elif issubclass ( _class , tf . keras . layers . Layer ): doc_class_to_remove = tf . keras . layers . Layer for k in doc_class_to_remove . __dict__ : if k not in [ 'call' , '__doc__' , '__module__' , '__init__' , '__name__' ]: pdoc [ f ' { _class . __name__ } . { k } ' ] = None pdoc [ f ' { _class . __name__ } .with_name_scope' ] = None","title":"Module kerod.utils.documentation"},{"location":"reference/kerod/utils/documentation/#functions","text":"","title":"Functions"},{"location":"reference/kerod/utils/documentation/#remove_unwanted_doc","text":"def remove_unwanted_doc ( _class , pdoc : dict ) Remove unwanted documentation from tensorflow keras inheritance. View Source def remove_unwanted_doc ( _class , pdoc : dict ) : \"\"\" Remove unwanted documentation from tensorflow keras inheritance. \"\"\" if issubclass ( _class , Model ) : doc_class_to_remove = Model elif issubclass ( _class , tf . keras . layers . Layer ) : doc_class_to_remove = tf . keras . layers . Layer for k in doc_class_to_remove . __dict__ : if k not in [ ' call ' , ' __doc__ ' , ' __module__ ' , ' __init__ ' , ' __name__ ' ]: pdoc [ f ' {_class.__name__}.{k} ' ] = None pdoc [ f ' {_class.__name__}.with_name_scope ' ] = None","title":"remove_unwanted_doc"},{"location":"reference/kerod/utils/drawing/","text":"Module kerod.utils.drawing None None View Source from typing import List , Union import matplotlib.colors as pltc import matplotlib.patches as patches import matplotlib.pyplot as plt import numpy as np import tensorflow as tf class BoxDrawer : \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *classes*: The ordered list of classes that we could display \"\"\" def __init__ ( self , classes : List [ str ]): self . _classes = classes all_colors = [ color for color in pltc . cnames . keys ()] self . _colors = [ all_colors [ i ] for i in np . random . randint ( 0 , len ( all_colors ), size = len ( classes )) ] def __call__ ( self , images , images_information , boxes , labels : Union [ np . array , List [ List [ int ]], tf . Tensor ], scores : Union [ np . array , List [ List [ float ]], tf . Tensor ], num_valid_detections : Union [ np . array , List [ int ], tf . Tensor ], resize = True ): \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *images*: An image (tf.Tensor or numpy array) with shape [batch, height, width, 3] - *images_information*: A 2D (tf.Tensor or numpy array) and shape [batch, (height, width)]. It contains the shape of the image without any padding. It allows to remove padding of the image - *boxes*: An array (tf.Tensor, or Numpy array) of boxes with the following shape [batch, num_boxes, (y_min, x_min, y_max, x_max)] are as described by `1`. If set to false the boxes won't be resized. - *labels*: Label of the predicted boxes. - *scores*: Score of the predicted boxes. - *num_valid_detections*: Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. - *resize*: Allow to resize the bounding boxes to the proper size. If set to true the inputs \"\"\" if isinstance ( images_information , np . ndarray ): images_information = images_information . astype ( np . int32 ) elif tf . is_tensor ( images_information ): images_information = tf . cast ( images_information , tf . int32 ) if isinstance ( labels , np . ndarray ): labels = labels . tolist () elif tf . is_tensor ( labels ): labels = labels . numpy () . tolist () if isinstance ( scores , np . ndarray ): scores = scores . tolist () elif tf . is_tensor ( scores ): scores = scores . numpy () . tolist () if isinstance ( num_valid_detections , np . ndarray ): num_valid_detections = num_valid_detections . tolist () elif tf . is_tensor ( num_valid_detections ): num_valid_detections = num_valid_detections . numpy () . tolist () for im , im_info , bb , cls , scr , nvd in zip ( images , images_information , boxes , labels , scores , num_valid_detections ): labels = [ self . _classes [ int ( ind )] for ind in cls ] colors = [ self . _colors [ int ( ind )] for ind in cls ] im = im [: im_info [ 0 ], : im_info [ 1 ]] draw_bounding_boxes ( im , bb , scores = scr , labels = labels , num_valid_detections = nvd , resize = resize , colors = colors ) def draw_bounding_boxes ( image , boxes , labels : list = None , scores : np . array = None , num_valid_detections : int = None , colors : List [ str ] = None , resize = True ): \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *image*: An image (tf.Tensor or numpy array)with shape [height, width, 3] - *boxes*: An array (tf.Tensor, or Numpy array) of boxes with the following shape [num_boxes, (y_min, x_min, y_max, x_max)] - *labels*: A list of string corresponding to the predicted label. - *scores*: A list of scores predicted - *num_valid_detections*: Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. - *colors*: A list of matplotlib colors of length = num_boxes - *resize*: Allow to resize the bounding boxes to the proper size. If set to true the inputs are as described by `1`. If set to false the boxes won't be resized. \"\"\" if isinstance ( image , tf . Tensor ): image = image . numpy () if isinstance ( boxes , tf . Tensor ): boxes = boxes . numpy () plt . figure ( figsize = ( 15 , 15 )) plt . imshow ( image . astype ( np . uint8 )) axes = plt . gca () if isinstance ( boxes , tf . Tensor ): boxes = boxes . numpy () if resize : boxes [:, 0 :: 2 ] *= image . shape [ 0 ] boxes [:, 1 :: 2 ] *= image . shape [ 1 ] if num_valid_detections is not None : boxes = boxes [: num_valid_detections ] for i , box in enumerate ( boxes ): x_y = ( box [ 1 ], box [ 0 ]) width = box [ 3 ] - box [ 1 ] height = box [ 2 ] - box [ 0 ] color = 'r' if colors is None else colors [ i ] # Draw the boxes r = patches . Rectangle ( x_y , width , height , fill = False , edgecolor = color ) axes . add_patch ( r ) if labels is not None : score = scores [ i ] if scores is not None else 1 axes . annotate ( f ' { labels [ i ] } _ { str ( round ( score , 4 )) } ' , x_y , color = color , weight = 'bold' , fontsize = 10 , ha = 'center' , va = 'center' ) plt . show () Functions draw_bounding_boxes def draw_bounding_boxes ( image , boxes , labels : list = None , scores : < built - in function array > = None , num_valid_detections : int = None , colors : List [ str ] = None , resize = True ) Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: floats in [0.0, 1.0] relative to the width and height of the underlying image already resized to the corresponding size Arguments: image : An image (tf.Tensor or numpy array)with shape [height, width, 3] boxes : An array (tf.Tensor, or Numpy array) of boxes with the following shape [num_boxes, (y_min, x_min, y_max, x_max)] labels : A list of string corresponding to the predicted label. scores : A list of scores predicted num_valid_detections : Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. colors : A list of matplotlib colors of length = num_boxes resize : Allow to resize the bounding boxes to the proper size. If set to true the inputs are as described by 1 . If set to false the boxes won't be resized. View Source def draw_bounding_boxes ( image , boxes , labels : list = None , scores : np . array = None , num_valid_detections : int = None , colors : List [ str ] = None , resize = True ) : \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *image*: An image (tf.Tensor or numpy array)with shape [height, width, 3] - *boxes*: An array (tf.Tensor, or Numpy array) of boxes with the following shape [num_boxes, (y_min, x_min, y_max, x_max)] - *labels*: A list of string corresponding to the predicted label. - *scores*: A list of scores predicted - *num_valid_detections*: Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. - *colors*: A list of matplotlib colors of length = num_boxes - *resize*: Allow to resize the bounding boxes to the proper size. If set to true the inputs are as described by `1`. If set to false the boxes won't be resized. \"\"\" if isinstance ( image , tf . Tensor ) : image = image . numpy () if isinstance ( boxes , tf . Tensor ) : boxes = boxes . numpy () plt . figure ( figsize = ( 15 , 15 )) plt . imshow ( image . astype ( np . uint8 )) axes = plt . gca () if isinstance ( boxes , tf . Tensor ) : boxes = boxes . numpy () if resize : boxes [ :, 0::2 ] *= image . shape [ 0 ] boxes [ :, 1::2 ] *= image . shape [ 1 ] if num_valid_detections is not None : boxes = boxes [ :num_valid_detections ] for i , box in enumerate ( boxes ) : x_y = ( box [ 1 ] , box [ 0 ] ) width = box [ 3 ] - box [ 1 ] height = box [ 2 ] - box [ 0 ] color = 'r' if colors is None else colors [ i ] # Draw the boxes r = patches . Rectangle ( x_y , width , height , fill = False , edgecolor = color ) axes . add_patch ( r ) if labels is not None : score = scores [ i ] if scores is not None else 1 axes . annotate ( f '{labels[i]}_{str(round(score, 4))}' , x_y , color = color , weight = 'bold' , fontsize = 10 , ha = 'center' , va = 'center' ) plt . show () Classes BoxDrawer class BoxDrawer ( classes : List [ str ] ) by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: floats in [0.0, 1.0] relative to the width and height of the underlying image already resized to the corresponding size Arguments: classes : The ordered list of classes that we could display","title":"Drawing"},{"location":"reference/kerod/utils/drawing/#module-kerodutilsdrawing","text":"None None View Source from typing import List , Union import matplotlib.colors as pltc import matplotlib.patches as patches import matplotlib.pyplot as plt import numpy as np import tensorflow as tf class BoxDrawer : \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *classes*: The ordered list of classes that we could display \"\"\" def __init__ ( self , classes : List [ str ]): self . _classes = classes all_colors = [ color for color in pltc . cnames . keys ()] self . _colors = [ all_colors [ i ] for i in np . random . randint ( 0 , len ( all_colors ), size = len ( classes )) ] def __call__ ( self , images , images_information , boxes , labels : Union [ np . array , List [ List [ int ]], tf . Tensor ], scores : Union [ np . array , List [ List [ float ]], tf . Tensor ], num_valid_detections : Union [ np . array , List [ int ], tf . Tensor ], resize = True ): \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *images*: An image (tf.Tensor or numpy array) with shape [batch, height, width, 3] - *images_information*: A 2D (tf.Tensor or numpy array) and shape [batch, (height, width)]. It contains the shape of the image without any padding. It allows to remove padding of the image - *boxes*: An array (tf.Tensor, or Numpy array) of boxes with the following shape [batch, num_boxes, (y_min, x_min, y_max, x_max)] are as described by `1`. If set to false the boxes won't be resized. - *labels*: Label of the predicted boxes. - *scores*: Score of the predicted boxes. - *num_valid_detections*: Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. - *resize*: Allow to resize the bounding boxes to the proper size. If set to true the inputs \"\"\" if isinstance ( images_information , np . ndarray ): images_information = images_information . astype ( np . int32 ) elif tf . is_tensor ( images_information ): images_information = tf . cast ( images_information , tf . int32 ) if isinstance ( labels , np . ndarray ): labels = labels . tolist () elif tf . is_tensor ( labels ): labels = labels . numpy () . tolist () if isinstance ( scores , np . ndarray ): scores = scores . tolist () elif tf . is_tensor ( scores ): scores = scores . numpy () . tolist () if isinstance ( num_valid_detections , np . ndarray ): num_valid_detections = num_valid_detections . tolist () elif tf . is_tensor ( num_valid_detections ): num_valid_detections = num_valid_detections . numpy () . tolist () for im , im_info , bb , cls , scr , nvd in zip ( images , images_information , boxes , labels , scores , num_valid_detections ): labels = [ self . _classes [ int ( ind )] for ind in cls ] colors = [ self . _colors [ int ( ind )] for ind in cls ] im = im [: im_info [ 0 ], : im_info [ 1 ]] draw_bounding_boxes ( im , bb , scores = scr , labels = labels , num_valid_detections = nvd , resize = resize , colors = colors ) def draw_bounding_boxes ( image , boxes , labels : list = None , scores : np . array = None , num_valid_detections : int = None , colors : List [ str ] = None , resize = True ): \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *image*: An image (tf.Tensor or numpy array)with shape [height, width, 3] - *boxes*: An array (tf.Tensor, or Numpy array) of boxes with the following shape [num_boxes, (y_min, x_min, y_max, x_max)] - *labels*: A list of string corresponding to the predicted label. - *scores*: A list of scores predicted - *num_valid_detections*: Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. - *colors*: A list of matplotlib colors of length = num_boxes - *resize*: Allow to resize the bounding boxes to the proper size. If set to true the inputs are as described by `1`. If set to false the boxes won't be resized. \"\"\" if isinstance ( image , tf . Tensor ): image = image . numpy () if isinstance ( boxes , tf . Tensor ): boxes = boxes . numpy () plt . figure ( figsize = ( 15 , 15 )) plt . imshow ( image . astype ( np . uint8 )) axes = plt . gca () if isinstance ( boxes , tf . Tensor ): boxes = boxes . numpy () if resize : boxes [:, 0 :: 2 ] *= image . shape [ 0 ] boxes [:, 1 :: 2 ] *= image . shape [ 1 ] if num_valid_detections is not None : boxes = boxes [: num_valid_detections ] for i , box in enumerate ( boxes ): x_y = ( box [ 1 ], box [ 0 ]) width = box [ 3 ] - box [ 1 ] height = box [ 2 ] - box [ 0 ] color = 'r' if colors is None else colors [ i ] # Draw the boxes r = patches . Rectangle ( x_y , width , height , fill = False , edgecolor = color ) axes . add_patch ( r ) if labels is not None : score = scores [ i ] if scores is not None else 1 axes . annotate ( f ' { labels [ i ] } _ { str ( round ( score , 4 )) } ' , x_y , color = color , weight = 'bold' , fontsize = 10 , ha = 'center' , va = 'center' ) plt . show ()","title":"Module kerod.utils.drawing"},{"location":"reference/kerod/utils/drawing/#functions","text":"","title":"Functions"},{"location":"reference/kerod/utils/drawing/#draw_bounding_boxes","text":"def draw_bounding_boxes ( image , boxes , labels : list = None , scores : < built - in function array > = None , num_valid_detections : int = None , colors : List [ str ] = None , resize = True ) Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: floats in [0.0, 1.0] relative to the width and height of the underlying image already resized to the corresponding size Arguments: image : An image (tf.Tensor or numpy array)with shape [height, width, 3] boxes : An array (tf.Tensor, or Numpy array) of boxes with the following shape [num_boxes, (y_min, x_min, y_max, x_max)] labels : A list of string corresponding to the predicted label. scores : A list of scores predicted num_valid_detections : Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. colors : A list of matplotlib colors of length = num_boxes resize : Allow to resize the bounding boxes to the proper size. If set to true the inputs are as described by 1 . If set to false the boxes won't be resized. View Source def draw_bounding_boxes ( image , boxes , labels : list = None , scores : np . array = None , num_valid_detections : int = None , colors : List [ str ] = None , resize = True ) : \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *image*: An image (tf.Tensor or numpy array)with shape [height, width, 3] - *boxes*: An array (tf.Tensor, or Numpy array) of boxes with the following shape [num_boxes, (y_min, x_min, y_max, x_max)] - *labels*: A list of string corresponding to the predicted label. - *scores*: A list of scores predicted - *num_valid_detections*: Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. - *colors*: A list of matplotlib colors of length = num_boxes - *resize*: Allow to resize the bounding boxes to the proper size. If set to true the inputs are as described by `1`. If set to false the boxes won't be resized. \"\"\" if isinstance ( image , tf . Tensor ) : image = image . numpy () if isinstance ( boxes , tf . Tensor ) : boxes = boxes . numpy () plt . figure ( figsize = ( 15 , 15 )) plt . imshow ( image . astype ( np . uint8 )) axes = plt . gca () if isinstance ( boxes , tf . Tensor ) : boxes = boxes . numpy () if resize : boxes [ :, 0::2 ] *= image . shape [ 0 ] boxes [ :, 1::2 ] *= image . shape [ 1 ] if num_valid_detections is not None : boxes = boxes [ :num_valid_detections ] for i , box in enumerate ( boxes ) : x_y = ( box [ 1 ] , box [ 0 ] ) width = box [ 3 ] - box [ 1 ] height = box [ 2 ] - box [ 0 ] color = 'r' if colors is None else colors [ i ] # Draw the boxes r = patches . Rectangle ( x_y , width , height , fill = False , edgecolor = color ) axes . add_patch ( r ) if labels is not None : score = scores [ i ] if scores is not None else 1 axes . annotate ( f '{labels[i]}_{str(round(score, 4))}' , x_y , color = color , weight = 'bold' , fontsize = 10 , ha = 'center' , va = 'center' ) plt . show ()","title":"draw_bounding_boxes"},{"location":"reference/kerod/utils/drawing/#classes","text":"","title":"Classes"},{"location":"reference/kerod/utils/drawing/#boxdrawer","text":"class BoxDrawer ( classes : List [ str ] ) by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: floats in [0.0, 1.0] relative to the width and height of the underlying image already resized to the corresponding size Arguments: classes : The ordered list of classes that we could display","title":"BoxDrawer"},{"location":"reference/kerod/utils/ops/","text":"Module kerod.utils.ops None None View Source import tensorflow as tf def indices_to_dense_vector ( indices , size , indices_value = 1. , default_value = 0 , dtype = tf . float32 ): \"\"\"Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Arguments: - *indices*: 1d Tensor with integer indices which are to be set to indices_values. - *size*: scalar with size (integer) of output Tensor. - *indices_value*: values of elements specified by indices in the output vector - *default_value*: values of other elements in the output vector. - *dtype*: data type. Returns: A dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. \"\"\" size = tf . cast ( size , dtype = tf . int32 ) zeros = tf . ones ([ size ], dtype = dtype ) * default_value values = tf . ones_like ( indices , dtype = dtype ) * indices_value return tf . dynamic_stitch ([ tf . range ( size ), tf . cast ( indices , dtype = tf . int32 )], [ zeros , values ]) def item_assignment ( tensor : tf . Tensor , indicator : tf . Tensor , val ): \"\"\"Set the indicated fields of tensor to val. ```python tensor = tf.constant([1, 2, 3, 4]) # won't work in tensorflow tensor[tensor == 2] = 1 tensor = item_assignment(tensor, tensor == 2, 1) ``` Arguments: - *tensor*: A tensor without shape constraint. - *indicator*: boolean tensor with same shape as `tensor`. - *val*: scalar with value to set. \"\"\" indicator = tf . cast ( indicator , tensor . dtype ) return tensor * ( 1 - indicator ) + val * indicator def get_full_indices ( indices ): \"\"\" This operation allows to extract full indices from indices. These full-indices have the proper format for gather_nd operations. Example: ```python indices = [[0, 1], [2, 3]] full_indices = [[[0, 0], [0, 1]], [[1, 2], [1, 3]]] ``` Arguments: - *indices*: Indices without their assorciated batch format [batch_size, k]. Returns: Full-indices tensor [batch_size, k, 2] \"\"\" batch_size = tf . shape ( indices )[ 0 ] num_elements = tf . shape ( indices )[ 1 ] batch_ids = tf . range ( 0 , batch_size ) # Repeat the batch indice for every indices per batch # [0, 1, ..., n] => [[0, ..., 0], [1, ..., 1], ..., [n, ..., n]] batch_ids = tf . tile ( batch_ids [:, None ], [ 1 , num_elements ]) # [[a1, ..., au], ...,[n1, ..., nu]] => # [[[0, a1], ..., [0, au]], ..., [[n, n1], ..., [n, nu]]] full_indices = tf . concat ( [ batch_ids [ ... , None ], indices [ ... , None ]], axis =- 1 ) return full_indices Functions get_full_indices def get_full_indices ( indices ) This operation allows to extract full indices from indices. These full-indices have the proper format for gather_nd operations. Example: indices = [[ 0 , 1 ], [ 2 , 3 ]] full_indices = [[[ 0 , 0 ], [ 0 , 1 ]], [[ 1 , 2 ], [ 1 , 3 ]]] Arguments: indices : Indices without their assorciated batch format [batch_size, k]. Returns: Full-indices tensor [batch_size, k, 2] View Source def get_full_indices ( indices ) : \"\"\" This operation allows to extract full indices from indices. These full - indices have the proper format for gather_nd operations . Example : ``` python indices = [[ 0 , 1 ], [ 2 , 3 ]] full_indices = [[[ 0 , 0 ], [ 0 , 1 ]], [[ 1 , 2 ], [ 1 , 3 ]]] ``` Arguments : - * indices * : Indices without their assorciated batch format [ batch_size , k ]. Returns : Full - indices tensor [ batch_size , k , 2 ] \"\"\" batch_size = tf . shape ( indices ) [ 0 ] num_elements = tf . shape ( indices ) [ 1 ] batch_ids = tf . range ( 0 , batch_size ) # Repeat the batch indice for every indices per batch # [ 0 , 1 , ..., n ] => [[ 0 , ..., 0 ], [ 1 , ..., 1 ], ..., [ n , ..., n ]] batch_ids = tf . tile ( batch_ids [:, None ], [ 1 , num_elements ] ) # [[ a1 , ..., au ], ...,[ n1 , ..., nu ]] => # [[[ 0 , a1 ], ..., [ 0 , au ]], ..., [[ n , n1 ], ..., [ n , nu ]]] full_indices = tf . concat ( [ batch_ids [..., None ], indices [..., None ]], axis =- 1 ) return full_indices indices_to_dense_vector def indices_to_dense_vector ( indices , size , indices_value = 1.0 , default_value = 0 , dtype = tf . float32 ) Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Arguments: indices : 1d Tensor with integer indices which are to be set to indices_values. size : scalar with size (integer) of output Tensor. indices_value : values of elements specified by indices in the output vector default_value : values of other elements in the output vector. dtype : data type. Returns: A dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. View Source def indices_to_dense_vector ( indices , size , indices_value = 1. , default_value = 0 , dtype = tf . float32 ) : \"\"\"Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Arguments: - *indices*: 1d Tensor with integer indices which are to be set to indices_values. - *size*: scalar with size (integer) of output Tensor. - *indices_value*: values of elements specified by indices in the output vector - *default_value*: values of other elements in the output vector. - *dtype*: data type. Returns: A dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. \"\"\" size = tf . cast ( size , dtype = tf . int32 ) zeros = tf . ones ( [ size ] , dtype = dtype ) * default_value values = tf . ones_like ( indices , dtype = dtype ) * indices_value return tf . dynamic_stitch ( [ tf.range(size), tf.cast(indices, dtype=tf.int32) ] , [ zeros, values ] ) item_assignment def item_assignment ( tensor : tensorflow . python . framework . ops . Tensor , indicator : tensorflow . python . framework . ops . Tensor , val ) Set the indicated fields of tensor to val. tensor = tf . constant ([ 1 , 2 , 3 , 4 ]) # won't work in tensorflow tensor [ tensor == 2 ] = 1 tensor = item_assignment ( tensor , tensor == 2 , 1 ) Arguments: tensor : A tensor without shape constraint. indicator : boolean tensor with same shape as tensor . val : scalar with value to set. View Source def item_assignment ( tensor : tf . Tensor , indicator : tf . Tensor , val ) : \" \"\" Set the indicated fields of tensor to val. ```python tensor = tf.constant([1, 2, 3, 4]) # won't work in tensorflow tensor[tensor == 2] = 1 tensor = item_assignment(tensor, tensor == 2, 1) ``` Arguments: - *tensor*: A tensor without shape constraint. - *indicator*: boolean tensor with same shape as `tensor`. - *val*: scalar with value to set. \"\" \" indicator = tf . cast ( indicator , tensor . dtype ) return tensor * ( 1 - indicator ) + val * indicator","title":"Ops"},{"location":"reference/kerod/utils/ops/#module-kerodutilsops","text":"None None View Source import tensorflow as tf def indices_to_dense_vector ( indices , size , indices_value = 1. , default_value = 0 , dtype = tf . float32 ): \"\"\"Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Arguments: - *indices*: 1d Tensor with integer indices which are to be set to indices_values. - *size*: scalar with size (integer) of output Tensor. - *indices_value*: values of elements specified by indices in the output vector - *default_value*: values of other elements in the output vector. - *dtype*: data type. Returns: A dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. \"\"\" size = tf . cast ( size , dtype = tf . int32 ) zeros = tf . ones ([ size ], dtype = dtype ) * default_value values = tf . ones_like ( indices , dtype = dtype ) * indices_value return tf . dynamic_stitch ([ tf . range ( size ), tf . cast ( indices , dtype = tf . int32 )], [ zeros , values ]) def item_assignment ( tensor : tf . Tensor , indicator : tf . Tensor , val ): \"\"\"Set the indicated fields of tensor to val. ```python tensor = tf.constant([1, 2, 3, 4]) # won't work in tensorflow tensor[tensor == 2] = 1 tensor = item_assignment(tensor, tensor == 2, 1) ``` Arguments: - *tensor*: A tensor without shape constraint. - *indicator*: boolean tensor with same shape as `tensor`. - *val*: scalar with value to set. \"\"\" indicator = tf . cast ( indicator , tensor . dtype ) return tensor * ( 1 - indicator ) + val * indicator def get_full_indices ( indices ): \"\"\" This operation allows to extract full indices from indices. These full-indices have the proper format for gather_nd operations. Example: ```python indices = [[0, 1], [2, 3]] full_indices = [[[0, 0], [0, 1]], [[1, 2], [1, 3]]] ``` Arguments: - *indices*: Indices without their assorciated batch format [batch_size, k]. Returns: Full-indices tensor [batch_size, k, 2] \"\"\" batch_size = tf . shape ( indices )[ 0 ] num_elements = tf . shape ( indices )[ 1 ] batch_ids = tf . range ( 0 , batch_size ) # Repeat the batch indice for every indices per batch # [0, 1, ..., n] => [[0, ..., 0], [1, ..., 1], ..., [n, ..., n]] batch_ids = tf . tile ( batch_ids [:, None ], [ 1 , num_elements ]) # [[a1, ..., au], ...,[n1, ..., nu]] => # [[[0, a1], ..., [0, au]], ..., [[n, n1], ..., [n, nu]]] full_indices = tf . concat ( [ batch_ids [ ... , None ], indices [ ... , None ]], axis =- 1 ) return full_indices","title":"Module kerod.utils.ops"},{"location":"reference/kerod/utils/ops/#functions","text":"","title":"Functions"},{"location":"reference/kerod/utils/ops/#get_full_indices","text":"def get_full_indices ( indices ) This operation allows to extract full indices from indices. These full-indices have the proper format for gather_nd operations. Example: indices = [[ 0 , 1 ], [ 2 , 3 ]] full_indices = [[[ 0 , 0 ], [ 0 , 1 ]], [[ 1 , 2 ], [ 1 , 3 ]]] Arguments: indices : Indices without their assorciated batch format [batch_size, k]. Returns: Full-indices tensor [batch_size, k, 2] View Source def get_full_indices ( indices ) : \"\"\" This operation allows to extract full indices from indices. These full - indices have the proper format for gather_nd operations . Example : ``` python indices = [[ 0 , 1 ], [ 2 , 3 ]] full_indices = [[[ 0 , 0 ], [ 0 , 1 ]], [[ 1 , 2 ], [ 1 , 3 ]]] ``` Arguments : - * indices * : Indices without their assorciated batch format [ batch_size , k ]. Returns : Full - indices tensor [ batch_size , k , 2 ] \"\"\" batch_size = tf . shape ( indices ) [ 0 ] num_elements = tf . shape ( indices ) [ 1 ] batch_ids = tf . range ( 0 , batch_size ) # Repeat the batch indice for every indices per batch # [ 0 , 1 , ..., n ] => [[ 0 , ..., 0 ], [ 1 , ..., 1 ], ..., [ n , ..., n ]] batch_ids = tf . tile ( batch_ids [:, None ], [ 1 , num_elements ] ) # [[ a1 , ..., au ], ...,[ n1 , ..., nu ]] => # [[[ 0 , a1 ], ..., [ 0 , au ]], ..., [[ n , n1 ], ..., [ n , nu ]]] full_indices = tf . concat ( [ batch_ids [..., None ], indices [..., None ]], axis =- 1 ) return full_indices","title":"get_full_indices"},{"location":"reference/kerod/utils/ops/#indices_to_dense_vector","text":"def indices_to_dense_vector ( indices , size , indices_value = 1.0 , default_value = 0 , dtype = tf . float32 ) Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Arguments: indices : 1d Tensor with integer indices which are to be set to indices_values. size : scalar with size (integer) of output Tensor. indices_value : values of elements specified by indices in the output vector default_value : values of other elements in the output vector. dtype : data type. Returns: A dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. View Source def indices_to_dense_vector ( indices , size , indices_value = 1. , default_value = 0 , dtype = tf . float32 ) : \"\"\"Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Arguments: - *indices*: 1d Tensor with integer indices which are to be set to indices_values. - *size*: scalar with size (integer) of output Tensor. - *indices_value*: values of elements specified by indices in the output vector - *default_value*: values of other elements in the output vector. - *dtype*: data type. Returns: A dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. \"\"\" size = tf . cast ( size , dtype = tf . int32 ) zeros = tf . ones ( [ size ] , dtype = dtype ) * default_value values = tf . ones_like ( indices , dtype = dtype ) * indices_value return tf . dynamic_stitch ( [ tf.range(size), tf.cast(indices, dtype=tf.int32) ] , [ zeros, values ] )","title":"indices_to_dense_vector"},{"location":"reference/kerod/utils/ops/#item_assignment","text":"def item_assignment ( tensor : tensorflow . python . framework . ops . Tensor , indicator : tensorflow . python . framework . ops . Tensor , val ) Set the indicated fields of tensor to val. tensor = tf . constant ([ 1 , 2 , 3 , 4 ]) # won't work in tensorflow tensor [ tensor == 2 ] = 1 tensor = item_assignment ( tensor , tensor == 2 , 1 ) Arguments: tensor : A tensor without shape constraint. indicator : boolean tensor with same shape as tensor . val : scalar with value to set. View Source def item_assignment ( tensor : tf . Tensor , indicator : tf . Tensor , val ) : \" \"\" Set the indicated fields of tensor to val. ```python tensor = tf.constant([1, 2, 3, 4]) # won't work in tensorflow tensor[tensor == 2] = 1 tensor = item_assignment(tensor, tensor == 2, 1) ``` Arguments: - *tensor*: A tensor without shape constraint. - *indicator*: boolean tensor with same shape as `tensor`. - *val*: scalar with value to set. \"\" \" indicator = tf . cast ( indicator , tensor . dtype ) return tensor * ( 1 - indicator ) + val * indicator","title":"item_assignment"},{"location":"reference/kerod/utils/training/","text":"Module kerod.utils.training None None View Source import tensorflow as tf from typing import Callable def freeze_layers_before ( model : tf . keras . Model , layer_name : str ): \"\"\"Freezes layers of a Keras `model` before a given `layer_name` (excluded).\"\"\" freeze_before = model . get_layer ( layer_name ) index_freeze_before = model . layers . index ( freeze_before ) for layer in model . layers [: index_freeze_before ]: layer . trainable = False def freeze_batch_normalization ( model : tf . keras . Model ): \"\"\"In Object detection we usually do not train on big batch. The BatchNormalization is not useful and should be frozen. \"\"\" for layer in model . layers : if isinstance ( layer , tf . keras . layers . BatchNormalization ): layer . trainable = False def apply_kernel_regularization ( func : Callable , model : tf . keras . Model ): \"\"\"Apply kernel regularization on all the trainable layers of a Layer or a Model\"\"\" for layer in model . layers : if hasattr ( layer , 'kernel' ) and layer . trainable : model . add_loss ( func ( layer . kernel )) Functions apply_kernel_regularization def apply_kernel_regularization ( func : Callable , model : keras . engine . training . Model ) Apply kernel regularization on all the trainable layers of a Layer or a Model View Source def apply_kernel_regularization ( func: Callable , model: tf . keras . Model ) : \"\"\"Apply kernel regularization on all the trainable layers of a Layer or a Model\"\"\" for layer in model . layers: if hasattr ( layer , ' kernel ') and layer . trainable: model . add_loss ( func ( layer . kernel )) freeze_batch_normalization def freeze_batch_normalization ( model : keras . engine . training . Model ) In Object detection we usually do not train on big batch. The BatchNormalization is not useful and should be frozen. View Source def freeze_batch_normalization ( model : tf . keras . Model ) : \"\"\" In Object detection we usually do not train on big batch. The BatchNormalization is not useful and should be frozen . \"\"\" for layer in model . layers : if isinstance ( layer , tf . keras . layers . BatchNormalization ) : layer . trainable = False freeze_layers_before def freeze_layers_before ( model : keras . engine . training . Model , layer_name : str ) Freezes layers of a Keras model before a given layer_name (excluded). View Source def freeze_layers_before ( model : tf . keras . Model , layer_name : str ) : \" \"\" Freezes layers of a Keras `model` before a given `layer_name` (excluded). \"\" \" freeze_before = model . get_layer ( layer_name ) index_freeze_before = model . layers . index ( freeze_before ) for layer in model . layers [ : index_freeze_before ] : layer . trainable = False","title":"Training"},{"location":"reference/kerod/utils/training/#module-kerodutilstraining","text":"None None View Source import tensorflow as tf from typing import Callable def freeze_layers_before ( model : tf . keras . Model , layer_name : str ): \"\"\"Freezes layers of a Keras `model` before a given `layer_name` (excluded).\"\"\" freeze_before = model . get_layer ( layer_name ) index_freeze_before = model . layers . index ( freeze_before ) for layer in model . layers [: index_freeze_before ]: layer . trainable = False def freeze_batch_normalization ( model : tf . keras . Model ): \"\"\"In Object detection we usually do not train on big batch. The BatchNormalization is not useful and should be frozen. \"\"\" for layer in model . layers : if isinstance ( layer , tf . keras . layers . BatchNormalization ): layer . trainable = False def apply_kernel_regularization ( func : Callable , model : tf . keras . Model ): \"\"\"Apply kernel regularization on all the trainable layers of a Layer or a Model\"\"\" for layer in model . layers : if hasattr ( layer , 'kernel' ) and layer . trainable : model . add_loss ( func ( layer . kernel ))","title":"Module kerod.utils.training"},{"location":"reference/kerod/utils/training/#functions","text":"","title":"Functions"},{"location":"reference/kerod/utils/training/#apply_kernel_regularization","text":"def apply_kernel_regularization ( func : Callable , model : keras . engine . training . Model ) Apply kernel regularization on all the trainable layers of a Layer or a Model View Source def apply_kernel_regularization ( func: Callable , model: tf . keras . Model ) : \"\"\"Apply kernel regularization on all the trainable layers of a Layer or a Model\"\"\" for layer in model . layers: if hasattr ( layer , ' kernel ') and layer . trainable: model . add_loss ( func ( layer . kernel ))","title":"apply_kernel_regularization"},{"location":"reference/kerod/utils/training/#freeze_batch_normalization","text":"def freeze_batch_normalization ( model : keras . engine . training . Model ) In Object detection we usually do not train on big batch. The BatchNormalization is not useful and should be frozen. View Source def freeze_batch_normalization ( model : tf . keras . Model ) : \"\"\" In Object detection we usually do not train on big batch. The BatchNormalization is not useful and should be frozen . \"\"\" for layer in model . layers : if isinstance ( layer , tf . keras . layers . BatchNormalization ) : layer . trainable = False","title":"freeze_batch_normalization"},{"location":"reference/kerod/utils/training/#freeze_layers_before","text":"def freeze_layers_before ( model : keras . engine . training . Model , layer_name : str ) Freezes layers of a Keras model before a given layer_name (excluded). View Source def freeze_layers_before ( model : tf . keras . Model , layer_name : str ) : \" \"\" Freezes layers of a Keras `model` before a given `layer_name` (excluded). \"\" \" freeze_before = model . get_layer ( layer_name ) index_freeze_before = model . layers . index ( freeze_before ) for layer in model . layers [ : index_freeze_before ] : layer . trainable = False","title":"freeze_layers_before"}]}